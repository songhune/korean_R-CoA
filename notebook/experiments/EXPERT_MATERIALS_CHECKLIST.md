# Expert Evaluation Materials Checklist

실제로 한문학 전공자들에게 보낼 자료 체크리스트입니다.

## 📋 필수 준비 자료

### 1. 연구 소개 자료 (Korean)

#### 📄 Executive Summary (2-3 페이지)
- [ ] 연구 배경 및 목적
- [ ] KLSBench 개요
  - [ ] 5개 태스크 설명
  - [ ] 데이터 규모 (7,871 항목)
  - [ ] 데이터 출처 (과거시험, 사서)
- [ ] AI 모델 평가 결과 요약
- [ ] 전문가 검증의 필요성
- [ ] 참여 혜택 (사례금, 공동저자 등)

#### 📊 벤치마크 상세 설명서 (10-15 페이지)
- [ ] 각 태스크별 상세 설명
- [ ] 분류 체계 (Classification 19개 라벨)
- [ ] 데이터 수집 방법론
- [ ] 품질 관리 과정
- [ ] 샘플 예시 (각 태스크당 5-10개)
- [ ] 현재까지의 AI 평가 결과

### 2. 평가용 샘플 데이터

#### 📑 Stratified Sample Dataset
- [ ] Classification: 80개 항목 (10% 샘플)
  - [ ] 균형 클래스 60개 (각 10개)
  - [ ] 기타 클래스 20개
- [ ] Retrieval: 120개 항목
  - [ ] 논어 30개, 맹자 30개, 대학 30개, 중용 30개
- [ ] Punctuation: 200개 항목
  - [ ] 쉬움 50개, 중간 100개, 어려움 50개
- [ ] NLI: 180개 항목
  - [ ] 각 라벨 60개 (entailment, contradiction, neutral)
- [ ] Translation: 200개 항목
  - [ ] 방향별 균등 분포

#### 📊 데이터 포맷
```
Option A: Excel/Google Sheets
- 항목 ID
- 입력 텍스트
- 현재 라벨/정답
- 평가 의견란
- 정확도 평가 (1-5)
- 오류 지적란

Option B: Web Interface (Recommended)
- Streamlit/Gradio 기반
- 실시간 저장
- 진행률 표시
```

### 3. 평가 가이드라인

#### 📖 평가 매뉴얼 (5-7 페이지)
- [ ] 평가 목적 및 중요성
- [ ] 평가 절차
  - [ ] 로그인/접속 방법
  - [ ] 평가 방법
  - [ ] 저장/제출 방법
- [ ] 평가 기준
  - [ ] 정확성 평가 기준
  - [ ] 난이도 평가 기준
  - [ ] 오류 지적 방법
- [ ] FAQ
- [ ] 문의처

#### 📝 평가 질문지
- [ ] 항목별 평가 (샘플 데이터)
- [ ] 전반적 품질 평가 (5-point Likert)
- [ ] 난이도 평가
- [ ] 분류 체계 타당성 (Classification)
- [ ] 오류 및 개선 제안
- [ ] 자유 의견

### 4. 법적/윤리 문서

#### 📜 동의서
- [ ] 연구 참여 동의서
  - [ ] 연구 목적 설명
  - [ ] 데이터 사용 범위
  - [ ] 개인정보 보호
  - [ ] 중도 포기 권리
  - [ ] 사례금 지급 조건
- [ ] 저작권 관련 동의
  - [ ] 평가 의견의 연구 활용 동의
  - [ ] 공동저자 기여 조건 (선택)

#### 🔒 개인정보 보호
- [ ] 데이터 보안 계획
- [ ] 익명화 처리 방법
- [ ] IRB 승인 (필요시)

### 5. 보상/인센티브

#### 💰 사례금 관련
- [ ] 사례금 지급 기준 명시
  - [ ] 파일럿: 5만원
  - [ ] 본 평가: 10만원
  - [ ] 베이스라인 참여: 15만원
- [ ] 지급 방법 (계좌이체)
- [ ] 세금 처리 안내 (원천징수)
- [ ] 지급 일정

#### 🏆 학술적 인정
- [ ] 기여도에 따른 저자 표기 기준
  - [ ] 단순 평가: Acknowledgments
  - [ ] 심층 피드백: Co-author (조건 명시)
- [ ] 논문 사전 공유 약속
- [ ] 발표 자료 제공

## 🔧 기술적 준비사항

### 평가 플랫폼

#### Option 1: Web-based (추천)
```bash
# Streamlit app
streamlit run expert_evaluation_app.py

Features:
- 로그인 시스템
- 진행률 저장
- 실시간 백업
- 응답 내보내기
```

- [ ] 앱 개발 완료
- [ ] 서버 배포 (Hugging Face Spaces or AWS)
- [ ] 테스트 완료
- [ ] 사용자 가이드 작성

#### Option 2: Google Forms/Sheets
- [ ] 폼 생성 (각 태스크별)
- [ ] 데이터 검증 규칙 설정
- [ ] 자동 응답 수집 시트
- [ ] 접근 권한 설정

#### Option 3: Excel 파일
- [ ] 엑셀 템플릿 생성
- [ ] 매크로/수식 설정 (선택)
- [ ] 작성 가이드
- [ ] 제출 방법 안내

### 데이터 수집 시스템
- [ ] 응답 자동 저장
- [ ] 백업 시스템
- [ ] 데이터 검증
- [ ] 분석 파이프라인

## 📧 커뮤니케이션 자료

### 초기 접촉

#### 📨 모집 이메일 템플릿
```
제목: [협조 요청] 한국 고전 문헌 AI 벤치마크 전문가 검증 참여 요청

[교수님/선생님] 귀하,

안녕하십니까. [소속/이름]입니다.

저희는 대규모 언어 모델(LLM)의 한국 고전 문헌 이해 능력을
평가하기 위한 KLSBench 벤치마크를 개발하였습니다.

본 벤치마크는 과거시험 데이터와 사서(四書) 텍스트를 기반으로
5가지 태스크(분류, 검색, 구두점 복원, 추론, 번역)로 구성되어 있으며,
총 7,871개의 항목을 포함하고 있습니다.

학술적 타당성 확보를 위해 한문학 전공 교수님들의 전문적 검증을
요청드리고자 합니다.

【참여 내용】
- 평가 항목: 각 태스크별 샘플 평가 (총 780개 항목 중 일부)
- 소요 시간: 2-3시간 (온라인 플랫폼)
- 평가 기간: [시작일] ~ [종료일] (유연한 일정)
- 사례금: 10만원
- 추가 혜택: 논문 공동저자 또는 Acknowledgments

【첨부 자료】
- 연구 소개서
- 벤치마크 상세 설명
- 샘플 데이터 예시

관심 있으신 경우 회신 부탁드립니다.
상세 평가 자료와 동의서를 보내드리겠습니다.

감사합니다.

[연락처]
[이메일]
```

#### 📞 사전 미팅 자료
- [ ] 프레젠테이션 슬라이드 (10-15 슬라이드)
  - [ ] 연구 소개
  - [ ] 벤치마크 설명
  - [ ] 평가 방법
  - [ ] 기대 효과
  - [ ] Q&A
- [ ] Demo 준비
  - [ ] 평가 플랫폼 시연
  - [ ] 샘플 데이터 쇼케이스

### 진행 중 커뮤니케이션

#### 📬 평가 시작 메일
```
제목: KLSBench 전문가 평가 시작 안내

안녕하십니까,

먼저 참여에 감사드립니다.
평가 시작을 위한 정보를 안내드립니다.

【접속 정보】
- URL: [평가 플랫폼 링크]
- 로그인 ID: [ID]
- 임시 비밀번호: [PW]

【평가 자료】
- 평가 매뉴얼: [링크]
- 동의서: [링크] (작성 후 제출 필요)

【일정】
- 평가 기간: [날짜]
- 예상 소요 시간: 2-3시간
- 중간 저장 가능

【문의】
- 이메일: [이메일]
- 전화: [번호]
- 업무 시간: 평일 10:00-18:00

감사합니다.
```

#### 🔔 리마인더 메일 (중간)
- [ ] 1주일 후 진행률 확인
- [ ] 격려 메시지
- [ ] 질문 확인

#### ✅ 완료 감사 메일
- [ ] 참여 감사
- [ ] 사례금 지급 안내
- [ ] 결과 공유 약속
- [ ] 후속 연구 참여 의향 확인

### 사후 커뮤니케이션

#### 📊 결과 보고서 (참여자용)
- [ ] 평가 결과 요약
- [ ] 통계 분석 결과
- [ ] 벤치마크 개선 사항
- [ ] 논문 초고 (공유 약속시)

## 📂 파일 구조 예시

```
expert_evaluation/
├── 00_recruitment/
│   ├── executive_summary_ko.pdf
│   ├── recruitment_email.txt
│   ├── presentation_slides.pptx
│   └── FAQ.md
│
├── 01_consent/
│   ├── consent_form.pdf
│   ├── privacy_policy.pdf
│   └── author_agreement.pdf (optional)
│
├── 02_evaluation_materials/
│   ├── evaluation_manual.pdf
│   ├── task_descriptions.pdf
│   ├── sample_data/
│   │   ├── classification_sample.xlsx
│   │   ├── retrieval_sample.xlsx
│   │   ├── punctuation_sample.xlsx
│   │   ├── nli_sample.xlsx
│   │   └── translation_sample.xlsx
│   └── questionnaire.pdf
│
├── 03_platform/
│   ├── expert_evaluation_app.py (Streamlit)
│   ├── requirements.txt
│   ├── README.md
│   └── user_guide.pdf
│
├── 04_communication/
│   ├── start_notification.txt
│   ├── reminder_template.txt
│   ├── thank_you_email.txt
│   └── result_report_template.pdf
│
└── 05_analysis/
    ├── data_collection.py
    ├── iaa_calculation.py
    ├── error_analysis.py
    └── report_generation.py
```

## ⏱️ 타임라인 체크리스트

### Week 1-2: Material Preparation
- [ ] Executive summary 작성
- [ ] 벤치마크 상세 설명서 작성
- [ ] 샘플 데이터 추출 및 검증
- [ ] 평가 질문지 작성
- [ ] 평가 플랫폼 개발/설정
- [ ] 법적 문서 준비

### Week 3: Testing
- [ ] 내부 테스트 (동료 검토)
- [ ] 플랫폼 버그 수정
- [ ] 자료 최종 검토
- [ ] IRB 제출 (필요시)

### Week 4: Pilot Recruitment
- [ ] 파일럿 전문가 2-3명 섭외
- [ ] 초기 자료 발송
- [ ] 사전 미팅 실시

### Week 5-6: Pilot Evaluation
- [ ] 파일럿 평가 진행
- [ ] 중간 피드백 수집
- [ ] 이슈 대응
- [ ] 자료 수정

### Week 7-8: Main Recruitment
- [ ] 본 평가 전문가 5-10명 섭외
- [ ] 모집 이메일 발송
- [ ] 미팅 및 설명회
- [ ] 참여 확정

### Week 9-12: Main Evaluation
- [ ] 평가 자료 배포
- [ ] 진행 모니터링
- [ ] 질문 대응
- [ ] 리마인더 발송
- [ ] 데이터 수집

### Week 13-16: Analysis & Reporting
- [ ] 통계 분석 (IAA, accuracy)
- [ ] 질적 분석 (주제 코딩)
- [ ] 벤치마크 수정
- [ ] 결과 보고서 작성
- [ ] 참여자에게 결과 공유

## 💡 Pro Tips

### 참여율 높이기
1. **적절한 보상**: 시간 대비 공정한 사례금
2. **학술적 인정**: 논문 저자 표기 기회
3. **유연한 일정**: 참여자 편의 고려
4. **명확한 가이드**: 평가 방법 상세 설명
5. **신속한 대응**: 질문에 빠른 답변

### 품질 관리
1. **파일럿 테스트**: 소규모로 먼저 검증
2. **명확한 기준**: 평가 기준 구체화
3. **예시 제공**: 좋은/나쁜 예시 포함
4. **중간 체크**: 진행 중 품질 확인
5. **인센티브**: 성실한 평가에 추가 보상

### 관계 유지
1. **정기적 소통**: 진행 상황 공유
2. **결과 환원**: 평가 결과 공유
3. **공동 연구**: 후속 연구 제안
4. **네트워크**: 학계 인맥 구축
5. **감사 표현**: 진심 어린 감사

## 📞 Contact Information

### 연구팀 연락처
```
주 연구자: [이름]
소속: [기관/학교]
이메일: [이메일]
전화: [번호]
웹사이트: [URL]

연구 지원: [이름]
이메일: [이메일]
```

### 긴급 연락
```
기술 지원: [이메일/전화]
평가 관련 문의: [이메일/전화]
사례금 문의: [이메일/전화]
```

---

**Version**: 1.0
**Last Updated**: 2025-10-30
**Status**: Ready for Use
