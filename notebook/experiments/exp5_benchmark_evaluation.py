"""
K-ClassicBench Evaluation Framework
í•œêµ­ ê³ ì „ ë¬¸í—Œ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ í”„ë ˆì„ì›Œí¬

ì§€ì› ëª¨ë¸:
1. ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸: Llama, Qwen, EXAONE ë“±
2. ë¹„ê³µê°œ API ëª¨ë¸: GPT-4, Claude, Gemini ë“±
3. ì§€ë„í•™ìŠµ ëª¨ë¸: GwenBert, Tongu ë“±

í‰ê°€ íƒœìŠ¤í¬:
- Classification: ë¬¸ì²´ ë¶„ë¥˜
- Retrieval: ì¶œì²˜ ì‹ë³„
- Punctuation: êµ¬ë‘ì  ë³µì›
- NLI: ìì—°ì–¸ì–´ì¶”ë¡ 
- Translation: ë²ˆì—­
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import time
import re
from tqdm import tqdm
import numpy as np
import unicodedata

# Metrics
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction


class KClassicBenchEvaluator:
    """K-ClassicBench ë²¤ì¹˜ë§ˆí¬ í‰ê°€ê¸°"""

    def __init__(self,
                 benchmark_path: str,
                 output_dir: str,
                 model_type: str = "api",  # "api", "opensource", "supervised"
                 max_samples_per_task: Optional[int] = None,
                 sample_ratio: Optional[float] = None):
        """
        Args:
            benchmark_path: ë²¤ì¹˜ë§ˆí¬ JSON íŒŒì¼ ê²½ë¡œ
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            model_type: ëª¨ë¸ íƒ€ì… (api/opensource/supervised)
            max_samples_per_task: íƒœìŠ¤í¬ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            sample_ratio: ìƒ˜í”Œë§ ë¹„ìœ¨ (0.0~1.0, Noneì´ë©´ ì „ì²´)
        """
        self.benchmark_path = Path(benchmark_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.model_type = model_type
        self.max_samples_per_task = max_samples_per_task
        self.sample_ratio = sample_ratio

        # ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ
        self.load_benchmark()

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.setup_prompts()

    def load_benchmark(self):
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ ë²¤ì¹˜ë§ˆí¬ ë¡œë”©: {self.benchmark_path}")
        with open(self.benchmark_path, 'r', encoding='utf-8') as f:
            self.benchmark = json.load(f)

        print(f"  âœ“ {self.benchmark['benchmark_info']['name']}")
        print(f"  âœ“ ì´ {self.benchmark['benchmark_info']['total_size']:,}ê°œ í•­ëª©")
        print(f"  âœ“ {len(self.benchmark['tasks'])}ê°œ íƒœìŠ¤í¬")

        # ìƒ˜í”Œ ì œí•œ ì ìš© (ìš°ì„ ìˆœìœ„: max_samples > sample_ratio)
        if self.max_samples_per_task:
            print(f"\nâš ï¸  ê° íƒœìŠ¤í¬ë‹¹ ìµœëŒ€ {self.max_samples_per_task}ê°œ ìƒ˜í”Œë¡œ ì œí•œ")
            for task_name, task_data in self.benchmark['tasks'].items():
                original_size = len(task_data['data'])
                task_data['data'] = task_data['data'][:self.max_samples_per_task]
                task_data['size'] = len(task_data['data'])
                print(f"  - {task_name}: {original_size} â†’ {task_data['size']}")
        elif self.sample_ratio:
            print(f"\nğŸ“Š ìƒ˜í”Œë§ ë¹„ìœ¨ {self.sample_ratio} ({self.sample_ratio*100:.1f}%) ì ìš©")
            total_sampled = 0
            for task_name, task_data in self.benchmark['tasks'].items():
                original_size = len(task_data['data'])
                sample_size = max(1, int(original_size * self.sample_ratio))

                # ëœë¤ ìƒ˜í”Œë§ (ì¬í˜„ì„±ì„ ìœ„í•´ seed ê³ ì •)
                np.random.seed(42)
                indices = np.random.choice(original_size, sample_size, replace=False)
                task_data['data'] = [task_data['data'][i] for i in sorted(indices)]
                task_data['size'] = len(task_data['data'])
                total_sampled += task_data['size']
                print(f"  - {task_name}: {original_size} â†’ {task_data['size']} ({task_data['size']/original_size*100:.1f}%)")
            print(f"  - ì´ê³„: {self.benchmark['benchmark_info']['total_size']} â†’ {total_sampled} ({total_sampled/self.benchmark['benchmark_info']['total_size']*100:.1f}%)")

    def setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        self.prompts = {
            'classification': {
                'system': "ë‹¹ì‹ ì€ í•œêµ­ ê³ ì „ ë¬¸í—Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í•œë¬¸ í…ìŠ¤íŠ¸ì˜ ë¬¸ì²´ë¥¼ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ì„¸ìš”.",
                'user': """ë‹¤ìŒ í•œë¬¸ í…ìŠ¤íŠ¸ì˜ ë¬¸ì²´ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

ê°€ëŠ¥í•œ ë¬¸ì²´: è³¦(ë¶€), è©©(ì‹œ), ç–‘(ì˜), ç¾©(ì˜), ç­–(ì±…), è«–(ë…¼), è¡¨(í‘œ), ç®‹(ì „), è¬›(ê°•), é Œ(ì†¡), ç®´(ì ), è©”(ì¡°), éŠ˜(ëª…), è©©ç¾©, ç¦®ç¾©, æ˜“ç¾©, æ›¸ç¾©, åˆ¶(ì œ), æ“¬(ì˜)

í…ìŠ¤íŠ¸: {input}

ë¬¸ì²´ (í•œ ë‹¨ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”):"""
            },
            'retrieval': {
                'system': "ë‹¹ì‹ ì€ í•œêµ­ ê³ ì „ ë¬¸í—Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ì¶œì²˜ë¥¼ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ì„¸ìš”.",
                'user': """ë‹¤ìŒ í•œë¬¸ ë¬¸ì¥ì€ ì–´ëŠ ì±…ì—ì„œ ë‚˜ì˜¨ ê²ƒì¸ì§€ ì‹ë³„í•˜ì„¸ìš”.

ê°€ëŠ¥í•œ ì±…: è«–èª(ë…¼ì–´), å­Ÿå­(ë§¹ì), å¤§å­¸(ëŒ€í•™), ä¸­åº¸(ì¤‘ìš©)

ë¬¸ì¥: {input}

ì¶œì²˜ (ì±… ì´ë¦„ë§Œ ë‹µí•˜ì„¸ìš”):"""
            },
            'punctuation': {
                'system': "ë‹¹ì‹ ì€ í•œêµ­ ê³ ì „ ë¬¸í—Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. êµ¬ë‘ì ì´ ì—†ëŠ” í•œë¬¸(ë°±ë¬¸)ì— ì ì ˆí•œ êµ¬ë‘ì ì„ ì¶”ê°€í•˜ì„¸ìš”.",
                'user': """ë‹¤ìŒ ë°±ë¬¸(êµ¬ë‘ì ì´ ì—†ëŠ” í•œë¬¸)ì— ì ì ˆí•œ êµ¬ë‘ì ì„ ì¶”ê°€í•˜ì„¸ìš”.

ë°±ë¬¸: {input}

êµ¬ë‘ì ì„ ì¶”ê°€í•œ ë¬¸ì¥:"""
            },
            'nli': {
                'system': "ë‹¹ì‹ ì€ í•œêµ­ ê³ ì „ ë¬¸í—Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‘ ë¬¸ì¥ ê°„ì˜ ë…¼ë¦¬ì  ê´€ê³„ë¥¼ íŒë‹¨í•˜ì„¸ìš”.",
                'user': """ë‘ ë¬¸ì¥ ê°„ì˜ ë…¼ë¦¬ì  ê´€ê³„ë¥¼ íŒë‹¨í•˜ì„¸ìš”.

ì „ì œ(Premise): {premise}
ê°€ì„¤(Hypothesis): {hypothesis}

ê´€ê³„:
- entailment: ì „ì œê°€ ì°¸ì´ë©´ ê°€ì„¤ë„ ë°˜ë“œì‹œ ì°¸
- contradiction: ì „ì œì™€ ê°€ì„¤ì´ ëª¨ìˆœ
- neutral: ê´€ê³„ë¥¼ ì•Œ ìˆ˜ ì—†ìŒ

ë‹µë³€ (entailment, contradiction, neutral ì¤‘ í•˜ë‚˜ë§Œ):"""
            },
            'translation': {
                'system': "ë‹¹ì‹ ì€ í•œêµ­ ê³ ì „ ë¬¸í—Œ ë²ˆì—­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                'user': """ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_lang}(ìœ¼)ë¡œ ë²ˆì—­í•˜ì„¸ìš”.

ì›ë¬¸ ({source_lang}): {input}

ë²ˆì—­:"""
            }
        }

    def format_prompt(self, task: str, data: Dict) -> Tuple[str, str]:
        """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        if task == 'nli':
            user_prompt = self.prompts[task]['user'].format(
                premise=data['premise'],
                hypothesis=data['hypothesis']
            )
        elif task == 'translation':
            # ì–¸ì–´ ì´ë¦„ ë§¤í•‘
            lang_names = {
                'classical_chinese': 'í•œë¬¸',
                'korean': 'í•œêµ­ì–´',
                'english': 'ì˜ì–´'
            }
            user_prompt = self.prompts[task]['user'].format(
                source_lang=lang_names.get(data['source_lang'], data['source_lang']),
                target_lang=lang_names.get(data['target_lang'], data['target_lang']),
                input=data['source_text']
            )
        else:
            user_prompt = self.prompts[task]['user'].format(input=data['input'])

        return self.prompts[task]['system'], user_prompt

    def evaluate_classification(self, predictions: List[str], ground_truths: List[str]) -> Dict:
        """ë¶„ë¥˜ íƒœìŠ¤í¬ í‰ê°€"""
        # ì •ê·œí™”: ê³µë°± ì œê±°, ì†Œê´„í˜¸ ë‚´ìš© ì œê±°
        def normalize(text):
            text = re.sub(r'\([^)]*\)', '', text)  # (ë¶€) ê°™ì€ í‘œí˜„ ì œê±°
            text = text.strip()
            return text

        preds_normalized = [normalize(p) for p in predictions]
        truths_normalized = [normalize(t) for t in ground_truths]

        # ë””ë²„ê·¸ ì¶œë ¥ (ì†ŒëŸ‰ ë°ì´í„°ì¼ ë•Œë§Œ)
        if len(predictions) <= 5:
            for i, (pred, truth) in enumerate(zip(preds_normalized, truths_normalized)):
                print(f"    {i+1}. Pred: '{pred}' vs Truth: '{truth}' -> {'âœ“' if pred == truth else 'âœ—'}")

        accuracy = accuracy_score(truths_normalized, preds_normalized)

        # Per-class metrics
        precision, recall, f1, support = precision_recall_fscore_support(
            truths_normalized, preds_normalized, average='weighted', zero_division=0
        )

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'num_samples': len(predictions)
        }

    def evaluate_retrieval(self, predictions: List[str], ground_truths: List[str]) -> Dict:
        """ê²€ìƒ‰ íƒœìŠ¤í¬ í‰ê°€"""
        correct = 0

        for pred, truth in zip(predictions, ground_truths):
            # ì±… ì´ë¦„ ì¶”ì¶œ (ì•ë¶€ë¶„ë§Œ)
            pred_book = pred.strip().split('-')[0].strip()
            truth_book = truth.strip().split('-')[0].strip()

            # ê´„í˜¸ ë‚´ìš© ë¨¼ì € ì œê±°, ê·¸ ë‹¤ìŒ í•œêµ­ì–´â†’í•œì ì •ê·œí™”
            pred_book = re.sub(r'\([^)]*\)', '', pred_book).strip()
            truth_book = re.sub(r'\([^)]*\)', '', truth_book).strip()

            # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (CJK Compatibility Ideographs â†’ í‘œì¤€ í•œì)
            pred_book = unicodedata.normalize('NFKC', pred_book)
            truth_book = unicodedata.normalize('NFKC', truth_book)

            # í•œêµ­ì–´â†’í•œì ì •ê·œí™”
            pred_book = pred_book.replace('ë…¼ì–´', 'è«–èª').replace('ë§¹ì', 'å­Ÿå­').replace('ëŒ€í•™', 'å¤§å­¸').replace('ì¤‘ìš©', 'ä¸­åº¸')
            truth_book = truth_book.replace('ë…¼ì–´', 'è«–èª').replace('ë§¹ì', 'å­Ÿå­').replace('ëŒ€í•™', 'å¤§å­¸').replace('ì¤‘ìš©', 'ä¸­åº¸')

            # ë¶€ë¶„ ë§¤ì¹­ë„ í—ˆìš©
            match_result = (pred_book in truth_book or truth_book in pred_book)
            if match_result:
                correct += 1

            # ë””ë²„ê·¸ ì¶œë ¥ (ì†ŒëŸ‰ ë°ì´í„°ì¼ ë•Œë§Œ)
            if len(predictions) <= 5:
                item_num = len([p for p, t in zip(predictions, ground_truths) if p and t])
                print(f"    {item_num}. Pred: '{pred_book}' vs Truth: '{truth_book}' -> {'âœ“' if match_result else 'âœ—'}")

        accuracy = correct / len(predictions) if predictions else 0

        return {
            'accuracy': accuracy,
            'correct': correct,
            'num_samples': len(predictions)
        }

    def evaluate_punctuation(self, predictions: List[str], ground_truths: List[str]) -> Dict:
        """êµ¬ë‘ì  íƒœìŠ¤í¬ í‰ê°€"""
        # Character-level F1
        total_precision = 0
        total_recall = 0
        total_f1 = 0

        for pred, truth in zip(predictions, ground_truths):
            # ë¬¸ì ë‹¨ìœ„ ë¹„êµ
            pred_chars = set(pred)
            truth_chars = set(truth)

            if len(pred_chars) == 0:
                continue

            intersection = pred_chars & truth_chars

            precision = len(intersection) / len(pred_chars) if pred_chars else 0
            recall = len(intersection) / len(truth_chars) if truth_chars else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

            total_precision += precision
            total_recall += recall
            total_f1 += f1

        n = len(predictions)

        # ROUGE scoreë„ ê³„ì‚°
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)
        rouge_scores = [scorer.score(truth, pred) for pred, truth in zip(predictions, ground_truths)]

        rouge1_f1 = np.mean([s['rouge1'].fmeasure for s in rouge_scores])
        rouge2_f1 = np.mean([s['rouge2'].fmeasure for s in rouge_scores])
        rougeL_f1 = np.mean([s['rougeL'].fmeasure for s in rouge_scores])

        return {
            'char_precision': total_precision / n if n > 0 else 0,
            'char_recall': total_recall / n if n > 0 else 0,
            'char_f1': total_f1 / n if n > 0 else 0,
            'rouge1_f1': rouge1_f1,
            'rouge2_f1': rouge2_f1,
            'rougeL_f1': rougeL_f1,
            'num_samples': n
        }

    def evaluate_nli(self, predictions: List[str], ground_truths: List[str]) -> Dict:
        """NLI íƒœìŠ¤í¬ í‰ê°€"""
        # ì •ê·œí™”
        def normalize(text):
            text = text.lower().strip()
            # í•œê¸€ë„ ì²˜ë¦¬
            mapping = {
                'í•¨ì˜': 'entailment',
                'ëª¨ìˆœ': 'contradiction',
                'ì¤‘ë¦½': 'neutral',
                'ë¬´ê´€': 'neutral'
            }
            for k, v in mapping.items():
                text = text.replace(k, v)
            return text

        preds_normalized = [normalize(p) for p in predictions]
        truths_normalized = [normalize(t) for t in ground_truths]

        accuracy = accuracy_score(truths_normalized, preds_normalized)

        # Per-class metrics
        precision, recall, f1, support = precision_recall_fscore_support(
            truths_normalized, preds_normalized, average='weighted', zero_division=0
        )

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'num_samples': len(predictions)
        }

    def evaluate_translation(self, predictions: List[str], ground_truths: List[str]) -> Dict:
        """ë²ˆì—­ íƒœìŠ¤í¬ í‰ê°€"""
        bleu_scores = []
        rouge_scores_list = []

        # BLEU ê³„ì‚°
        smoothing = SmoothingFunction().method1

        for pred, truth in zip(predictions, ground_truths):
            # BLEU (character-level for Korean/Chinese)
            pred_chars = list(pred)
            truth_chars = list(truth)

            if len(pred_chars) > 0 and len(truth_chars) > 0:
                bleu = sentence_bleu([truth_chars], pred_chars, smoothing_function=smoothing)
                bleu_scores.append(bleu)

        # ROUGE ê³„ì‚°
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)
        for pred, truth in zip(predictions, ground_truths):
            score = scorer.score(truth, pred)
            rouge_scores_list.append(score)

        rouge1_f1 = np.mean([s['rouge1'].fmeasure for s in rouge_scores_list])
        rouge2_f1 = np.mean([s['rouge2'].fmeasure for s in rouge_scores_list])
        rougeL_f1 = np.mean([s['rougeL'].fmeasure for s in rouge_scores_list])

        return {
            'bleu': np.mean(bleu_scores) if bleu_scores else 0,
            'rouge1_f1': rouge1_f1,
            'rouge2_f1': rouge2_f1,
            'rougeL_f1': rougeL_f1,
            'num_samples': len(predictions)
        }

    def evaluate_task(self, task_name: str, predictions: List[str], task_data: Dict) -> Dict:
        """ë‹¨ì¼ íƒœìŠ¤í¬ í‰ê°€"""
        # Ground truth ì¶”ì¶œ
        if task_name == 'classification':
            ground_truths = [item['label'] for item in task_data['data']]
        elif task_name == 'retrieval':
            ground_truths = [item['answer'] for item in task_data['data']]
        elif task_name == 'punctuation':
            ground_truths = [item['answer'] for item in task_data['data']]
        elif task_name == 'nli':
            ground_truths = [item['label'] for item in task_data['data']]
        elif task_name == 'translation':
            ground_truths = [item['target_text'] for item in task_data['data']]
        else:
            raise ValueError(f"Unknown task: {task_name}")

        # í‰ê°€
        if task_name == 'classification':
            return self.evaluate_classification(predictions, ground_truths)
        elif task_name == 'retrieval':
            return self.evaluate_retrieval(predictions, ground_truths)
        elif task_name == 'punctuation':
            return self.evaluate_punctuation(predictions, ground_truths)
        elif task_name == 'nli':
            return self.evaluate_nli(predictions, ground_truths)
        elif task_name == 'translation':
            return self.evaluate_translation(predictions, ground_truths)

    def run_evaluation(self, model, model_name: str) -> Dict:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹¤í–‰"""
        print(f"\n{'='*70}")
        print(f"ğŸš€ ëª¨ë¸ í‰ê°€ ì‹œì‘: {model_name}")
        print(f"{'='*70}\n")

        results = {
            'model_name': model_name,
            'model_type': self.model_type,
            'benchmark_version': self.benchmark['benchmark_info']['version'],
            'tasks': {}
        }

        for task_name, task_data in self.benchmark['tasks'].items():
            print(f"\nğŸ“Š [{task_name.upper()}] í‰ê°€ ì¤‘... ({task_data['size']}ê°œ ìƒ˜í”Œ)")

            predictions = []

            for item in tqdm(task_data['data'], desc=f"  Processing {task_name}"):
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt, user_prompt = self.format_prompt(task_name, item)

                # ëª¨ë¸ ì¶”ë¡ 
                try:
                    prediction = model.generate(system_prompt, user_prompt)
                    if not prediction or prediction.strip() == "":
                        print(f"  âš ï¸  Empty prediction for item {len(predictions)+1}")
                    predictions.append(prediction)
                except Exception as e:
                    print(f"  âŒ Model generation error: {e}")
                    predictions.append("")

                # API í˜¸ì¶œ ì œí•œ ëŒ€ì‘
                if self.model_type == 'api':
                    time.sleep(1.0)  # Rate limiting - increased for GPT-4

            # í‰ê°€
            metrics = self.evaluate_task(task_name, predictions, task_data)

            results['tasks'][task_name] = {
                'metrics': metrics,
                'predictions': predictions[:10]  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥
            }

            print(f"  âœ“ ì™„ë£Œ")
            self.print_task_results(task_name, metrics)

        # ê²°ê³¼ ì €ì¥
        self.save_results(results, model_name)

        return results

    def print_task_results(self, task_name: str, metrics: Dict):
        """íƒœìŠ¤í¬ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n  ğŸ“ˆ {task_name.upper()} ê²°ê³¼:")

        if task_name == 'classification':
            print(f"    - Accuracy: {metrics['accuracy']:.4f}")
            print(f"    - F1 Score: {metrics['f1']:.4f}")

        elif task_name == 'retrieval':
            print(f"    - Accuracy: {metrics['accuracy']:.4f}")
            print(f"    - Correct: {metrics['correct']}/{metrics['num_samples']}")

        elif task_name == 'punctuation':
            print(f"    - Character F1: {metrics['char_f1']:.4f}")
            print(f"    - ROUGE-L F1: {metrics['rougeL_f1']:.4f}")

        elif task_name == 'nli':
            print(f"    - Accuracy: {metrics['accuracy']:.4f}")
            print(f"    - F1 Score: {metrics['f1']:.4f}")

        elif task_name == 'translation':
            print(f"    - BLEU: {metrics['bleu']:.4f}")
            print(f"    - ROUGE-L F1: {metrics['rougeL_f1']:.4f}")

    def save_results(self, results: Dict, model_name: str):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')

        # JSON ì €ì¥
        json_path = self.output_dir / f"results_{model_name}_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {json_path}")

        # CSV ìš”ì•½ ì €ì¥
        summary_data = []
        for task_name, task_results in results['tasks'].items():
            row = {
                'model': model_name,
                'task': task_name,
                **task_results['metrics']
            }
            summary_data.append(row)

        df = pd.DataFrame(summary_data)
        csv_path = self.output_dir / f"summary_{model_name}_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        print(f"ğŸ’¾ ìš”ì•½ ì €ì¥: {csv_path}")


# ============================================================================
# ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤
# ============================================================================

class BaseModelWrapper:
    """ê¸°ë³¸ ëª¨ë¸ ë˜í¼"""

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        raise NotImplementedError


class OpenAIWrapper(BaseModelWrapper):
    """OpenAI API ë˜í¼ (GPT-4, GPT-3.5 ë“±)"""

    def __init__(self, model_name: str = "gpt-4", api_key: Optional[str] = None):
        import openai
        self.model_name = model_name
        self.client = openai.OpenAI(api_key=api_key)

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=500
            )
            content = response.choices[0].message.content
            if content is None:
                print(f"âš ï¸  Warning: Empty response from {self.model_name}")
                return ""
            return content.strip()
        except Exception as e:
            print(f"âŒ OpenAI API Error: {e}")
            print(f"   Model: {self.model_name}")
            print(f"   System: {system_prompt[:50]}...")
            print(f"   User: {user_prompt[:50]}...")
            return ""


class AnthropicWrapper(BaseModelWrapper):
    """Anthropic Claude API ë˜í¼"""

    def __init__(self, model_name: str = "claude-3-5-sonnet-20241022", api_key: Optional[str] = None):
        import anthropic
        self.model_name = model_name
        self.client = anthropic.Anthropic(api_key=api_key)

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        try:
            response = self.client.messages.create(
                model=self.model_name,
                max_tokens=500,
                temperature=0.0,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            return response.content[0].text.strip()
        except Exception as e:
            print(f"Error: {e}")
            return ""


class HuggingFaceWrapper(BaseModelWrapper):
    """HuggingFace ëª¨ë¸ ë˜í¼ (Llama, Qwen, EXAONE ë“±)"""

    def __init__(self, model_name: str, device: str = "cuda"):
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch

        self.device = device
        self.model_name = model_name

        print(f"Loading {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto"
        )
        print(f"âœ“ Model loaded")

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        # Chat template ì‚¬ìš©
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            # Apply chat template
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            # Fallback: simple concatenation
            prompt = f"{system_prompt}\n\n{user_prompt}"

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=500,
            temperature=0.0,
            do_sample=False
        )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Remove prompt from response
        response = response[len(prompt):].strip()

        return response


class TonguWrapper(BaseModelWrapper):
    """Tongu ëª¨ë¸ ë˜í¼"""

    def __init__(self, model_path: str):
        # Tongu ëª¨ë¸ ë¡œë“œ ë¡œì§
        # TODO: ì‹¤ì œ ëª¨ë¸ ë¡œë“œ êµ¬í˜„
        self.model_path = model_path
        print(f"âš ï¸  Tongu wrapper - êµ¬í˜„ í•„ìš”: {model_path}")

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        # TODO: Tongu ëª¨ë¸ ì¶”ë¡  êµ¬í˜„
        return ""


class GwenBertWrapper(BaseModelWrapper):
    """GwenBert ëª¨ë¸ ë˜í¼"""

    def __init__(self, model_path: str):
        # GwenBert ëª¨ë¸ ë¡œë“œ ë¡œì§
        # TODO: ì‹¤ì œ ëª¨ë¸ ë¡œë“œ êµ¬í˜„
        self.model_path = model_path
        print(f"âš ï¸  GwenBert wrapper - êµ¬í˜„ í•„ìš”: {model_path}")

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        # TODO: GwenBert ëª¨ë¸ ì¶”ë¡  êµ¬í˜„
        return ""


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description='K-ClassicBench Evaluation')
    parser.add_argument('--benchmark', type=str,
                       default='/Users/songhune/Workspace/korean_eda/benchmark/k_classic_bench/k_classic_bench_full.json',
                       help='ë²¤ì¹˜ë§ˆí¬ JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', type=str,
                       default='/Users/songhune/Workspace/korean_eda/benchmark/results',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--model-type', type=str, choices=['api', 'opensource', 'supervised'],
                       default='api', help='ëª¨ë¸ íƒ€ì…')
    parser.add_argument('--model-name', type=str, required=True,
                       help='ëª¨ë¸ ì´ë¦„ (ì˜ˆ: gpt-4, claude-3-5-sonnet, meta-llama/Llama-3.1-8B)')
    parser.add_argument('--api-key', type=str, default=None,
                       help='API í‚¤ (API ëª¨ë¸ ì‚¬ìš©ì‹œ)')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='íƒœìŠ¤í¬ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)')
    parser.add_argument('--sample-ratio', type=float, default=None,
                       help='ìƒ˜í”Œë§ ë¹„ìœ¨ (0.0~1.0, ì˜ˆ: 0.3=30%%)')

    args = parser.parse_args()

    # Evaluator ì´ˆê¸°í™”
    evaluator = KClassicBenchEvaluator(
        benchmark_path=args.benchmark,
        output_dir=args.output,
        model_type=args.model_type,
        max_samples_per_task=args.max_samples,
        sample_ratio=args.sample_ratio
    )

    # ëª¨ë¸ ì´ˆê¸°í™”
    if args.model_type == 'api':
        if 'gpt' in args.model_name.lower():
            model = OpenAIWrapper(model_name=args.model_name, api_key=args.api_key)
        elif 'claude' in args.model_name.lower():
            model = AnthropicWrapper(model_name=args.model_name, api_key=args.api_key)
        else:
            raise ValueError(f"Unknown API model: {args.model_name}")

    elif args.model_type == 'opensource':
        model = HuggingFaceWrapper(model_name=args.model_name)

    elif args.model_type == 'supervised':
        if 'tongu' in args.model_name.lower():
            model = TonguWrapper(model_path=args.model_name)
        elif 'gwenbert' in args.model_name.lower():
            model = GwenBertWrapper(model_path=args.model_name)
        else:
            raise ValueError(f"Unknown supervised model: {args.model_name}")

    # í‰ê°€ ì‹¤í–‰
    results = evaluator.run_evaluation(model, args.model_name)

    print("\n" + "="*70)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("="*70)


if __name__ == "__main__":
    main()
