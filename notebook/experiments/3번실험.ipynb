{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 3ë²ˆ ì‹¤í—˜: ê³¼ì‹œ ë°ì´í„° Abstract/Content ê·œì¹™ ê²€ì¦\n",
    "\n",
    "## ëª©ì \n",
    "ê³¼ì‹œ ë°ì´í„°ì˜ abstractì™€ content í•„ë“œ ì…ë ¥ ê·œì¹™ì„ ê²€ì¦:\n",
    "- **ê·œì¹™ 1**: ì‹œ/ë¶€ ë“± ìš´ë¬¸ì´ë‚˜ ì§§ì€ ë¬¸ì œ â†’ abstract = content\n",
    "- **ê·œì¹™ 2**: ç–‘/ç¾©/ç­– ë“± ê¸´ ë¬¸ì œ â†’ abstract ë¹„ì–´ìˆê³ , contentì— ì „ë¬¸\n",
    "\n",
    "## ë¶„ì„ ë‚´ìš©\n",
    "1. Abstract/Content ê¸¸ì´ ë¶„ì„ (ì¹´í…Œê³ ë¦¬ë³„)\n",
    "2. Abstract=Content ì¼ì¹˜ìœ¨ (ì¹´í…Œê³ ë¦¬ë³„)\n",
    "3. Anthropic APIë¥¼ ì‚¬ìš©í•œ ë²ˆì—­ (í•œêµ­ì–´/ì˜ì–´)\n",
    "4. ê·œì¹™ ì¤€ìˆ˜ìœ¨ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500c8fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì™„ë£Œ] Answer ì œê±° ë²„ì „ triples ì €ì¥: /Users/songhune/Workspace/korean_eda/notebook/eda_outputs/1ë²ˆì‹¤í—˜/links_fix/triples_no_answer.jsonl\n"
     ]
    }
   ],
   "source": [
    "# SPO ì¬êµ¬ì¶•(ì •ì •íŒ): Answer ì œê±°, Question í…ìŠ¤íŠ¸ 3ì¢…ì„ ì†ì„±ìœ¼ë¡œ ë§¤í•‘\n",
    "from pathlib import Path\n",
    "import pandas as pd, json, re, hashlib\n",
    "NB_DIR = Path(\"..\").resolve()\n",
    "DATA   = NB_DIR.parent / \"data\" / \"gwashi_utf8.csv\"            # ë²ˆì—­ ì „ ì›ë³¸ì´ë©´ ê·¸ëŒ€ë¡œ, ë²ˆì—­ë³¸ì´ë©´ íŒŒì¼ëª…ë§Œ ë°”ê¿” ì¨ë„ OK\n",
    "OUT    = NB_DIR / \"eda_outputs\" / \"1ë²ˆì‹¤í—˜\" / \"links_fix\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "TRIPLES = OUT / \"triples_no_answer.jsonl\"\n",
    "\n",
    "def clean(x): \n",
    "    if pd.isna(x): return \"\"\n",
    "    return re.sub(r\"\\s+\",\" \", str(x).strip())\n",
    "def hid(pfx,*vals):\n",
    "    s = \"||\".join(clean(v) for v in vals)\n",
    "    return pfx + hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "df = pd.read_csv(DATA)\n",
    "\n",
    "def time_id(r):\n",
    "    y,m,d = clean(r.get(\"year\",\"\")), clean(r.get(\"month\",\"\")), clean(r.get(\"day\",\"\"))\n",
    "    if y: return hid(\"T\", y,m,d, r.get(\"ganji_kr_year\",\"\"), r.get(\"ganji_kr_month\",\"\"), r.get(\"ganji_kr_day\",\"\"))\n",
    "    return hid(\"T\", m,d)\n",
    "def exam_id(r):\n",
    "    return hid(\"E\", r.get(\"year\",\"\"), r.get(\"sortC\",\"\"), r.get(\"sortD\",\"\"), r.get(\"sortE\",\"\"), r.get(\"name_exam\",\"\"))\n",
    "def question_id(r):\n",
    "    return hid(\"Q\", r.get(\"year\",\"\"), r.get(\"name_exam\",\"\"), r.get(\"name_question\",\"\"), r.get(\"category\",\"\"), r.get(\"category2\",\"\"))\n",
    "\n",
    "with open(TRIPLES, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, r in enumerate(df.to_dict(orient=\"records\")):\n",
    "        e = exam_id(r); t = time_id(r); q = question_id(r)\n",
    "        triples = [\n",
    "            {\"s\": e, \"p\":\"isHeldOn\", \"o\": t, \"o_type\":\"id\"},\n",
    "            {\"s\": q, \"p\":\"isPartOf\", \"o\": e, \"o_type\":\"id\"},\n",
    "        ]\n",
    "        # Question í…ìŠ¤íŠ¸ 3ì¢…\n",
    "        ab = clean(r.get(\"abstract\",\"\"))\n",
    "        co = clean(r.get(\"contents\",\"\"))\n",
    "        de = clean(r.get(\"description\",\"\"))\n",
    "        if ab: triples.append({\"s\": q, \"p\":\"hasAbstract\", \"o\": ab, \"o_type\":\"lit\"})\n",
    "        if co: triples.append({\"s\": q, \"p\":\"hasContent\",  \"o\": co, \"o_type\":\"lit\"})\n",
    "        if de: triples.append({\"s\": q, \"p\":\"hasDescription\",\"o\": de, \"o_type\":\"lit\"})\n",
    "        # ë²”ì£¼/ì„œë¸Œë²”ì£¼\n",
    "        c1, c2 = clean(r.get(\"category\",\"\")), clean(r.get(\"category2\",\"\"))\n",
    "        if c1: triples.append({\"s\": q, \"p\":\"hasCategory\", \"o\": c1, \"o_type\":\"lit\"})\n",
    "        if c2: triples.append({\"s\": q, \"p\":\"hasSubcategory\", \"o\": c2, \"o_type\":\"lit\"})\n",
    "        # Exam ë¶„ë¥˜/ì¶œì²˜\n",
    "        for p,v in [(\"hasTypeA\",\"sortA\"),(\"hasTypeB\",\"sortB\"),(\"hasCategory\",\"sortC\"),\n",
    "                    (\"hasStage\",\"sortD\"),(\"hasRound\",\"sortE\")]:\n",
    "            vv = clean(r.get(v,\"\"))\n",
    "            if vv: triples.append({\"s\": e, \"p\":p, \"o\": vv, \"o_type\":\"lit\"})\n",
    "        # Time ë¦¬í„°ëŸ´\n",
    "        for p,v in [(\"year\",\"year\"),(\"month\",\"month\"),(\"day\",\"day\")]:\n",
    "            vv = clean(r.get(v,\"\"))\n",
    "            if vv: triples.append({\"s\": t, \"p\":p, \"o\": vv, \"o_type\":\"lit\"})\n",
    "        kr = \"-\".join([clean(r.get(\"ganji_kr_year\",\"\")), clean(r.get(\"ganji_kr_month\",\"\")), clean(r.get(\"ganji_kr_day\",\"\"))]).strip(\"-\")\n",
    "        cn = \"-\".join([clean(r.get(\"ganji_cn_year\",\"\")), clean(r.get(\"ganji_cn_month\",\"\")), clean(r.get(\"ganji_cn_day\",\"\"))]).strip(\"-\")\n",
    "        if kr: triples.append({\"s\": t, \"p\":\"sexagenaryKR\", \"o\": kr, \"o_type\":\"lit\"})\n",
    "        if cn: triples.append({\"s\": t, \"p\":\"sexagenaryCN\", \"o\": cn, \"o_type\":\"lit\"})\n",
    "        # ì¶œì²˜\n",
    "        src, url = clean(r.get(\"source\",\"\")), clean(r.get(\"URL\",\"\"))\n",
    "        if src: triples.append({\"s\": e, \"p\":\"isRecordedIn\", \"o\": src, \"o_type\":\"lit\"})\n",
    "        if url: triples.append({\"s\": e, \"p\":\"hasRecordURL\", \"o\": url, \"o_type\":\"lit\"})\n",
    "        if src: triples.append({\"s\": q, \"p\":\"hasSource\", \"o\": src, \"o_type\":\"lit\"})\n",
    "        if url: triples.append({\"s\": q, \"p\":\"hasSourceURL\", \"o\": url, \"o_type\":\"lit\"})\n",
    "        rec = {\"row_index\": i, \"exam\":{\"id\":e,\"name\":clean(r.get(\"name_exam\",\"\"))},\n",
    "               \"time\":{\"id\":t}, \"question\":{\"id\":q,\"name\":clean(r.get(\"name_question\",\"\"))},\n",
    "               \"triples\": triples}\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"[ì™„ë£Œ] Answer ì œê±° ë²„ì „ triples ì €ì¥: {TRIPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í°íŠ¸ ì„¤ì •: AppleGothic\n",
      "âœ… í°íŠ¸ ì„¤ì •: AppleGothic\n",
      "âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í°íŠ¸ ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (ê°•í™” ë²„ì „)\n",
    "import platform\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def setup_korean_fonts():\n",
    "    \"\"\"í•œê¸€/í•œì í°íŠ¸ ê°•ì œ ì„¤ì • (matplotlib + seaborn ëŒ€ì‘)\"\"\"\n",
    "    system = platform.system()\n",
    "    \n",
    "    if system == 'Darwin':\n",
    "        font_candidates = ['AppleGothic', 'Apple SD Gothic Neo', 'AppleMyungjo']\n",
    "    elif system == 'Windows':\n",
    "        font_candidates = ['Malgun Gothic', 'Gulim', 'Batang']\n",
    "    else:\n",
    "        font_candidates = ['NanumGothic', 'Noto Sans CJK KR']\n",
    "    \n",
    "    available_fonts = {f.name for f in fm.fontManager.ttflist}\n",
    "    \n",
    "    selected_font = None\n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            selected_font = font\n",
    "            break\n",
    "    \n",
    "    if not selected_font:\n",
    "        print(\"âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return None\n",
    "    \n",
    "    # ê°•ì œ ì„¤ì • (3ê°€ì§€ ë°©ë²• ë™ì‹œ ì ìš©)\n",
    "    matplotlib.rcParams['font.family'] = selected_font\n",
    "    matplotlib.rcParams['font.sans-serif'] = [selected_font]\n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    plt.rcParams['font.family'] = selected_font\n",
    "    plt.rcParams['font.sans-serif'] = [selected_font]\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    plt.rc('font', family=selected_font)\n",
    "    \n",
    "    print(f\"âœ… í°íŠ¸ ì„¤ì •: {selected_font}\")\n",
    "    return selected_font\n",
    "\n",
    "# í°íŠ¸ ë¨¼ì € ì„¤ì •\n",
    "setup_korean_fonts()\n",
    "\n",
    "# seaborn ì„¤ì • (í°íŠ¸ë¥¼ ë¦¬ì…‹í•  ìˆ˜ ìˆìŒ)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# seaborn ì„¤ì • í›„ í°íŠ¸ ë‹¤ì‹œ ì ìš©! (ì¤‘ìš”)\n",
    "setup_korean_fonts()\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í°íŠ¸ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "load_env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Anthropic API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEYê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "print(\"âœ… Anthropic API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 3348 ê°œì˜ ë¬¸ì œ ë¡œë“œ\n",
      "\n",
      "ì»¬ëŸ¼: ['question_id', 'abstract', 'content', 'category', 'subcategory']\n",
      "\n",
      "ì¹´í…Œê³ ë¦¬ ë¶„í¬:\n",
      "category\n",
      "è³¦       749\n",
      "è¡¨       443\n",
      "è©©       424\n",
      "ç–‘       415\n",
      "ç¾©       365\n",
      "ç­–       292\n",
      "è«–        56\n",
      "éŠ˜        53\n",
      "ç®‹        49\n",
      "é Œ        25\n",
      "í™•ì¸ë¶ˆê°€     23\n",
      "ç®´        14\n",
      "æ˜“ç¾©        9\n",
      "è©”         7\n",
      "è©©ç¾©        7\n",
      "ç¦®ç¾©        7\n",
      "æ›¸ç¾©        6\n",
      "ï¦¶ç¾©        6\n",
      "åˆ¶         3\n",
      "è¬›         2\n",
      "ï¥         2\n",
      "æ“¬         2\n",
      "è£½è¿°        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "#BASE = \"/Users/songhune/Workspace/korean_eda/notebook/experiments\"\n",
    "IN_JSONL = \"/Users/songhune/Workspace/korean_eda/notebook/experiments/triples_no_answer.jsonl\"\n",
    "OUT_DIR = \"/Users/songhune/Workspace/korean_eda/results/\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# JSONL íŒŒì‹±\n",
    "questions = []\n",
    "with open(IN_JSONL, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            data = json.loads(line)\n",
    "            q_data = {\"question_id\": data[\"question\"][\"id\"]}\n",
    "            \n",
    "            # triplesì—ì„œ ì •ë³´ ì¶”ì¶œ\n",
    "            for triple in data[\"triples\"]:\n",
    "                if triple[\"s\"].startswith(\"Q\"):\n",
    "                    if triple[\"p\"] == \"hasAbstract\":\n",
    "                        q_data[\"abstract\"] = triple[\"o\"]\n",
    "                    elif triple[\"p\"] == \"hasContent\":\n",
    "                        q_data[\"content\"] = triple[\"o\"]\n",
    "                    elif triple[\"p\"] == \"hasCategory\":\n",
    "                        q_data[\"category\"] = triple[\"o\"]\n",
    "                    elif triple[\"p\"] == \"hasSubcategory\":\n",
    "                        q_data[\"subcategory\"] = triple[\"o\"]\n",
    "            \n",
    "            questions.append(q_data)\n",
    "\n",
    "df = pd.DataFrame(questions)\n",
    "print(f\"âœ… ì´ {len(df)} ê°œì˜ ë¬¸ì œ ë¡œë“œ\")\n",
    "print(f\"\\nì»¬ëŸ¼: {df.columns.tolist()}\")\n",
    "print(f\"\\nì¹´í…Œê³ ë¦¬ ë¶„í¬:\\n{df['category'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "analyze_patterns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ Abstract/Content ì…ë ¥ íŒ¨í„´:\n",
      "================================================================================\n",
      "pattern   Abstract_Only  Both_Diff  Both_Same  Content_Only  Neither\n",
      "category                                                            \n",
      "è³¦                     8          0        178           557        6\n",
      "è¡¨                    28          1         64           341        9\n",
      "è©©                     1          8        110           304        1\n",
      "ç–‘                     0          0          4           405        6\n",
      "ç¾©                     1          0        111           251        2\n",
      "ç­–                    30         29         18           162       53\n",
      "è«–                     9          0         15            27        5\n",
      "éŠ˜                     0          0         24            29        0\n",
      "ç®‹                     6          1          2            40        0\n",
      "é Œ                     8          0          5            11        1\n",
      "í™•ì¸ë¶ˆê°€                  0          0          0             0       23\n",
      "ç®´                     2          0          0            10        2\n",
      "\n",
      "\n",
      "ğŸ“ˆ ì „ì²´ íŒ¨í„´ ë¶„í¬:\n",
      "================================================================================\n",
      "Content_Only        : 2204 ( 65.8%)\n",
      "Both_Same           :  533 ( 15.9%)\n",
      "Neither             :  478 ( 14.3%)\n",
      "Abstract_Only       :   94 (  2.8%)\n",
      "Both_Diff           :   39 (  1.2%)\n",
      "\n",
      "ğŸ’¡ íŒ¨í„´ í•´ì„:\n",
      "  - Both_Same: Abstract=Content (ì§§ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì…ë ¥)\n",
      "  - Both_Diff: Abstractâ‰ Content (Abstract=ì œëª©/ì£¼ì œ, Content=ì „ë¬¸)\n",
      "  - Content_Only: Contentë§Œ ìˆìŒ (ê·œì¹™: ê¸´ ë‚´ìš©ì€ Abstract ë¯¸ì…ë ¥)\n",
      "  - Abstract_Only: Abstractë§Œ ìˆìŒ (Content ëˆ„ë½?)\n",
      "  - Neither: ë‘˜ ë‹¤ ì—†ìŒ (ë°ì´í„° ëˆ„ë½)\n"
     ]
    }
   ],
   "source": [
    "# 1. Abstract/Content ì…ë ¥ íŒ¨í„´ ë¶„ì„ (ì˜¬ë°”ë¥¸ ë¶„ì„)\n",
    "df['abstract_len'] = df['abstract'].fillna('').str.len()\n",
    "df['content_len'] = df['content'].fillna('').str.len()\n",
    "df['has_abstract'] = df['abstract'].notna() & (df['abstract'] != '')\n",
    "df['has_content'] = df['content'].notna() & (df['content'] != '')\n",
    "df['abstract_eq_content'] = df.apply(\n",
    "    lambda x: x['abstract'] == x['content'] if (x['has_abstract'] and x['has_content']) else False,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ì…ë ¥ íŒ¨í„´ ë¶„ë¥˜\n",
    "def classify_pattern(row):\n",
    "    \"\"\"ê° ë¬¸ì œì˜ ì…ë ¥ íŒ¨í„´ ë¶„ë¥˜\"\"\"\n",
    "    has_a = row['has_abstract']\n",
    "    has_c = row['has_content']\n",
    "    same = row['abstract_eq_content']\n",
    "    \n",
    "    if has_a and has_c:\n",
    "        if same:\n",
    "            return 'Both_Same'  # ì§§ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ\n",
    "        else:\n",
    "            return 'Both_Diff'  # Abstract=ì œëª©, Content=ì „ë¬¸\n",
    "    elif has_a and not has_c:\n",
    "        return 'Abstract_Only'\n",
    "    elif not has_a and has_c:\n",
    "        return 'Content_Only'  # ê·œì¹™: ê¸´ ë‚´ìš©ì€ Abstract ë¯¸ì…ë ¥\n",
    "    else:\n",
    "        return 'Neither'\n",
    "\n",
    "df['pattern'] = df.apply(classify_pattern, axis=1)\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ íŒ¨í„´ í†µê³„\n",
    "pattern_stats = df.groupby(['category', 'pattern']).size().unstack(fill_value=0)\n",
    "\n",
    "# ì£¼ìš” ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§ (ìƒìœ„ 12ê°œ)\n",
    "top_categories = df['category'].value_counts().head(12).index\n",
    "pattern_stats_top = pattern_stats.loc[top_categories]\n",
    "\n",
    "print(\"\\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ Abstract/Content ì…ë ¥ íŒ¨í„´:\")\n",
    "print(\"=\"*80)\n",
    "print(pattern_stats_top)\n",
    "\n",
    "# íŒ¨í„´ë³„ í†µê³„\n",
    "print(\"\\n\\nğŸ“ˆ ì „ì²´ íŒ¨í„´ ë¶„í¬:\")\n",
    "print(\"=\"*80)\n",
    "pattern_total = df['pattern'].value_counts()\n",
    "for pattern, count in pattern_total.items():\n",
    "    print(f\"{pattern:20s}: {count:4d} ({count/len(df)*100:5.1f}%)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ íŒ¨í„´ í•´ì„:\")\n",
    "print(\"  - Both_Same: Abstract=Content (ì§§ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì…ë ¥)\")\n",
    "print(\"  - Both_Diff: Abstractâ‰ Content (Abstract=ì œëª©/ì£¼ì œ, Content=ì „ë¬¸)\")\n",
    "print(\"  - Content_Only: Contentë§Œ ìˆìŒ (ê·œì¹™: ê¸´ ë‚´ìš©ì€ Abstract ë¯¸ì…ë ¥)\")\n",
    "print(\"  - Abstract_Only: Abstractë§Œ ìˆìŒ (Content ëˆ„ë½?)\")\n",
    "print(\"  - Neither: ë‘˜ ë‹¤ ì—†ìŒ (ë°ì´í„° ëˆ„ë½)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_lengths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ì‹œê°í™” 1: ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ê¸¸ì´ ë¹„êµ\n",
    "# ê·¸ë˜í”„ ìƒì„± ì§ì „ í°íŠ¸ ì¬í™•ì¸\n",
    "setup_korean_fonts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Abstract ê¸¸ì´\n",
    "df_plot = df[df['has_abstract']].groupby('category')['abstract_len'].mean().sort_values(ascending=False)\n",
    "axes[0].bar(range(len(df_plot)), df_plot.values, color='steelblue')\n",
    "axes[0].set_xticks(range(len(df_plot)))\n",
    "axes[0].set_xticklabels(df_plot.index, rotation=45, ha='right', fontsize=12)\n",
    "axes[0].set_ylabel('Average Length (characters)', fontsize=16)\n",
    "axes[0].set_title('Average Abstract Length by Category', fontsize=18)\n",
    "axes[0].tick_params(axis='y', labelsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Content ê¸¸ì´\n",
    "df_plot2 = df[df['has_content']].groupby('category')['content_len'].mean().sort_values(ascending=False)\n",
    "axes[1].bar(range(len(df_plot2)), df_plot2.values, color='coral')\n",
    "axes[1].set_xticks(range(len(df_plot2)))\n",
    "axes[1].set_xticklabels(df_plot2.index, rotation=45, ha='right', fontsize=12)\n",
    "axes[1].set_ylabel('Average Length (characters)', fontsize=16)\n",
    "axes[1].set_title('Average Content Length by Category', fontsize=18)\n",
    "axes[1].tick_params(axis='y', labelsize=12)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# í†µí•© ê·¸ë˜í”„ ì €ì¥\n",
    "plt.savefig(os.path.join(OUT_DIR, \"C1_category_length_comparison.pdf\"), dpi=200, bbox_inches='tight', format='pdf')\n",
    "print(\"âœ… ì €ì¥: C1_category_length_comparison.pdf (í†µí•©)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ê°œë³„ ê·¸ë˜í”„ ì €ì¥\n",
    "# Abstract ê·¸ë˜í”„ë§Œ\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 5))\n",
    "ax1.bar(range(len(df_plot)), df_plot.values, color='steelblue')\n",
    "ax1.set_xticks(range(len(df_plot)))\n",
    "ax1.set_xticklabels(df_plot.index, rotation=45, ha='right', fontsize=12)\n",
    "ax1.set_ylabel('Average Length (characters)', fontsize=16)\n",
    "ax1.set_title('Average Abstract Length by Category', fontsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"C1a_abstract_length_by_category.pdf\"), dpi=200, bbox_inches='tight', format='pdf')\n",
    "plt.close()\n",
    "print(\"âœ… ì €ì¥: C1a_abstract_length_by_category.pdf (Abstractë§Œ)\")\n",
    "\n",
    "# Content ê·¸ë˜í”„ë§Œ\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 5))\n",
    "ax2.bar(range(len(df_plot2)), df_plot2.values, color='coral')\n",
    "ax2.set_xticks(range(len(df_plot2)))\n",
    "ax2.set_xticklabels(df_plot2.index, rotation=45, ha='right', fontsize=12)\n",
    "ax2.set_ylabel('Average Length (characters)', fontsize=16)\n",
    "ax2.set_title('Average Content Length by Category', fontsize=18)\n",
    "ax2.tick_params(axis='y', labelsize=12)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"C1b_content_length_by_category.pdf\"), dpi=200, bbox_inches='tight', format='pdf')\n",
    "plt.close()\n",
    "print(\"âœ… ì €ì¥: C1b_content_length_by_category.pdf (Contentë§Œ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tuthoswph0h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” Content_Only ì¼€ì´ìŠ¤ ë¶„ì„ (Abstract ë¯¸ì…ë ¥)\n",
      "================================================================================\n",
      "\n",
      "ì´ 2204ê°œ (ì „ì²´ì˜ 65.8%)\n",
      "\n",
      "ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:\n",
      "  è³¦       :  557 /  749 ( 74.4%)\n",
      "  ç–‘       :  405 /  415 ( 97.6%)\n",
      "  è¡¨       :  341 /  443 ( 77.0%)\n",
      "  è©©       :  304 /  424 ( 71.7%)\n",
      "  ç¾©       :  251 /  365 ( 68.8%)\n",
      "  ç­–       :  162 /  292 ( 55.5%)\n",
      "  ç®‹       :   40 /   49 ( 81.6%)\n",
      "  éŠ˜       :   29 /   53 ( 54.7%)\n",
      "  è«–       :   27 /   56 ( 48.2%)\n",
      "  é Œ       :   11 /   25 ( 44.0%)\n",
      "\n",
      "ğŸ“ Content ê¸¸ì´ ë¶„ì„:\n",
      "  - í‰ê· : 37.3 ê¸€ì\n",
      "  - ì¤‘ê°„ê°’: 11.0 ê¸€ì\n",
      "  - ìµœì†Œ: 1 ê¸€ì\n",
      "  - ìµœëŒ€: 1358 ê¸€ì\n",
      "\n",
      "ğŸ”„ ë¹„êµ: Both_Same (ì§§ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ) ì¼€ì´ìŠ¤\n",
      "  - í‰ê· : 22.1 ê¸€ì\n",
      "  - ì¤‘ê°„ê°’: 11.0 ê¸€ì\n",
      "\n",
      "ğŸ’¡ í•´ì„:\n",
      "  Content_Only í‰ê·  ê¸¸ì´ê°€ Both_Sameë³´ë‹¤ 1.7ë°° ê¹€\n",
      "  â†’ ê·œì¹™ ê²€ì¦: 'ê¸´ ë‚´ìš©ì€ Abstract ë¯¸ì…ë ¥' âœ…\n",
      "\n",
      "ğŸ“ Content_Only ìƒ˜í”Œ (ì¹´í…Œê³ ë¦¬ë³„):\n",
      "\n",
      "[è³¦] ê¸¸ì´: 3ê¸€ì\n",
      "  æ˜é¡è³¦...\n",
      "\n",
      "[ç–‘] ê¸¸ì´: 127ê¸€ì\n",
      "  å•: å­æ›°: â€œçŸ¥åŠä¹‹, ä»èƒ½å®ˆä¹‹, èŠè€Œ_ä¹‹, å‹•ä¹‹ä¸ä»¥ç¦®, æœªå–„ä¹Ÿ.â€ ä»è€…, æœ¬å¿ƒä¹‹å…¨å¾·, ç¦®è€…, å¤©ç†ä¹‹ç¯€æ–‡ä¹Ÿ, è“‹å¿ƒä¹‹å…¨å¾·, è«éå¤©ç†, å¤©ç†å…¨æ–¼æˆ‘,...\n",
      "\n",
      "[ç­–] ê¸¸ì´: 291ê¸€ì\n",
      "  ç‹è‹¥æ›°: å¸ç‹ä¹‹æ²», å¿…æœ‰å…¶é“, å¸ç‹ä¹‹å­¸, å¿…æœ‰å…¶æ³•. å ¯èˆœç¦¹æ¹¯æ–‡æ­¦ä¹‹çˆ²å›, å…¶æ‰€ä»¥çˆ²æ²»è€Œæ‰€ä»¥çˆ²å­¸, å¯å¾—èå…¶è©³æ­Ÿ? å¥å¾Œæ¼¢Â·é­Â·ç§¦Â·éš‹Â·å”Â·å®‹ä¹‹å›æ²»èˆ‡å­¸, ...\n",
      "\n",
      "[è©©] ê¸¸ì´: 16ê¸€ì\n",
      "  ç¯‰å£‡æ‹œå¤§å°‡ äºŒåéŸ»â—‹ç™¸æœªåº­è©¦å±…é­...\n"
     ]
    }
   ],
   "source": [
    "# 2-1. Content_Only ì¼€ì´ìŠ¤ ì‹¬ì¸µ ë¶„ì„\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” Content_Only ì¼€ì´ìŠ¤ ë¶„ì„ (Abstract ë¯¸ì…ë ¥)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "content_only_df = df[df['pattern'] == 'Content_Only']\n",
    "print(f\"\\nì´ {len(content_only_df)}ê°œ (ì „ì²´ì˜ {len(content_only_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬\n",
    "print(\"\\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:\")\n",
    "for cat, count in content_only_df['category'].value_counts().head(10).items():\n",
    "    total_in_cat = len(df[df['category'] == cat])\n",
    "    print(f\"  {cat:8s}: {count:4d} / {total_in_cat:4d} ({count/total_in_cat*100:5.1f}%)\")\n",
    "\n",
    "# Content ê¸¸ì´ ë¶„ì„\n",
    "print(f\"\\nğŸ“ Content ê¸¸ì´ ë¶„ì„:\")\n",
    "print(f\"  - í‰ê· : {content_only_df['content_len'].mean():.1f} ê¸€ì\")\n",
    "print(f\"  - ì¤‘ê°„ê°’: {content_only_df['content_len'].median():.1f} ê¸€ì\")\n",
    "print(f\"  - ìµœì†Œ: {content_only_df['content_len'].min()} ê¸€ì\")\n",
    "print(f\"  - ìµœëŒ€: {content_only_df['content_len'].max()} ê¸€ì\")\n",
    "\n",
    "# ë¹„êµ: Both_Same ì¼€ì´ìŠ¤ì™€ ê¸¸ì´ ë¹„êµ\n",
    "both_same_df = df[df['pattern'] == 'Both_Same']\n",
    "print(f\"\\nğŸ”„ ë¹„êµ: Both_Same (ì§§ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ) ì¼€ì´ìŠ¤\")\n",
    "print(f\"  - í‰ê· : {both_same_df['content_len'].mean():.1f} ê¸€ì\")\n",
    "print(f\"  - ì¤‘ê°„ê°’: {both_same_df['content_len'].median():.1f} ê¸€ì\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ í•´ì„:\")\n",
    "print(f\"  Content_Only í‰ê·  ê¸¸ì´ê°€ Both_Sameë³´ë‹¤ {content_only_df['content_len'].mean() / both_same_df['content_len'].mean():.1f}ë°° ê¹€\")\n",
    "print(f\"  â†’ ê·œì¹™ ê²€ì¦: 'ê¸´ ë‚´ìš©ì€ Abstract ë¯¸ì…ë ¥' âœ…\")\n",
    "\n",
    "# ìƒ˜í”Œ ë³´ê¸°\n",
    "print(f\"\\nğŸ“ Content_Only ìƒ˜í”Œ (ì¹´í…Œê³ ë¦¬ë³„):\")\n",
    "for cat in ['è³¦', 'ç–‘', 'ç­–', 'è©©']:\n",
    "    sample = content_only_df[content_only_df['category'] == cat].head(1)\n",
    "    if not sample.empty:\n",
    "        content = sample.iloc[0]['content']\n",
    "        print(f\"\\n[{cat}] ê¸¸ì´: {len(content)}ê¸€ì\")\n",
    "        print(f\"  {content[:80]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_match_rate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ì‹œê°í™” 2: ì¹´í…Œê³ ë¦¬ë³„ ì…ë ¥ íŒ¨í„´ (ì ì¸µ ë§‰ëŒ€ ê·¸ë˜í”„)\n",
    "# ê·¸ë˜í”„ ìƒì„± ì§ì „ í°íŠ¸ ì¬í™•ì¸\n",
    "setup_korean_fonts()\n",
    "\n",
    "# ì£¼ìš” ì¹´í…Œê³ ë¦¬ (ìƒìœ„ 12ê°œ)\n",
    "top_cats = df['category'].value_counts().head(12).index\n",
    "df_top = df[df['category'].isin(top_cats)]\n",
    "\n",
    "# íŒ¨í„´ë³„ ê°œìˆ˜ ê³„ì‚°\n",
    "pattern_data = []\n",
    "for cat in top_cats:\n",
    "    cat_df = df_top[df_top['category'] == cat]\n",
    "    total = len(cat_df)\n",
    "    patterns = {\n",
    "        'Both_Same': (cat_df['pattern'] == 'Both_Same').sum(),\n",
    "        'Both_Diff': (cat_df['pattern'] == 'Both_Diff').sum(),\n",
    "        'Content_Only': (cat_df['pattern'] == 'Content_Only').sum(),\n",
    "        'Abstract_Only': (cat_df['pattern'] == 'Abstract_Only').sum(),\n",
    "        'Neither': (cat_df['pattern'] == 'Neither').sum()\n",
    "    }\n",
    "    pattern_data.append({\n",
    "        'category': cat,\n",
    "        'total': total,\n",
    "        **patterns\n",
    "    })\n",
    "\n",
    "pattern_df = pd.DataFrame(pattern_data).set_index('category')\n",
    "\n",
    "# ë°±ë¶„ìœ¨ ê³„ì‚°\n",
    "pattern_pct = pattern_df.div(pattern_df['total'], axis=0) * 100\n",
    "pattern_pct = pattern_pct.drop('total', axis=1)\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# ìƒ‰ìƒ ì •ì˜\n",
    "colors = {\n",
    "    'Both_Same': '#2ecc71',      # ì´ˆë¡ (ê·œì¹™ ì¤€ìˆ˜)\n",
    "    'Both_Diff': '#3498db',      # íŒŒë‘ (ì œëª©+ì „ë¬¸)\n",
    "    'Content_Only': '#e74c3c',   # ë¹¨ê°• (ê¸´ ë‚´ìš©, Abstract ë¯¸ì…ë ¥)\n",
    "    'Abstract_Only': '#f39c12',  # ì£¼í™© (ì´ìƒ íŒ¨í„´)\n",
    "    'Neither': '#95a5a6'         # íšŒìƒ‰ (ë°ì´í„° ëˆ„ë½)\n",
    "}\n",
    "\n",
    "# ì ì¸µ ë§‰ëŒ€ ê·¸ë˜í”„\n",
    "bottom = np.zeros(len(pattern_pct))\n",
    "for pattern in ['Both_Same', 'Both_Diff', 'Content_Only', 'Abstract_Only', 'Neither']:\n",
    "    if pattern in pattern_pct.columns:\n",
    "        ax.barh(range(len(pattern_pct)), pattern_pct[pattern], left=bottom, \n",
    "                color=colors[pattern], label=pattern, height=0.7)\n",
    "        bottom += pattern_pct[pattern].values\n",
    "\n",
    "ax.set_yticks(range(len(pattern_pct)))\n",
    "ax.set_yticklabels(pattern_pct.index, fontsize=14)\n",
    "ax.set_xlabel('Percentage (%)', fontsize=16)\n",
    "ax.set_title('Abstract/Content Input Patterns by Category\\n(Stacked Bar Chart)', fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=True, fontsize=12)\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "# ì£¼ìš” íŒ¨í„´ í‘œì‹œ\n",
    "for i, cat in enumerate(pattern_pct.index):\n",
    "    content_only_pct = pattern_pct.loc[cat, 'Content_Only']\n",
    "    both_same_pct = pattern_pct.loc[cat, 'Both_Same']\n",
    "    \n",
    "    # Content_Onlyê°€ ë†’ìœ¼ë©´ í‘œì‹œ (ê¸´ ë‚´ìš©)\n",
    "    if content_only_pct > 60:\n",
    "        ax.text(102, i, f'Long form', va='center', fontsize=10, color='#e74c3c')\n",
    "    # Both_Sameì´ ë†’ìœ¼ë©´ í‘œì‹œ (ì§§ì€ ì›ë¬¸)\n",
    "    elif both_same_pct > 40:\n",
    "        ax.text(102, i, f'Short form', va='center', fontsize=10, color='#2ecc71')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"C2_input_pattern_by_category.pdf\"), dpi=200, bbox_inches='tight', format='pdf')\n",
    "plt.show()\n",
    "print(\"âœ… ì €ì¥: C2_input_pattern_by_category.pdf\")\n",
    "\n",
    "# íŒ¨í„´ í†µê³„ CSV ì €ì¥\n",
    "pattern_df.to_csv(os.path.join(OUT_DIR, \"input_pattern_stats.csv\"))\n",
    "print(\"âœ… ì €ì¥: input_pattern_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "translation_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë²ˆì—­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 4. ë²ˆì—­ í•¨ìˆ˜ ì •ì˜\n",
    "def translate_text(text, target_lang=\"Korean\", max_retries=3):\n",
    "    \"\"\"\n",
    "    Anthropic Claudeë¥¼ ì‚¬ìš©í•´ í•œë¬¸ì„ ë²ˆì—­\n",
    "    \n",
    "    Args:\n",
    "        text: ë²ˆì—­í•  í•œë¬¸ í…ìŠ¤íŠ¸\n",
    "        target_lang: \"Korean\" ë˜ëŠ” \"English\"\n",
    "        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        ë²ˆì—­ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    prompt = f\"\"\"ë‹¤ìŒ í•œë¬¸ í…ìŠ¤íŠ¸ë¥¼ {target_lang}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. \n",
    "ë²ˆì—­ë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "í•œë¬¸: {text}\n",
    "\n",
    "{target_lang} ë²ˆì—­:\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=1024,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return message.content[0].text.strip()\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"âš ï¸ ì¬ì‹œë„ {attempt+1}/{max_retries}: {str(e)[:50]}\")\n",
    "                time.sleep(2 ** attempt)  # exponential backoff\n",
    "            else:\n",
    "                print(f\"âŒ ë²ˆì—­ ì‹¤íŒ¨: {str(e)[:100]}\")\n",
    "                return \"\"\n",
    "\n",
    "print(\"âœ… ë²ˆì—­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "in14w137e8o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š ì „ì²´ ë²ˆì—­ ë¹„ìš© ì˜ˆì¸¡\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ ë°ì´í„° í˜„í™©:\n",
      "  - ì´ ë¬¸ì œ ìˆ˜: 3,348\n",
      "  - Abstract ìˆìŒ: 666 (19.9%)\n",
      "  - Content ìˆìŒ: 2,776 (82.9%)\n",
      "  - Abstract í‰ê·  ê¸¸ì´: 22.5 ê¸€ì\n",
      "  - Content í‰ê·  ê¸¸ì´: 38.3 ê¸€ì\n",
      "\n",
      "ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡:\n",
      "  Abstract ë²ˆì—­:\n",
      "    - ì…ë ¥ í† í°: 223,074\n",
      "    - ì¶œë ¥ í† í°: 179,748\n",
      "  Content ë²ˆì—­:\n",
      "    - ì…ë ¥ í† í°: 1,193,828\n",
      "    - ì¶œë ¥ í† í°: 1,277,256\n",
      "  \n",
      "  ğŸ“Š ì´ ì˜ˆìƒ í† í°: 2,873,906\n",
      "     - ì…ë ¥: 1,416,902\n",
      "     - ì¶œë ¥: 1,457,004\n",
      "\n",
      "ğŸ’µ ë¹„ìš© ì˜ˆì¸¡ (Claude 3.5 Sonnet):\n",
      "  - ì…ë ¥ ë¹„ìš©: $4.25 (1,416,902 tokens @ $3.0/MTok)\n",
      "  - ì¶œë ¥ ë¹„ìš©: $21.86 (1,457,004 tokens @ $15.0/MTok)\n",
      "  \n",
      "  ğŸ’° ì´ ì˜ˆìƒ ë¹„ìš©: $26.11 (ì•½ â‚©35,243)\n",
      "\n",
      "â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„:\n",
      "  - ì´ API ìš”ì²­ ìˆ˜: 6,884\n",
      "  - Rate limit ëŒ€ê¸°: 0.5ì´ˆ/ìš”ì²­\n",
      "  - ì˜ˆìƒ ì‹œê°„: 57.4ë¶„ (1.0ì‹œê°„)\n",
      "\n",
      "âš ï¸ ì£¼ì˜ì‚¬í•­:\n",
      "  1. ìœ„ ì˜ˆì¸¡ì€ ë³´ìˆ˜ì  ì¶”ì •ì¹˜ì…ë‹ˆë‹¤ (ì‹¤ì œ ë¹„ìš©ì€ ë” ë‚®ì„ ìˆ˜ ìˆìŒ)\n",
      "  2. API ì˜¤ë¥˜/ì¬ì‹œë„ë¡œ ì¸í•œ ì¶”ê°€ ë¹„ìš© ë°œìƒ ê°€ëŠ¥\n",
      "  3. í…ŒìŠ¤íŠ¸ ì‹œ MAX_TRANSLATE ê°’ì„ ì¡°ì ˆí•˜ì—¬ ì†Œê·œëª¨ë¡œ ì‹œì‘ ê¶Œì¥\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’° ë¹„ìš© ë° í† í° ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š ì „ì²´ ë²ˆì—­ ë¹„ìš© ì˜ˆì¸¡\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë°ì´í„° í†µê³„\n",
    "total_questions = len(df)\n",
    "with_abstract = df['has_abstract'].sum()\n",
    "with_content = df['has_content'].sum()\n",
    "avg_abstract_len = df[df['has_abstract']]['abstract_len'].mean()\n",
    "avg_content_len = df[df['has_content']]['content_len'].mean()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ë°ì´í„° í˜„í™©:\")\n",
    "print(f\"  - ì´ ë¬¸ì œ ìˆ˜: {total_questions:,}\")\n",
    "print(f\"  - Abstract ìˆìŒ: {with_abstract:,} ({with_abstract/total_questions*100:.1f}%)\")\n",
    "print(f\"  - Content ìˆìŒ: {with_content:,} ({with_content/total_questions*100:.1f}%)\")\n",
    "print(f\"  - Abstract í‰ê·  ê¸¸ì´: {avg_abstract_len:.1f} ê¸€ì\")\n",
    "print(f\"  - Content í‰ê·  ê¸¸ì´: {avg_content_len:.1f} ê¸€ì\")\n",
    "\n",
    "# í† í° ì˜ˆì¸¡ (ëŒ€ëµì )\n",
    "# - í•œë¬¸/í•œê¸€: 1ê¸€ì â‰ˆ 2-3 í† í° (ì•ˆì „í•˜ê²Œ 3ìœ¼ë¡œ ê³„ì‚°)\n",
    "# - í”„ë¡¬í”„íŠ¸ ì˜¤ë²„í—¤ë“œ: ~100 í† í°/ìš”ì²­\n",
    "# - ì‘ë‹µ: ì›ë¬¸ì˜ 1.5-2ë°° (ë²ˆì—­ + ì¶”ê°€ ë¬¸ì)\n",
    "\n",
    "CHAR_TO_TOKEN = 3  # í•œë¬¸/í•œê¸€ ê¸€ìë‹¹ í† í° ìˆ˜ (ë³´ìˆ˜ì )\n",
    "PROMPT_OVERHEAD = 100  # í”„ë¡¬í”„íŠ¸ ê¸°ë³¸ í† í°\n",
    "RESPONSE_MULTIPLIER = 2  # ì‘ë‹µì´ ì›ë¬¸ì˜ ëª‡ ë°°ì¸ì§€\n",
    "\n",
    "# Abstract ë²ˆì—­ (Korean + English)\n",
    "abstract_input_tokens_per_req = avg_abstract_len * CHAR_TO_TOKEN + PROMPT_OVERHEAD\n",
    "abstract_output_tokens_per_req = avg_abstract_len * CHAR_TO_TOKEN * RESPONSE_MULTIPLIER\n",
    "\n",
    "abstract_total_input = abstract_input_tokens_per_req * with_abstract * 2  # í•œêµ­ì–´ + ì˜ì–´\n",
    "abstract_total_output = abstract_output_tokens_per_req * with_abstract * 2\n",
    "\n",
    "# Content ë²ˆì—­ (Korean + English)  \n",
    "content_input_tokens_per_req = avg_content_len * CHAR_TO_TOKEN + PROMPT_OVERHEAD\n",
    "content_output_tokens_per_req = avg_content_len * CHAR_TO_TOKEN * RESPONSE_MULTIPLIER\n",
    "\n",
    "content_total_input = content_input_tokens_per_req * with_content * 2  # í•œêµ­ì–´ + ì˜ì–´\n",
    "content_total_output = content_output_tokens_per_req * with_content * 2\n",
    "\n",
    "# ì „ì²´ í•©ê³„\n",
    "total_input_tokens = abstract_total_input + content_total_input\n",
    "total_output_tokens = abstract_total_output + content_total_output\n",
    "total_tokens = total_input_tokens + total_output_tokens\n",
    "\n",
    "print(f\"\\nğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡:\")\n",
    "print(f\"  Abstract ë²ˆì—­:\")\n",
    "print(f\"    - ì…ë ¥ í† í°: {abstract_total_input:,.0f}\")\n",
    "print(f\"    - ì¶œë ¥ í† í°: {abstract_total_output:,.0f}\")\n",
    "print(f\"  Content ë²ˆì—­:\")\n",
    "print(f\"    - ì…ë ¥ í† í°: {content_total_input:,.0f}\")\n",
    "print(f\"    - ì¶œë ¥ í† í°: {content_total_output:,.0f}\")\n",
    "print(f\"  \")\n",
    "print(f\"  ğŸ“Š ì´ ì˜ˆìƒ í† í°: {total_tokens:,.0f}\")\n",
    "print(f\"     - ì…ë ¥: {total_input_tokens:,.0f}\")\n",
    "print(f\"     - ì¶œë ¥: {total_output_tokens:,.0f}\")\n",
    "\n",
    "# ë¹„ìš© ê³„ì‚° (Claude 3.5 Sonnet ê°€ê²©)\n",
    "# https://www.anthropic.com/pricing\n",
    "INPUT_COST_PER_MTok = 3.00   # $3 per million input tokens\n",
    "OUTPUT_COST_PER_MTok = 15.00  # $15 per million output tokens\n",
    "\n",
    "input_cost = (total_input_tokens / 1_000_000) * INPUT_COST_PER_MTok\n",
    "output_cost = (total_output_tokens / 1_000_000) * OUTPUT_COST_PER_MTok\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(f\"\\nğŸ’µ ë¹„ìš© ì˜ˆì¸¡ (Claude 3.5 Sonnet):\")\n",
    "print(f\"  - ì…ë ¥ ë¹„ìš©: ${input_cost:.2f} ({total_input_tokens:,.0f} tokens @ ${INPUT_COST_PER_MTok}/MTok)\")\n",
    "print(f\"  - ì¶œë ¥ ë¹„ìš©: ${output_cost:.2f} ({total_output_tokens:,.0f} tokens @ ${OUTPUT_COST_PER_MTok}/MTok)\")\n",
    "print(f\"  \")\n",
    "print(f\"  ğŸ’° ì´ ì˜ˆìƒ ë¹„ìš©: ${total_cost:.2f} (ì•½ â‚©{total_cost*1350:,.0f})\")\n",
    "\n",
    "# ì‹œê°„ ì˜ˆì¸¡\n",
    "requests_per_field = with_abstract * 2 + with_content * 2  # ê° í•„ë“œë‹¹ í•œêµ­ì–´+ì˜ì–´\n",
    "RATE_LIMIT_DELAY = 0.5  # ì´ˆ/ìš”ì²­\n",
    "total_time_sec = requests_per_field * RATE_LIMIT_DELAY\n",
    "total_time_min = total_time_sec / 60\n",
    "\n",
    "print(f\"\\nâ±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„:\")\n",
    "print(f\"  - ì´ API ìš”ì²­ ìˆ˜: {requests_per_field:,}\")\n",
    "print(f\"  - Rate limit ëŒ€ê¸°: {RATE_LIMIT_DELAY}ì´ˆ/ìš”ì²­\")\n",
    "print(f\"  - ì˜ˆìƒ ì‹œê°„: {total_time_min:.1f}ë¶„ ({total_time_sec/3600:.1f}ì‹œê°„)\")\n",
    "\n",
    "print(f\"\\nâš ï¸ ì£¼ì˜ì‚¬í•­:\")\n",
    "print(f\"  1. ìœ„ ì˜ˆì¸¡ì€ ë³´ìˆ˜ì  ì¶”ì •ì¹˜ì…ë‹ˆë‹¤ (ì‹¤ì œ ë¹„ìš©ì€ ë” ë‚®ì„ ìˆ˜ ìˆìŒ)\")\n",
    "print(f\"  2. API ì˜¤ë¥˜/ì¬ì‹œë„ë¡œ ì¸í•œ ì¶”ê°€ ë¹„ìš© ë°œìƒ ê°€ëŠ¥\")\n",
    "print(f\"  3. í…ŒìŠ¤íŠ¸ ì‹œ MAX_TRANSLATE ê°’ì„ ì¡°ì ˆí•˜ì—¬ ì†Œê·œëª¨ë¡œ ì‹œì‘ ê¶Œì¥\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sample_translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸ (5ê°œ):\n",
      "================================================================================\n",
      "\n",
      "[è³¦] ì›ë¬¸: è“‹éºŸäº‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/cj4b9h512gs7by1fyqkz5j5c0000gn/T/ipykernel_40109/785398881.py:26: DeprecationWarning: The model 'claude-3-5-sonnet-20241022' is deprecated and will reach end-of-life on October 22, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  message = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì–´: ëŒ€ë¦°ìš´\n",
      "ì˜ì–´: Gai Lin Yun\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[è©©] ì›ë¬¸: æ€ä¸­èˆˆè‚¡è‚±ä¹‹ç¾, åœ–_å…¶äººæ–¼éº’éºŸé–£. æŠ¼_.\n",
      "í•œêµ­ì–´: ë‚˜ë¼ë¥¼ ì¼ìœ¼í‚¤ëŠ”ë° íŒ”ë‹¤ë¦¬ì²˜ëŸ¼ ê³µì„ ì„¸ìš´ ì‹ í•˜ë“¤ì˜ ì•„ë¦„ë‹¤ìš´ ê³µì ì„ ìƒê°í•˜ì—¬, ê·¸ë“¤ì˜ ì´ˆìƒí™”ë¥¼ ê¸°ë¦°ê°ì— ê·¸ë¦¬ë‹¤.\n",
      "ì˜ì–´: Pondering the beauty of those who served as arms and legs (loyal ministers) to the revival, their portraits were painted in the Qilin Hall.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ç¾©] ì›ë¬¸: ç„¡æ„Ÿæˆ‘_å…®.\n",
      "í•œêµ­ì–´: ë‚˜ë¥¼ ê°ë™ì‹œí‚¤ì§€ ëª»í•˜ëŠ”êµ¬ë‚˜.\n",
      "ì˜ì–´: Cannot feel me.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[è³¦] ì›ë¬¸: ä¹ƒä»¤å£«æŒæ»¿æ¯‹ç™¼.\n",
      "í•œêµ­ì–´: ì´ì— ë³‘ì‚¬ë“¤ì—ê²Œ í™œì„ ë‹¹ê¸°ê³  ìˆë˜ ì˜ì§€ëŠ” ë§ë¼ê³  ëª…í–ˆë‹¤.\n",
      "ì˜ì–´: Then he ordered his soldiers to hold their drawn bows but not to shoot.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[è©©] ì›ë¬¸: è¡Œç„¡éƒ¨ä¼, å°±å–„æ°´è‰èˆæ­¢. æŠ¼èˆ.\n",
      "í•œêµ­ì–´: ë¶€ëŒ€ë¥¼ ì´ë£¨ì§€ ì•Šê³  ë‹¤ë‹ˆë©°, ë¬¼ê³¼ í’€ì´ ì¢‹ì€ ê³³ì„ ì°¾ì•„ê°€ ë¨¸ë¬¼ë €ë‹¤. ê±°ì²˜ë¥¼ ì§€ì¼°ë‹¤.\n",
      "ì˜ì–´: Move without formations, settle where there is good water and grass. Set up camp.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ… ìƒ˜í”Œ ë²ˆì—­ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 5. ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œ)\n",
    "print(\"\\nğŸ§ª ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸ (5ê°œ):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_df = df[df['has_abstract']].head(5).copy()\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "    text = row['abstract']\n",
    "    category = row['category']\n",
    "    \n",
    "    print(f\"\\n[{category}] ì›ë¬¸: {text}\")\n",
    "    \n",
    "    # í•œêµ­ì–´ ë²ˆì—­\n",
    "    ko_trans = translate_text(text, \"Korean\")\n",
    "    print(f\"í•œêµ­ì–´: {ko_trans}\")\n",
    "    \n",
    "    # ì˜ì–´ ë²ˆì—­\n",
    "    en_trans = translate_text(text, \"English\")\n",
    "    print(f\"ì˜ì–´: {en_trans}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    time.sleep(1)  # API rate limit ê³ ë ¤\n",
    "\n",
    "print(\"\\nâœ… ìƒ˜í”Œ ë²ˆì—­ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "full_translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â­ï¸  ì „ì²´ ë²ˆì—­ ê±´ë„ˆë›°ê¸° (TRANSLATE_ALL=False)\n",
      "ğŸ’¡ ì „ì²´ ë²ˆì—­ì„ ì›í•˜ë©´:\n",
      "   1. TRANSLATE_ALL=True ë¡œ ë³€ê²½\n",
      "   2. TRANSLATE_ABSTRACT, TRANSLATE_CONTENT ì„ íƒ\n",
      "\n",
      "ğŸ’° ì˜ˆìƒ ë¹„ìš©: ~$23 (ì•½ â‚©31,000)\n",
      "â±ï¸  ì˜ˆìƒ ì‹œê°„: ~1ì‹œê°„\n"
     ]
    }
   ],
   "source": [
    "# 6. ì „ì²´ ë°ì´í„° ë²ˆì—­\n",
    "# ë¹„ìš©: ì•½ $23 (â‚©31,000), ì†Œìš”ì‹œê°„: ì•½ 1ì‹œê°„\n",
    "\n",
    "TRANSLATE_ALL = False  # Trueë¡œ ë³€ê²½í•˜ë©´ ì „ì²´ ë²ˆì—­ ì‹¤í–‰\n",
    "TRANSLATE_ABSTRACT = True  # Abstract ë²ˆì—­ ì—¬ë¶€\n",
    "TRANSLATE_CONTENT = True   # Content ë²ˆì—­ ì—¬ë¶€\n",
    "\n",
    "if TRANSLATE_ALL:\n",
    "    print(f\"\\nğŸ”„ ì „ì²´ ë²ˆì—­ ì‹œì‘...\")\n",
    "    print(f\"  - Abstract ë²ˆì—­: {TRANSLATE_ABSTRACT}\")\n",
    "    print(f\"  - Content ë²ˆì—­: {TRANSLATE_CONTENT}\")\n",
    "    \n",
    "    # ë²ˆì—­í•  ë°ì´í„° ì¤€ë¹„\n",
    "    df_translated = df.copy()\n",
    "    \n",
    "    # Abstract ë²ˆì—­ (í•œêµ­ì–´ + ì˜ì–´)\n",
    "    if TRANSLATE_ABSTRACT:\n",
    "        print(f\"\\nğŸ“ Abstract ë²ˆì—­ ì¤‘... ({df['has_abstract'].sum()}ê°œ)\")\n",
    "        df_translated['abstract_ko'] = \"\"\n",
    "        df_translated['abstract_en'] = \"\"\n",
    "        \n",
    "        abstract_rows = df_translated[df_translated['has_abstract']]\n",
    "        for idx, row in tqdm(abstract_rows.iterrows(), total=len(abstract_rows), desc=\"Abstract â†’ Korean\"):\n",
    "            df_translated.at[idx, 'abstract_ko'] = translate_text(row['abstract'], \"Korean\")\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        for idx, row in tqdm(abstract_rows.iterrows(), total=len(abstract_rows), desc=\"Abstract â†’ English\"):\n",
    "            df_translated.at[idx, 'abstract_en'] = translate_text(row['abstract'], \"English\")\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    # Content ë²ˆì—­ (í•œêµ­ì–´ + ì˜ì–´)\n",
    "    if TRANSLATE_CONTENT:\n",
    "        print(f\"\\nğŸ“ Content ë²ˆì—­ ì¤‘... ({df['has_content'].sum()}ê°œ)\")\n",
    "        df_translated['content_ko'] = \"\"\n",
    "        df_translated['content_en'] = \"\"\n",
    "        \n",
    "        content_rows = df_translated[df_translated['has_content']]\n",
    "        for idx, row in tqdm(content_rows.iterrows(), total=len(content_rows), desc=\"Content â†’ Korean\"):\n",
    "            df_translated.at[idx, 'content_ko'] = translate_text(row['content'], \"Korean\")\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        for idx, row in tqdm(content_rows.iterrows(), total=len(content_rows), desc=\"Content â†’ English\"):\n",
    "            df_translated.at[idx, 'content_en'] = translate_text(row['content'], \"English\")\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    output_file = os.path.join(OUT_DIR, \"translated_full.csv\")\n",
    "    save_cols = ['question_id', 'category', 'abstract', 'content']\n",
    "    if TRANSLATE_ABSTRACT:\n",
    "        save_cols.extend(['abstract_ko', 'abstract_en'])\n",
    "    if TRANSLATE_CONTENT:\n",
    "        save_cols.extend(['content_ko', 'content_en'])\n",
    "    \n",
    "    df_translated[save_cols].to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nâœ… ë²ˆì—­ ì™„ë£Œ! ì €ì¥: {output_file}\")\n",
    "    print(f\"ğŸ“Š ë²ˆì—­ëœ í–‰: {len(df_translated)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâ­ï¸  ì „ì²´ ë²ˆì—­ ê±´ë„ˆë›°ê¸° (TRANSLATE_ALL=False)\")\n",
    "    print(\"ğŸ’¡ ì „ì²´ ë²ˆì—­ì„ ì›í•˜ë©´:\")\n",
    "    print(\"   1. TRANSLATE_ALL=True ë¡œ ë³€ê²½\")\n",
    "    print(\"   2. TRANSLATE_ABSTRACT, TRANSLATE_CONTENT ì„ íƒ\")\n",
    "    print(f\"\\nğŸ’° ì˜ˆìƒ ë¹„ìš©: ~$23 (ì•½ â‚©31,000)\")\n",
    "    print(f\"â±ï¸  ì˜ˆìƒ ì‹œê°„: ~1ì‹œê°„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“‹ 3ë²ˆ ì‹¤í—˜ ì¢…í•© ê²°ê³¼\n",
      "================================================================================\n",
      "\n",
      "âœ… ì „ì²´ ì…ë ¥ íŒ¨í„´ ë¶„ì„ (ì´ 3,348ê°œ):\n",
      "--------------------------------------------------------------------------------\n",
      "1. Content_Only: 2,204ê°œ (65.8%)\n",
      "   â†’ ê¸´ ë‚´ìš©ì˜ ê²½ìš° Abstract ë¯¸ì…ë ¥ (ê·œì¹™ ì¤€ìˆ˜)\n",
      "\n",
      "2. Both_Same: 533ê°œ (15.9%)\n",
      "   â†’ ì§§ì€ ì›ë¬¸ì„ Abstractì™€ Contentì— ë™ì¼í•˜ê²Œ ì…ë ¥\n",
      "\n",
      "3. Neither: 478ê°œ (14.3%)\n",
      "   â†’ ë°ì´í„° ëˆ„ë½ (ì£¼ë¡œ N/A ì¹´í…Œê³ ë¦¬)\n",
      "\n",
      "4. Abstract_Only: 94ê°œ (2.8%)\n",
      "   â†’ Content ëˆ„ë½ ì¼€ì´ìŠ¤ (ì˜ˆì™¸ íŒ¨í„´)\n",
      "\n",
      "5. Both_Diff: 39ê°œ (1.2%)\n",
      "   â†’ Abstract=ì œëª©/ì£¼ì œì–´, Content=ì „ë¬¸ (ì£¼ë¡œ ç­–)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” ì…ë ¥ íŒ¨í„´:\n",
      "================================================================================\n",
      "\n",
      "è³¦ (ì´ 749ê°œ):\n",
      "  - ì£¼ìš” íŒ¨í„´: 557 (557ê°œ, 74.4%)\n",
      "  - Content í‰ê·  ê¸¸ì´: 7.4ê¸€ì\n",
      "\n",
      "è¡¨ (ì´ 443ê°œ):\n",
      "  - ì£¼ìš” íŒ¨í„´: 341 (341ê°œ, 77.0%)\n",
      "  - Content í‰ê·  ê¸¸ì´: 19.0ê¸€ì\n",
      "\n",
      "è©© (ì´ 424ê°œ):\n",
      "  - ì£¼ìš” íŒ¨í„´: 304 (304ê°œ, 71.7%)\n",
      "  - Content í‰ê·  ê¸¸ì´: 20.2ê¸€ì\n",
      "\n",
      "ç–‘ (ì´ 415ê°œ):\n",
      "  - ì£¼ìš” íŒ¨í„´: 405 (405ê°œ, 97.6%)\n",
      "  - Content í‰ê·  ê¸¸ì´: 100.5ê¸€ì\n",
      "\n",
      "ç¾© (ì´ 365ê°œ):\n",
      "  - ì£¼ìš” íŒ¨í„´: 251 (251ê°œ, 68.8%)\n",
      "  - Content í‰ê·  ê¸¸ì´: 8.9ê¸€ì\n",
      "\n",
      "ç­– (ì´ 292ê°œ):\n",
      "  - ì£¼ìš” íŒ¨í„´: 162 (162ê°œ, 55.5%)\n",
      "  - Content í‰ê·  ê¸¸ì´: 180.1ê¸€ì\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ ì…ë ¥ ê·œì¹™ ê²€ì¦:\n",
      "================================================================================\n",
      "\n",
      "âœ… ê·œì¹™ 1: ì§§ì€ ë¬¸ì œëŠ” Abstract/Contentì— ë™ì¼í•˜ê²Œ ì…ë ¥\n",
      "   - Both_Same íŒ¨í„´: 533ê°œ\n",
      "   - í‰ê·  ê¸¸ì´: 22.1ê¸€ì\n",
      "\n",
      "âœ… ê·œì¹™ 2: ê¸´ ë‚´ìš©ì€ Abstract ë¯¸ì…ë ¥, Contentì— ì „ë¬¸\n",
      "   - Content_Only íŒ¨í„´: 2204ê°œ\n",
      "   - í‰ê·  ê¸¸ì´: 37.3ê¸€ì (Both_Sameì˜ 1.7ë°°)\n",
      "\n",
      "ğŸ’¡ íŠ¹ì´ íŒ¨í„´: Both_Diff (Abstractê°€ ì œëª© ì—­í• )\n",
      "   - ì£¼ë¡œ ç­– ì¹´í…Œê³ ë¦¬ì—ì„œ ë°œìƒ\n",
      "   - AbstractëŠ” ì§§ì€ ì œëª©/ì£¼ì œì–´, ContentëŠ” ì „ë¬¸\n",
      "\n",
      "ğŸ“Š ìƒì„±ëœ íŒŒì¼:\n",
      "   1. /Users/songhune/Workspace/korean_eda/results//input_pattern_stats.csv\n",
      "   2. /Users/songhune/Workspace/korean_eda/results//C1_category_length_comparison.pdf\n",
      "   3. /Users/songhune/Workspace/korean_eda/results//C2_input_pattern_by_category.pdf\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 7. ì¢…í•© ê²°ë¡  (ì˜¬ë°”ë¥¸ í•´ì„)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ 3ë²ˆ ì‹¤í—˜ ì¢…í•© ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# íŒ¨í„´ë³„ í†µê³„\n",
    "pattern_counts = df['pattern'].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "print(f\"\\nâœ… ì „ì²´ ì…ë ¥ íŒ¨í„´ ë¶„ì„ (ì´ {total:,}ê°œ):\")\n",
    "print(\"-\"*80)\n",
    "print(f\"1. Content_Only: {pattern_counts.get('Content_Only', 0):,}ê°œ ({pattern_counts.get('Content_Only', 0)/total*100:.1f}%)\")\n",
    "print(f\"   â†’ ê¸´ ë‚´ìš©ì˜ ê²½ìš° Abstract ë¯¸ì…ë ¥ (ê·œì¹™ ì¤€ìˆ˜)\")\n",
    "print(f\"\\n2. Both_Same: {pattern_counts.get('Both_Same', 0):,}ê°œ ({pattern_counts.get('Both_Same', 0)/total*100:.1f}%)\")\n",
    "print(f\"   â†’ ì§§ì€ ì›ë¬¸ì„ Abstractì™€ Contentì— ë™ì¼í•˜ê²Œ ì…ë ¥\")\n",
    "print(f\"\\n3. Neither: {pattern_counts.get('Neither', 0):,}ê°œ ({pattern_counts.get('Neither', 0)/total*100:.1f}%)\")\n",
    "print(f\"   â†’ ë°ì´í„° ëˆ„ë½ (ì£¼ë¡œ N/A ì¹´í…Œê³ ë¦¬)\")\n",
    "print(f\"\\n4. Abstract_Only: {pattern_counts.get('Abstract_Only', 0):,}ê°œ ({pattern_counts.get('Abstract_Only', 0)/total*100:.1f}%)\")\n",
    "print(f\"   â†’ Content ëˆ„ë½ ì¼€ì´ìŠ¤ (ì˜ˆì™¸ íŒ¨í„´)\")\n",
    "print(f\"\\n5. Both_Diff: {pattern_counts.get('Both_Diff', 0):,}ê°œ ({pattern_counts.get('Both_Diff', 0)/total*100:.1f}%)\")\n",
    "print(f\"   â†’ Abstract=ì œëª©/ì£¼ì œì–´, Content=ì „ë¬¸ (ì£¼ë¡œ ç­–)\")\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” íŒ¨í„´\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” ì…ë ¥ íŒ¨í„´:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "major_categories = ['è³¦', 'è¡¨', 'è©©', 'ç–‘', 'ç¾©', 'ç­–']\n",
    "for cat in major_categories:\n",
    "    cat_df = df[df['category'] == cat]\n",
    "    if len(cat_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    dominant_pattern = cat_df['pattern'].value_counts().iloc[0]\n",
    "    dominant_count = cat_df['pattern'].value_counts().values[0]\n",
    "    dominant_pct = dominant_count / len(cat_df) * 100\n",
    "    \n",
    "    avg_content_len = cat_df[cat_df['has_content']]['content_len'].mean() if cat_df['has_content'].any() else 0\n",
    "    \n",
    "    print(f\"\\n{cat} (ì´ {len(cat_df)}ê°œ):\")\n",
    "    print(f\"  - ì£¼ìš” íŒ¨í„´: {dominant_pattern} ({dominant_count}ê°œ, {dominant_pct:.1f}%)\")\n",
    "    print(f\"  - Content í‰ê·  ê¸¸ì´: {avg_content_len:.1f}ê¸€ì\")\n",
    "\n",
    "# ê·œì¹™ ê²€ì¦\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ ì…ë ¥ ê·œì¹™ ê²€ì¦:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "content_only_avg_len = df[df['pattern'] == 'Content_Only']['content_len'].mean()\n",
    "both_same_avg_len = df[df['pattern'] == 'Both_Same']['content_len'].mean()\n",
    "\n",
    "print(f\"\\nâœ… ê·œì¹™ 1: ì§§ì€ ë¬¸ì œëŠ” Abstract/Contentì— ë™ì¼í•˜ê²Œ ì…ë ¥\")\n",
    "print(f\"   - Both_Same íŒ¨í„´: {pattern_counts.get('Both_Same', 0)}ê°œ\")\n",
    "print(f\"   - í‰ê·  ê¸¸ì´: {both_same_avg_len:.1f}ê¸€ì\")\n",
    "\n",
    "print(f\"\\nâœ… ê·œì¹™ 2: ê¸´ ë‚´ìš©ì€ Abstract ë¯¸ì…ë ¥, Contentì— ì „ë¬¸\")\n",
    "print(f\"   - Content_Only íŒ¨í„´: {pattern_counts.get('Content_Only', 0)}ê°œ\")\n",
    "print(f\"   - í‰ê·  ê¸¸ì´: {content_only_avg_len:.1f}ê¸€ì (Both_Sameì˜ {content_only_avg_len/both_same_avg_len:.1f}ë°°)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ íŠ¹ì´ íŒ¨í„´: Both_Diff (Abstractê°€ ì œëª© ì—­í• )\")\n",
    "print(f\"   - ì£¼ë¡œ ç­– ì¹´í…Œê³ ë¦¬ì—ì„œ ë°œìƒ\")\n",
    "print(f\"   - AbstractëŠ” ì§§ì€ ì œëª©/ì£¼ì œì–´, ContentëŠ” ì „ë¬¸\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ìƒì„±ëœ íŒŒì¼:\")\n",
    "print(f\"   1. {OUT_DIR}/input_pattern_stats.csv\")\n",
    "print(f\"   2. {OUT_DIR}/C1_category_length_comparison.pdf\")\n",
    "print(f\"   3. {OUT_DIR}/C2_input_pattern_by_category.pdf\")\n",
    "# if TRANSLATE_ALL:\n",
    "#     print(f\"   4. {OUT_DIR}/translated_full.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc185e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
