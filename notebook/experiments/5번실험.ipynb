{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 5: K-ClassicBench 벤치마크 평가\n",
    "\n",
    "**목표**: C3Bench를 참고하여 개발한 K-ClassicBench로 다양한 LLM 평가\n",
    "\n",
    "## 평가 모델 종류\n",
    "\n",
    "### 1. 비공개 API 모델\n",
    "- GPT-4 Turbo\n",
    "- GPT-3.5 Turbo\n",
    "- Claude 3.5 Sonnet\n",
    "- Claude 3 Opus\n",
    "\n",
    "### 2. 오픈소스 모델\n",
    "- Llama 3.1 (8B, 70B)\n",
    "- Qwen 2.5 (7B, 14B, 72B)\n",
    "- EXAONE 3.0 (7.8B)\n",
    "\n",
    "### 3. 지도학습 모델\n",
    "- GwenBert\n",
    "- Tongu\n",
    "\n",
    "## 평가 태스크\n",
    "\n",
    "1. **Classification**: 문체 분류 (賦/詩/疑/義 등)\n",
    "2. **Retrieval**: 출처 식별 (論語/孟子/大學/中庸)\n",
    "3. **Punctuation**: 구두점 복원 (백문 → 구두점본)\n",
    "4. **NLI**: 자연언어추론 (entailment/contradiction/neutral)\n",
    "5. **Translation**: 번역 (한문↔한글↔영문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 경로 설정\n",
    "BENCHMARK_PATH = '/Users/songhune/Workspace/korean_eda/benchmark/k_classic_bench/k_classic_bench_full.json'\n",
    "RESULTS_DIR = '/Users/songhune/Workspace/korean_eda/benchmark/results'\n",
    "\n",
    "# 결과 디렉토리 생성\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✅ 환경 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 벤치마크 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벤치마크 로드\n",
    "with open(BENCHMARK_PATH, 'r', encoding='utf-8') as f:\n",
    "    benchmark = json.load(f)\n",
    "\n",
    "print(f\"📊 {benchmark['benchmark_info']['name']}\")\n",
    "print(f\"버전: {benchmark['benchmark_info']['version']}\")\n",
    "print(f\"총 항목 수: {benchmark['benchmark_info']['total_size']:,}개\")\n",
    "print(f\"\\n태스크:\")\n",
    "\n",
    "for task_name, task_data in benchmark['tasks'].items():\n",
    "    print(f\"  - {task_name}: {task_data['size']:,}개 ({task_data['metric']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 태스크 예시 확인\n",
    "for task_name, task_data in benchmark['tasks'].items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{task_name.upper()}] 예시\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    example = task_data['data'][0]\n",
    "    \n",
    "    if task_name == 'classification':\n",
    "        print(f\"입력: {example['input']}\")\n",
    "        print(f\"레이블: {example['label']}\")\n",
    "    \n",
    "    elif task_name == 'retrieval':\n",
    "        print(f\"입력: {example['input'][:100]}...\")\n",
    "        print(f\"정답: {example['answer']}\")\n",
    "    \n",
    "    elif task_name == 'punctuation':\n",
    "        print(f\"입력 (백문): {example['input'][:80]}...\")\n",
    "        print(f\"정답 (구두점): {example['answer'][:80]}...\")\n",
    "    \n",
    "    elif task_name == 'nli':\n",
    "        print(f\"전제: {example['premise'][:80]}...\")\n",
    "        print(f\"가설: {example['hypothesis'][:80]}...\")\n",
    "        print(f\"레이블: {example['label']}\")\n",
    "    \n",
    "    elif task_name == 'translation':\n",
    "        print(f\"원문 ({example['source_lang']}): {example['source_text'][:80]}...\")\n",
    "        print(f\"번역 ({example['target_lang']}): {example['target_text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 평가 실행\n",
    "\n",
    "### 3.1 API 모델 평가 (GPT-4, Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키 설정 (환경 변수 또는 직접 입력)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')  # 또는 직접 입력\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')  # 또는 직접 입력\n",
    "\n",
    "# API 키 확인\n",
    "print(f\"OpenAI API Key: {'설정됨' if OPENAI_API_KEY else '미설정'}\")\n",
    "print(f\"Anthropic API Key: {'설정됨' if ANTHROPIC_API_KEY else '미설정'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 평가 (샘플 테스트: 각 태스크당 10개)\n",
    "!python exp5_benchmark_evaluation.py \\\n",
    "    --model-type api \\\n",
    "    --model-name gpt-4-turbo \\\n",
    "    --max-samples 10 \\\n",
    "    --api-key $OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet 평가 (샘플 테스트)\n",
    "!python exp5_benchmark_evaluation.py \\\n",
    "    --model-type api \\\n",
    "    --model-name claude-3-5-sonnet-20241022 \\\n",
    "    --max-samples 10 \\\n",
    "    --api-key $ANTHROPIC_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 오픈소스 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.1 8B 평가 (샘플 테스트)\n",
    "# 주의: GPU 메모리 필요\n",
    "!python exp5_benchmark_evaluation.py \\\n",
    "    --model-type opensource \\\n",
    "    --model-name meta-llama/Llama-3.1-8B-Instruct \\\n",
    "    --max-samples 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen 2.5 7B 평가 (샘플 테스트)\n",
    "!python exp5_benchmark_evaluation.py \\\n",
    "    --model-type opensource \\\n",
    "    --model-name Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --max-samples 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 지도학습 모델 평가 (GwenBert, Tongu)\n",
    "\n",
    "**주의**: 이 부분은 모델별 구현이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tongu 모델 평가\n",
    "# TODO: Tongu 모델 경로 지정 및 래퍼 구현 필요\n",
    "# !python exp5_benchmark_evaluation.py \\\n",
    "#     --model-type supervised \\\n",
    "#     --model-name tongu \\\n",
    "#     --max-samples 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 파일 로드\n",
    "result_files = list(Path(RESULTS_DIR).glob('summary_*.csv'))\n",
    "\n",
    "print(f\"📁 발견된 결과 파일: {len(result_files)}개\")\n",
    "for f in result_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 결과 병합\n",
    "all_results = []\n",
    "\n",
    "for file in result_files:\n",
    "    df = pd.read_csv(file)\n",
    "    all_results.append(df)\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"✅ 총 {len(results_df)} 개의 결과 로드됨\")\n",
    "    display(results_df.head())\n",
    "else:\n",
    "    print(\"⚠️  결과 파일이 없습니다. 먼저 평가를 실행하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 태스크별 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # 태스크별 주요 메트릭 추출\n",
    "    metric_map = {\n",
    "        'classification': 'accuracy',\n",
    "        'retrieval': 'accuracy',\n",
    "        'punctuation': 'rougeL_f1',\n",
    "        'nli': 'accuracy',\n",
    "        'translation': 'bleu'\n",
    "    }\n",
    "    \n",
    "    # 피벗 테이블 생성\n",
    "    pivot_data = []\n",
    "    \n",
    "    for task, metric in metric_map.items():\n",
    "        task_data = results_df[results_df['task'] == task]\n",
    "        for _, row in task_data.iterrows():\n",
    "            pivot_data.append({\n",
    "                'model': row['model'],\n",
    "                'task': task,\n",
    "                'score': row.get(metric, 0)\n",
    "            })\n",
    "    \n",
    "    pivot_df = pd.DataFrame(pivot_data)\n",
    "    pivot_table = pivot_df.pivot(index='model', columns='task', values='score')\n",
    "    \n",
    "    print(\"\\n📊 태스크별 성능 (주요 메트릭)\")\n",
    "    display(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # 태스크별 성능 비교 그래프\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('K-ClassicBench: 태스크별 모델 성능 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    tasks = list(metric_map.keys())\n",
    "    \n",
    "    for idx, task in enumerate(tasks):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        task_df = pivot_df[pivot_df['task'] == task]\n",
    "        \n",
    "        if not task_df.empty:\n",
    "            task_df = task_df.sort_values('score', ascending=True)\n",
    "            \n",
    "            ax.barh(task_df['model'], task_df['score'], color='skyblue')\n",
    "            ax.set_xlabel('Score')\n",
    "            ax.set_title(f'{task.upper()} ({metric_map[task]})')\n",
    "            ax.set_xlim(0, 1.0)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 빈 subplot 제거\n",
    "    if len(tasks) < 6:\n",
    "        for idx in range(len(tasks), 6):\n",
    "            fig.delaxes(axes[idx // 3, idx % 3])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # 히트맵\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Score'})\n",
    "    plt.title('모델별 태스크 성능 히트맵', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('태스크')\n",
    "    plt.ylabel('모델')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 모델별 종합 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # 평균 점수 계산\n",
    "    model_avg = pivot_table.mean(axis=1).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\n🏆 모델별 평균 성능 (전체 태스크)\")\n",
    "    print(\"=\"*50)\n",
    "    for idx, (model, score) in enumerate(model_avg.items(), 1):\n",
    "        print(f\"{idx}. {model}: {score:.4f}\")\n",
    "    \n",
    "    # 그래프\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model_avg.plot(kind='barh', color='coral')\n",
    "    plt.xlabel('평균 점수')\n",
    "    plt.title('모델별 평균 성능', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/overall_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 상세 분석\n",
    "\n",
    "### 5.1 Classification: 문체별 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification 상세 결과 로드\n",
    "# TODO: 각 모델의 JSON 결과 파일에서 예측값 추출하여 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 오류 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오답 케이스 분석\n",
    "# TODO: JSON 결과에서 예측 실패 케이스 추출 및 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결론 및 인사이트\n",
    "\n",
    "### 주요 발견사항\n",
    "\n",
    "1. **모델별 성능**\n",
    "   - 비공개 API 모델 (GPT-4, Claude)의 전반적인 우수성\n",
    "   - 오픈소스 모델의 태스크별 편차\n",
    "   - 지도학습 모델의 특정 태스크 강점\n",
    "\n",
    "2. **태스크별 난이도**\n",
    "   - 가장 쉬운 태스크: [분석 필요]\n",
    "   - 가장 어려운 태스크: [분석 필요]\n",
    "\n",
    "3. **개선 방향**\n",
    "   - 프롬프트 엔지니어링\n",
    "   - Few-shot learning 적용\n",
    "   - Fine-tuning 가능성\n",
    "\n",
    "### C3Bench와의 비교\n",
    "\n",
    "- [분석 필요]\n",
    "\n",
    "### 향후 연구 방향\n",
    "\n",
    "1. 더 많은 모델 평가\n",
    "2. Few-shot 성능 비교\n",
    "3. 도메인 특화 Fine-tuning\n",
    "4. 벤치마크 확장 (더 많은 태스크, 데이터)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 부록: 전체 평가 실행 스크립트\n",
    "\n",
    "### 배치 평가 스크립트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 모델을 순차적으로 평가하는 스크립트\n",
    "api_models = [\n",
    "    ('gpt-4-turbo', 'openai'),\n",
    "    ('gpt-3.5-turbo', 'openai'),\n",
    "    ('claude-3-5-sonnet-20241022', 'anthropic'),\n",
    "]\n",
    "\n",
    "opensource_models = [\n",
    "    'meta-llama/Llama-3.1-8B-Instruct',\n",
    "    'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct',\n",
    "]\n",
    "\n",
    "# API 모델 평가\n",
    "for model_name, provider in api_models:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"평가 중: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    api_key = OPENAI_API_KEY if provider == 'openai' else ANTHROPIC_API_KEY\n",
    "    \n",
    "    !python exp5_benchmark_evaluation.py \\\n",
    "        --model-type api \\\n",
    "        --model-name {model_name} \\\n",
    "        --api-key {api_key}\n",
    "\n",
    "# 오픈소스 모델 평가\n",
    "for model_name in opensource_models:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"평가 중: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    !python exp5_benchmark_evaluation.py \\\n",
    "        --model-type opensource \\\n",
    "        --model-name {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
