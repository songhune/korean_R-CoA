{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be344ec5",
   "metadata": {},
   "source": [
    "# C3Bench: 대규모 언어 모델(LLM)을 위한 포괄적인 고전 중국어 이해(CCU) 벤치마크 개발 제안서\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 서론 (Introduction)\n",
    "\n",
    "고전 중국어는 5000년 중국 문명의 정수와 문화유산을 담고 있는 핵심 매개체로서, 이를 깊이 있게 이해하는 능력(Classical Chinese Understanding, CCU)은 인류의 지적 자산을 계승하고 발전시키는 데 필수적입니다. \n",
    "\n",
    "최근 대규모 언어 모델(LLM)은 **'탁월한 이해력과 의미 분석 능력(remarkable comprehension and semantic capabilities)'**을 바탕으로 자연어 처리(NLP)의 여러 난제를 해결하며 경이로운 발전을 이루었습니다. 이는 오랫동안 전문가의 영역으로 여겨졌던 CCU 분야의 고질적인 장벽을 돌파할 독보적인 잠재력을 시사합니다.\n",
    "\n",
    "그러나 현재 LLM의 CCU 능력을 체계적이고 포괄적으로 평가할 표준화된 벤치마크가 부재하다는 심각한 연구 격차가 존재합니다. 이러한 평가 기준의 부재는 LLM을 활용한 CCU 연구의 발전을 저해하는 결정적인 장애물로 작용하며, 모델의 성능을 객관적으로 비교하거나 약점을 정밀하게 진단하는 것을 불가능하게 만듭니다.\n",
    "\n",
    "이러한 문제를 정면으로 해결하기 위해, 본 연구는 **C3Bench**라는 새로운 벤치마크 개발을 제안합니다. C3Bench는 LLM의 CCU 능력을 체계적으로 평가하기 위해 설계된 최초의 포괄적인 다중 과업(multi-task) 벤치마크이며, 특히 자연어 생성(natural language generation) 방식을 사용하여 단순한 선택형 평가를 넘어 모델의 실질적인 이해 및 생성 능력을 다각적으로 측정하도록 설계되었다는 점에서 기존 벤치마크와 근본적인 차별성을 갖습니다.\n",
    "\n",
    "**본 연구 제안의 최종 목표**는 C3Bench를 통해 현존하는 주요 LLM들의 CCU 성능에 대한 신뢰할 수 있는 기준선(baseline)을 수립하고, 이를 공개하여 향후 관련 연구 발전에 기여할 귀중한 통찰력을 제공하는 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 연구 배경 및 필요성 (Research Background and Necessity)\n",
    "\n",
    "본 섹션에서는 기존 연구의 한계를 면밀히 분석하고, 이를 통해 제안하는 C3Bench 벤치마크 개발이 왜 시급하고 중요한지를 논리적으로 입증하고자 합니다.\n",
    "\n",
    "### 2.1. 기존 NLP 벤치마크의 한계 분석\n",
    "\n",
    "지금까지 중국어 NLP를 위한 **CLUE, CUGE, C-Eval** 등 다양한 평가 벤치마크와 고전 중국어 분야의 **C-CLUE, CASIA-AHCDB** 같은 특정 목적의 벤치마크가 개발되었습니다. \n",
    "\n",
    "그러나 이들 벤치마크는 근본적인 패러다임의 한계에 부딪히고 있습니다:\n",
    "\n",
    "- **제한된 과업 범위 및 단순한 평가 방식**: C-CLUE와 같은 벤치마크는 개체명 인식(NER) 등 특정 정보 추출 과업에 국한되거나 객관식 평가 방식을 채택하여, 번역이나 요약과 같은 LLM의 포괄적인 생성 능력을 측정하지 못합니다.\n",
    "\n",
    "- **평가 목적의 불일치**: CASIA-AHCDB와 같은 데이터셋은 고문서의 문자 인식과 같은 시각적 과업에 초점을 맞추고 있어, LLM의 순수한 텍스트 이해 및 생성 능력을 평가하는 데는 적합하지 않습니다.\n",
    "\n",
    "**결론적으로**, 기존의 평가 도구들은 LLM이 가진 진정한 CCU 잠재력을 평가하고 모델 간의 성능을 공정하게 비교하는 데 명백한 한계를 드러냅니다.\n",
    "\n",
    "### 2.2. 연구 격차 및 C3Bench의 필요성\n",
    "\n",
    "앞선 분석은 **'LLM을 위한 자연어 생성 기반의 포괄적인 다중 과업 CCU 벤치마크'가 부재**하다는 명확한 연구 격차를 정의합니다. \n",
    "\n",
    "현재의 평가 도구로는 LLM이 고전 중국어의 미묘한 뉘앙스를 이해하고, 다양한 요구에 맞춰 적절한 텍스트를 생성하는 능력을 제대로 측정할 수 없습니다. 이러한 격차를 해소하는 것은 CCU 분야에서 LLM의 잠재력을 완전히 탐색하고 기술 발전을 촉진하기 위한 필수적인 단계입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. C3Bench: 제안하는 벤치마크 설계 (C3Bench: Proposed Benchmark Design)\n",
    "\n",
    "본 섹션은 제안서의 핵심적인 기술 내용을 구체화하는 부분으로, C3Bench의 설계 원칙, 5가지 핵심 과업, 그리고 데이터셋 구축 과정 및 통계를 상세히 설명합니다.\n",
    "\n",
    "### 3.1. 핵심 과업 정의 (Definition of Core Tasks)\n",
    "\n",
    "C3Bench는 LLM의 CCU 능력을 종합적으로 평가하기 위해 **5가지 핵심 과업**으로 구성됩니다:\n",
    "\n",
    "1. **분류 (Classification)**  \n",
    "   주어진 고전 중국어 문장을 10개의 사전 정의된 범주(예: 시, 역사, 유교 등) 중 하나로 정확하게 할당하는 과업입니다.\n",
    "\n",
    "2. **검색 (Retrieval)**  \n",
    "   주어진 문장이 유래한 원문의 제목을 정확하게 식별하는 과업입니다.\n",
    "\n",
    "3. **개체명 인식 (Named Entity Recognition)**  \n",
    "   문장 내에 포함된 인명, 관직, 지명 등의 고유한 개체명을 추출하는 과업입니다.\n",
    "\n",
    "4. **구두점 찍기 (Punctuation)**  \n",
    "   구두점이 없는 원문에 문맥에 맞게 적절한 문장 부호를 삽입하는 과업입니다.\n",
    "\n",
    "5. **번역 (Translation)**  \n",
    "   주어진 고전 중국어 문장을 자연스러운 현대 중국어로 번역하는 과업입니다.\n",
    "\n",
    "이 5가지 과업은 고차원적 주제 이해(분류)와 지식 회상(검색)부터 미시적 텍스트 분석(개체명 인식, 구두점 찍기), 그리고 고도의 언어 간 생성 능력(번역)에 이르기까지 LLM의 능력을 종합적으로 진단하기 위해 전략적으로 선정되었습니다.\n",
    "\n",
    "### 3.2. 데이터셋 구축 절차 및 통계 (Dataset Construction Process and Statistics)\n",
    "\n",
    "#### 데이터셋 구축 절차\n",
    "\n",
    "C3Bench 데이터셋은 전문성과 신뢰도를 확보하기 위해 다음과 같은 엄격한 절차를 거쳐 구축되었습니다:\n",
    "\n",
    "1. **데이터 수집**: 인터넷을 통해 10개의 대표 분야에서 고문서 텍스트를 수집하고, 검색 과업의 정답이 될 원문 제목을 함께 보존\n",
    "\n",
    "2. **번역 데이터 확보**: 수집된 문장에 대해 온라인 검색 및 수동 주석 작업을 통해 현대 중국어 번역문을 확보\n",
    "\n",
    "3. **전문가 주석**: 전문 주석가를 고용하여 각 문장의 개체명을 정밀하게 레이블링\n",
    "\n",
    "4. **데이터 가공**: 구두점 찍기 과업을 위해 원문에서 모든 문장 부호를 제거하여 입력 데이터를 생성\n",
    "\n",
    "5. **이중 검증**: 모든 데이터에 대해 엄격하고 철저한 이중 검증(double-check)을 수행하여 데이터의 품질을 최고 수준으로 보장\n",
    "\n",
    "#### 데이터셋 통계\n",
    "\n",
    "그 결과, C3Bench는 총 10개 분야에 걸쳐 각 과업당 10,000개, **총 50,000개의 고품질 텍스트 쌍**으로 구성되었습니다.\n",
    "\n",
    "| 분야 (Domain) | 분류 | 검색 | 개체명 인식 | 구두점 찍기 | 번역 |\n",
    "|:---|---:|---:|---:|---:|---:|\n",
    "| 시 (Poetry) | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 |\n",
    "| 역사 (History) | 2,000 | 2,000 | 2,000 | 2,000 | 2,000 |\n",
    "| 불교 (Buddhism) | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 |\n",
    "| 유교 (Confucianism) | 2,000 | 2,000 | 2,000 | 2,000 | 2,000 |\n",
    "| 도교 (Taoism) | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 |\n",
    "| 의학 (Medicine) | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 |\n",
    "| 예술 (Art) | 500 | 500 | 500 | 500 | 500 |\n",
    "| 군사 (Military) | 500 | 500 | 500 | 500 | 500 |\n",
    "| 법률 (Law) | 500 | 500 | 500 | 500 | 500 |\n",
    "| 농업 (Agriculture) | 500 | 500 | 500 | 500 | 500 |\n",
    "| **총계 (Total)** | **10,000** | **10,000** | **10,000** | **10,000** | **10,000** |\n",
    "\n",
    "이처럼 체계적으로 설계된 C3Bench는 LLM의 CCU 능력을 다각적이면서도 엄격하게 평가할 수 있는 견고한 기반을 제공합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 평가 방법론 (Evaluation Methodology)\n",
    "\n",
    "본 섹션에서는 제안된 C3Bench를 활용하여 어떻게 객관적이고 공정한 방식으로 LLM의 성능 평가를 수행할 것인지에 대한 구체적인 방법론을 설명합니다.\n",
    "\n",
    "### 4.1. 평가 대상 모델 및 절차 (Models and Procedure for Evaluation)\n",
    "\n",
    "#### 평가 대상 모델\n",
    "\n",
    "평가의 포괄성을 확보하기 위해, 현재 널리 사용되는 **15개의 대표적인 LLM**을 평가 대상으로 선정합니다:\n",
    "\n",
    "1. **오픈소스 모델 (Open-source models)**  \n",
    "   LLaMA2-Chinese, Baichuan2, Qwen 등 학계 및 개발 커뮤니티에서 활발히 연구되고 있는 모델들\n",
    "\n",
    "2. **비공개 소스 모델 (Closed-source models)**  \n",
    "   GPT-3.5, GPT-4, ERNIE-bot-turbo 등 상업적으로 널리 사용되는 고성능 API 기반 모델들\n",
    "\n",
    "3. **지도 학습 모델 (Supervised methods)**  \n",
    "   범용 LLM의 성능을 가늠하기 위한 고성능 비교 기준선 역할을 합니다. GuwenBERT 기반 모델이나 4억 개의 번역 데이터로 미세 조정한 LLaMA-13B와 같이 특정 CCU 과업에 고도로 최적화된 이 모델들과의 성능 비교는 LLM의 현재 수준을 명확히 보여줄 것입니다.\n",
    "\n",
    "#### 평가 절차\n",
    "\n",
    "평가의 일관성과 공정성을 보장하기 위해 모든 모델에 동일한 절차를 적용합니다:\n",
    "\n",
    "- 각 과업별로 **표준화된 프롬프트(prompt)** 사용\n",
    "- 모든 실험에서 **일관된 추론 설정(inference settings)** 유지\n",
    "- 모델 외부의 변수를 통제하고 오직 모델 자체의 능력만을 공정하게 비교\n",
    "\n",
    "### 4.2. 평가 지표 (Evaluation Metrics)\n",
    "\n",
    "각 과업의 특성을 고려하여, 결과의 객관성을 보장할 수 있는 정량적 평가 지표를 사용합니다:\n",
    "\n",
    "| 과업 | 평가 지표 |\n",
    "|:---|:---|\n",
    "| 분류 (Classification) | 정확도 (Accuracy) |\n",
    "| 검색 (Retrieval) | 정확도 (Accuracy) |\n",
    "| 개체명 인식 (NER) | F1 점수 (F1 Score) |\n",
    "| 구두점 찍기 (Punctuation) | F1 점수 (F1 Score) |\n",
    "| 번역 (Translation) | BLEU 점수 |\n",
    "\n",
    "이러한 체계적인 평가 방법론을 통해 도출된 결과는 높은 신뢰도를 가지며, CCU 분야의 LLM 성능에 대한 학술적으로 의미 있는 분석의 기반이 될 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 기대 결과 및 연구 기여 (Expected Results and Contributions)\n",
    "\n",
    "C3Bench 기반의 포괄적인 평가는 LLM의 CCU 능력에 대한 전례 없는 수준의 심층적인 분석을 가능하게 할 것입니다.\n",
    "\n",
    "### 5.1. LLM의 CCU 능력에 대한 심층적 통찰력 확보\n",
    "\n",
    "본 평가는 LLM의 CCU 능력에 대한 다음의 핵심적인 발견들을 명확히 입증할 것입니다:\n",
    "\n",
    "#### 1) 현존 LLM의 CCU 능력 한계\n",
    "\n",
    "대부분의 LLM이 C3Bench에서 **평균 50점 미만의 낮은 성능**을 기록할 것입니다. 특히 고도의 문맥 이해와 배경지식을 요구하는 검색과 번역 과업에서의 저조한 성과는, CCU가 최신 LLM에게도 여전히 매우 도전적인 과제임을 명확히 증명합니다.\n",
    "\n",
    "#### 2) 지도 학습 모델 대비 열세\n",
    "\n",
    "본 평가는 범용 LLM이 특정 전문 분야에서 최적화된 모델을 능가하기 위해 추가적인 개선이 필요함을 정량적으로 제시할 것입니다:\n",
    "\n",
    "- **분류 과업**: 지도 학습 모델은 최고 LLM보다 0.26점 높은 **0.8266의 정확도** 기록\n",
    "- **번역 과업**: 지도 학습 모델은 **52.02 BLEU 점수**를 달성하여 LLM 최고 점수인 34.58을 압도\n",
    "\n",
    "#### 3) 불균형한 CCU 능력\n",
    "\n",
    "특정 LLM이 어떤 과업(예: 구두점 찍기)에서는 높은 성능을 보이지만, 다른 과업(예: 개체명 인식, 번역)에서는 매우 저조한 성능을 보이는 등 **불균형한 능력 분포**가 발견될 것입니다. \n",
    "\n",
    "이는 LLM의 학습 데이터가 특정 CCU 과업에 편중되어 있음을 시사하며, 향후 모델 개발 시 데이터 균형의 중요성을 강조합니다.\n",
    "\n",
    "#### 4) CCU 분야의 특수성 강조\n",
    "\n",
    "평가 결과는 단순히 현대 중국어 데이터로 미세 조정한 모델이 고전 중국어의 고유한 어휘, 문법, 문화적 배경을 이해하는 데 명백한 한계를 보인다는 점을 실증적으로 증명할 것입니다. \n",
    "\n",
    "이는 CCU가 단순한 언어 변환 문제를 넘어, **특화된 데이터와 접근법을 요구하는 독립적인 연구 분야**임을 강력하게 뒷받침합니다.\n",
    "\n",
    "### 5.2. 학술적 및 산업적 기여\n",
    "\n",
    "본 연구는 다음과 같은 구체적인 학술적, 산업적 기여를 할 것입니다:\n",
    "\n",
    "#### 1) 표준 벤치마크 제공\n",
    "\n",
    "C3Bench는 향후 LLM 기반 CCU 연구를 위한 **최초의 표준화된 평가 기준**을 제공합니다. 이를 통해 연구자들은 자신의 모델 성능을 공정하고 일관된 척도로 측정하고 비교할 수 있게 되어, 연구 분야 전체의 발전을 가속화할 것입니다.\n",
    "\n",
    "#### 2) 공개 리더보드 및 베이스라인 구축\n",
    "\n",
    "15개 대표 LLM의 성능을 측정한 **포괄적인 베이스라인과 공개 리더보드**를 구축하여 제공합니다. 이는 후속 연구자들에게 명확한 출발점을 제시하고, 더 나은 CCU 모델 개발을 위한 건전한 기술 경쟁을 촉진하는 역할을 할 것입니다.\n",
    "\n",
    "#### 3) 미래 연구 방향 제시\n",
    "\n",
    "평가 결과에서 드러난 LLM의 다양한 약점과 한계에 대한 심층 분석을 제공합니다. 이는 향후 CCU 모델의 성능을 향상시키기 위해 어떤 부분에 집중해야 하는지에 대한 귀중한 통찰력과 구체적인 연구 방향을 제시합니다:\n",
    "\n",
    "- 특정 분야 데이터 보강\n",
    "- 새로운 학습 전략 개발\n",
    "- 데이터 균형 최적화\n",
    "\n",
    "이러한 기여들은 궁극적으로 **LLM 기반의 고전 중국어 이해 연구 생태계를 활성화**하고, 관련 기술이 학술적 탐구를 넘어 실질적인 응용 분야로 나아가는 데 중요한 발판이 될 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 결론 및 향후 과제 (Conclusion and Future Work)\n",
    "\n",
    "본 연구 제안서는 대규모 언어 모델(LLM) 시대에 고전 중국어 이해(CCU) 연구의 중요성을 재확인하고, 기존 평가 방법의 명백한 한계를 지적하며, 이를 극복하기 위한 새로운 솔루션으로 **C3Bench**를 제안했습니다.\n",
    "\n",
    "C3Bench는 5가지 핵심 과업과 10개 분야를 아우르는 **최초의 포괄적인 CCU 벤치마크**로서, LLM의 능력을 다각적이고 심층적으로 평가할 수 있는 표준화된 기틀을 마련한다는 점에서 큰 의의를 가집니다.\n",
    "\n",
    "### 향후 과제 (Future Work)\n",
    "\n",
    "물론 본 연구에도 몇 가지 한계점이 존재하며, 이는 향후 연구를 위한 중요한 과제가 될 것입니다:\n",
    "\n",
    "#### 1) 과업 범위 확장\n",
    "\n",
    "C3Bench는 5개의 대표적인 과업에 집중하였으나, **고문서 요약(summary)**과 같이 CCU 분야의 모든 과업 유형을 포함하지는 못했습니다. 향후 더 다양한 과업을 포함하여 벤치마크를 확장하는 연구가 필요합니다.\n",
    "\n",
    "#### 2) 다양한 프롬프팅 전략 탐색\n",
    "\n",
    "본 평가는 모델의 **제로샷(zero-shot) 능력**에 초점을 맞추었으며, 소수의 예시를 제공했을 때의 성능을 측정하는 **퓨샷(few-shot) 시나리오**는 탐색하지 않았습니다. 다양한 프롬프팅 전략에 따른 성능 변화를 분석하는 것도 의미 있는 후속 연구가 될 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 최종 요약\n",
    "\n",
    "궁극적으로 C3Bench는 LLM 기반 CCU 연구에 다음을 제공함으로써 해당 분야의 학술적, 기술적 성장을 가속화하는 핵심적인 초석이 될 것입니다:\n",
    "\n",
    "- ✓ 필수적인 표준 벤치마크\n",
    "- ✓ 포괄적인 베이스라인\n",
    "- ✓ 미래 연구를 위한 심층적 통찰력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77247c",
   "metadata": {},
   "source": "# C3Bench: 대규모 언어 모델(LLM)을 위한 포괄적인 고전 중국어 이해(CCU) 벤치마크 개발 제안서\n\n---\n\n## 1. 서론 (Introduction)\n\n고전 중국어는 5000년 중국 문명의 정수와 문화유산을 담고 있는 핵심 매개체로서, 이를 깊이 있게 이해하는 능력(Classical Chinese Understanding, CCU)은 인류의 지적 자산을 계승하고 발전시키는 데 필수적입니다. \n\n최근 대규모 언어 모델(LLM)은 **'탁월한 이해력과 의미 분석 능력(remarkable comprehension and semantic capabilities)'**을 바탕으로 자연어 처리(NLP)의 여러 난제를 해결하며 경이로운 발전을 이루었습니다. 이는 오랫동안 전문가의 영역으로 여겨졌던 CCU 분야의 고질적인 장벽을 돌파할 독보적인 잠재력을 시사합니다.\n\n그러나 현재 LLM의 CCU 능력을 체계적이고 포괄적으로 평가할 표준화된 벤치마크가 부재하다는 심각한 연구 격차가 존재합니다. 이러한 평가 기준의 부재는 LLM을 활용한 CCU 연구의 발전을 저해하는 결정적인 장애물로 작용하며, 모델의 성능을 객관적으로 비교하거나 약점을 정밀하게 진단하는 것을 불가능하게 만듭니다.\n\n이러한 문제를 정면으로 해결하기 위해, 본 연구는 **C3Bench**라는 새로운 벤치마크 개발을 제안합니다. C3Bench는 LLM의 CCU 능력을 체계적으로 평가하기 위해 설계된 최초의 포괄적인 다중 과업(multi-task) 벤치마크이며, 특히 자연어 생성(natural language generation) 방식을 사용하여 단순한 선택형 평가를 넘어 모델의 실질적인 이해 및 생성 능력을 다각적으로 측정하도록 설계되었다는 점에서 기존 벤치마크와 근본적인 차별성을 갖습니다.\n\n**본 연구 제안의 최종 목표**는 C3Bench를 통해 현존하는 주요 LLM들의 CCU 성능에 대한 신뢰할 수 있는 기준선(baseline)을 수립하고, 이를 공개하여 향후 관련 연구 발전에 기여할 귀중한 통찰력을 제공하는 것입니다.\n\n---\n\n## 2. 연구 배경 및 필요성 (Research Background and Necessity)\n\n본 섹션에서는 기존 연구의 한계를 면밀히 분석하고, 이를 통해 제안하는 C3Bench 벤치마크 개발이 왜 시급하고 중요한지를 논리적으로 입증하고자 합니다.\n\n### 2.1. 기존 NLP 벤치마크의 한계 분석\n\n지금까지 중국어 NLP를 위한 **CLUE, CUGE, C-Eval** 등 다양한 평가 벤치마크와 고전 중국어 분야의 **C-CLUE, CASIA-AHCDB** 같은 특정 목적의 벤치마크가 개발되었습니다. \n\n그러나 이들 벤치마크는 근본적인 패러다임의 한계에 부딪히고 있습니다:\n\n- **제한된 과업 범위 및 단순한 평가 방식**: C-CLUE와 같은 벤치마크는 개체명 인식(NER) 등 특정 정보 추출 과업에 국한되거나 객관식 평가 방식을 채택하여, 번역이나 요약과 같은 LLM의 포괄적인 생성 능력을 측정하지 못합니다.\n\n- **평가 목적의 불일치**: CASIA-AHCDB와 같은 데이터셋은 고문서의 문자 인식과 같은 시각적 과업에 초점을 맞추고 있어, LLM의 순수한 텍스트 이해 및 생성 능력을 평가하는 데는 적합하지 않습니다.\n\n**결론적으로**, 기존의 평가 도구들은 LLM이 가진 진정한 CCU 잠재력을 평가하고 모델 간의 성능을 공정하게 비교하는 데 명백한 한계를 드러냅니다.\n\n### 2.2. 연구 격차 및 C3Bench의 필요성\n\n앞선 분석은 **'LLM을 위한 자연어 생성 기반의 포괄적인 다중 과업 CCU 벤치마크'가 부재**하다는 명확한 연구 격차를 정의합니다. \n\n현재의 평가 도구로는 LLM이 고전 중국어의 미묘한 뉘앙스를 이해하고, 다양한 요구에 맞춰 적절한 텍스트를 생성하는 능력을 제대로 측정할 수 없습니다. 이러한 격차를 해소하는 것은 CCU 분야에서 LLM의 잠재력을 완전히 탐색하고 기술 발전을 촉진하기 위한 필수적인 단계입니다.\n\n---\n\n## 3. C3Bench: 제안하는 벤치마크 설계 (C3Bench: Proposed Benchmark Design)\n\n본 섹션은 제안서의 핵심적인 기술 내용을 구체화하는 부분으로, C3Bench의 설계 원칙, 5가지 핵심 과업, 그리고 데이터셋 구축 과정 및 통계를 상세히 설명합니다.\n\n### 3.1. 핵심 과업 정의 (Definition of Core Tasks)\n\nC3Bench는 LLM의 CCU 능력을 종합적으로 평가하기 위해 **5가지 핵심 과업**으로 구성됩니다:\n\n1. **분류 (Classification)**  \n   주어진 고전 중국어 문장을 10개의 사전 정의된 범주(예: 시, 역사, 유교 등) 중 하나로 정확하게 할당하는 과업입니다.\n\n2. **검색 (Retrieval)**  \n   주어진 문장이 유래한 원문의 제목을 정확하게 식별하는 과업입니다.\n\n3. **개체명 인식 (Named Entity Recognition)**  \n   문장 내에 포함된 인명, 관직, 지명 등의 고유한 개체명을 추출하는 과업입니다.\n\n4. **구두점 찍기 (Punctuation)**  \n   구두점이 없는 원문에 문맥에 맞게 적절한 문장 부호를 삽입하는 과업입니다.\n\n5. **번역 (Translation)**  \n   주어진 고전 중국어 문장을 자연스러운 현대 중국어로 번역하는 과업입니다.\n\n이 5가지 과업은 고차원적 주제 이해(분류)와 지식 회상(검색)부터 미시적 텍스트 분석(개체명 인식, 구두점 찍기), 그리고 고도의 언어 간 생성 능력(번역)에 이르기까지 LLM의 능력을 종합적으로 진단하기 위해 전략적으로 선정되었습니다.\n\n### 3.2. 데이터셋 구축 절차 및 통계 (Dataset Construction Process and Statistics)\n\n#### 데이터셋 구축 절차\n\nC3Bench 데이터셋은 전문성과 신뢰도를 확보하기 위해 다음과 같은 엄격한 절차를 거쳐 구축되었습니다:\n\n1. **데이터 수집**: 인터넷을 통해 10개의 대표 분야에서 고문서 텍스트를 수집하고, 검색 과업의 정답이 될 원문 제목을 함께 보존\n\n2. **번역 데이터 확보**: 수집된 문장에 대해 온라인 검색 및 수동 주석 작업을 통해 현대 중국어 번역문을 확보\n\n3. **전문가 주석**: 전문 주석가를 고용하여 각 문장의 개체명을 정밀하게 레이블링\n\n4. **데이터 가공**: 구두점 찍기 과업을 위해 원문에서 모든 문장 부호를 제거하여 입력 데이터를 생성\n\n5. **이중 검증**: 모든 데이터에 대해 엄격하고 철저한 이중 검증(double-check)을 수행하여 데이터의 품질을 최고 수준으로 보장\n\n#### 데이터셋 통계\n\n그 결과, C3Bench는 총 10개 분야에 걸쳐 각 과업당 10,000개, **총 50,000개의 고품질 텍스트 쌍**으로 구성되었습니다.\n\n| 분야 (Domain) | 분류 | 검색 | 개체명 인식 | 구두점 찍기 | 번역 |\n|:---|---:|---:|---:|---:|---:|\n| 시 (Poetry) | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 |\n| 역사 (History) | 2,000 | 2,000 | 2,000 | 2,000 | 2,000 |\n| 불교 (Buddhism) | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 |\n| 유교 (Confucianism) | 2,000 | 2,000 | 2,000 | 2,000 | 2,000 |\n| 도교 (Taoism) | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 |\n| 의학 (Medicine) | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 |\n| 예술 (Art) | 500 | 500 | 500 | 500 | 500 |\n| 군사 (Military) | 500 | 500 | 500 | 500 | 500 |\n| 법률 (Law) | 500 | 500 | 500 | 500 | 500 |\n| 농업 (Agriculture) | 500 | 500 | 500 | 500 | 500 |\n| **총계 (Total)** | **10,000** | **10,000** | **10,000** | **10,000** | **10,000** |\n\n이처럼 체계적으로 설계된 C3Bench는 LLM의 CCU 능력을 다각적이면서도 엄격하게 평가할 수 있는 견고한 기반을 제공합니다.\n\n---\n\n## 4. 평가 방법론 (Evaluation Methodology)\n\n본 섹션에서는 제안된 C3Bench를 활용하여 어떻게 객관적이고 공정한 방식으로 LLM의 성능 평가를 수행할 것인지에 대한 구체적인 방법론을 설명합니다.\n\n### 4.1. 평가 대상 모델 및 절차 (Models and Procedure for Evaluation)\n\n#### 평가 대상 모델\n\n평가의 포괄성을 확보하기 위해, 현재 널리 사용되는 **15개의 대표적인 LLM**을 평가 대상으로 선정합니다:\n\n1. **오픈소스 모델 (Open-source models)**  \n   LLaMA2-Chinese, Baichuan2, Qwen 등 학계 및 개발 커뮤니티에서 활발히 연구되고 있는 모델들\n\n2. **비공개 소스 모델 (Closed-source models)**  \n   GPT-3.5, GPT-4, ERNIE-bot-turbo 등 상업적으로 널리 사용되는 고성능 API 기반 모델들\n\n3. **지도 학습 모델 (Supervised methods)**  \n   범용 LLM의 성능을 가늠하기 위한 고성능 비교 기준선 역할을 합니다. GuwenBERT 기반 모델이나 4억 개의 번역 데이터로 미세 조정한 LLaMA-13B와 같이 특정 CCU 과업에 고도로 최적화된 이 모델들과의 성능 비교는 LLM의 현재 수준을 명확히 보여줄 것입니다.\n\n#### 평가 절차\n\n평가의 일관성과 공정성을 보장하기 위해 모든 모델에 동일한 절차를 적용합니다:\n\n- 각 과업별로 **표준화된 프롬프트(prompt)** 사용\n- 모든 실험에서 **일관된 추론 설정(inference settings)** 유지\n- 모델 외부의 변수를 통제하고 오직 모델 자체의 능력만을 공정하게 비교\n\n### 4.2. 평가 지표 (Evaluation Metrics)\n\n각 과업의 특성을 고려하여, 결과의 객관성을 보장할 수 있는 정량적 평가 지표를 사용합니다:\n\n| 과업 | 평가 지표 |\n|:---|:---|\n| 분류 (Classification) | 정확도 (Accuracy) |\n| 검색 (Retrieval) | 정확도 (Accuracy) |\n| 개체명 인식 (NER) | F1 점수 (F1 Score) |\n| 구두점 찍기 (Punctuation) | F1 점수 (F1 Score) |\n| 번역 (Translation) | BLEU 점수 |\n\n이러한 체계적인 평가 방법론을 통해 도출된 결과는 높은 신뢰도를 가지며, CCU 분야의 LLM 성능에 대한 학술적으로 의미 있는 분석의 기반이 될 것입니다.\n\n---\n\n## 5. 기대 결과 및 연구 기여 (Expected Results and Contributions)\n\nC3Bench 기반의 포괄적인 평가는 LLM의 CCU 능력에 대한 전례 없는 수준의 심층적인 분석을 가능하게 할 것입니다.\n\n### 5.1. LLM의 CCU 능력에 대한 심층적 통찰력 확보\n\n본 평가는 LLM의 CCU 능력에 대한 다음의 핵심적인 발견들을 명확히 입증할 것입니다:\n\n#### 1) 현존 LLM의 CCU 능력 한계\n\n대부분의 LLM이 C3Bench에서 **평균 50점 미만의 낮은 성능**을 기록할 것입니다. 특히 고도의 문맥 이해와 배경지식을 요구하는 검색과 번역 과업에서의 저조한 성과는, CCU가 최신 LLM에게도 여전히 매우 도전적인 과제임을 명확히 증명합니다.\n\n#### 2) 지도 학습 모델 대비 열세\n\n본 평가는 범용 LLM이 특정 전문 분야에서 최적화된 모델을 능가하기 위해 추가적인 개선이 필요함을 정량적으로 제시할 것입니다:\n\n- **분류 과업**: 지도 학습 모델은 최고 LLM보다 0.26점 높은 **0.8266의 정확도** 기록\n- **번역 과업**: 지도 학습 모델은 **52.02 BLEU 점수**를 달성하여 LLM 최고 점수인 34.58을 압도\n\n#### 3) 불균형한 CCU 능력\n\n특정 LLM이 어떤 과업(예: 구두점 찍기)에서는 높은 성능을 보이지만, 다른 과업(예: 개체명 인식, 번역)에서는 매우 저조한 성능을 보이는 등 **불균형한 능력 분포**가 발견될 것입니다. \n\n이는 LLM의 학습 데이터가 특정 CCU 과업에 편중되어 있음을 시사하며, 향후 모델 개발 시 데이터 균형의 중요성을 강조합니다.\n\n#### 4) CCU 분야의 특수성 강조\n\n평가 결과는 단순히 현대 중국어 데이터로 미세 조정한 모델이 고전 중국어의 고유한 어휘, 문법, 문화적 배경을 이해하는 데 명백한 한계를 보인다는 점을 실증적으로 증명할 것입니다. \n\n이는 CCU가 단순한 언어 변환 문제를 넘어, **특화된 데이터와 접근법을 요구하는 독립적인 연구 분야**임을 강력하게 뒷받침합니다.\n\n### 5.2. 학술적 및 산업적 기여\n\n본 연구는 다음과 같은 구체적인 학술적, 산업적 기여를 할 것입니다:\n\n#### 1) 표준 벤치마크 제공\n\nC3Bench는 향후 LLM 기반 CCU 연구를 위한 **최초의 표준화된 평가 기준**을 제공합니다. 이를 통해 연구자들은 자신의 모델 성능을 공정하고 일관된 척도로 측정하고 비교할 수 있게 되어, 연구 분야 전체의 발전을 가속화할 것입니다.\n\n#### 2) 공개 리더보드 및 베이스라인 구축\n\n15개 대표 LLM의 성능을 측정한 **포괄적인 베이스라인과 공개 리더보드**를 구축하여 제공합니다. 이는 후속 연구자들에게 명확한 출발점을 제시하고, 더 나은 CCU 모델 개발을 위한 건전한 기술 경쟁을 촉진하는 역할을 할 것입니다.\n\n#### 3) 미래 연구 방향 제시\n\n평가 결과에서 드러난 LLM의 다양한 약점과 한계에 대한 심층 분석을 제공합니다. 이는 향후 CCU 모델의 성능을 향상시키기 위해 어떤 부분에 집중해야 하는지에 대한 귀중한 통찰력과 구체적인 연구 방향을 제시합니다:\n\n- 특정 분야 데이터 보강\n- 새로운 학습 전략 개발\n- 데이터 균형 최적화\n\n이러한 기여들은 궁극적으로 **LLM 기반의 고전 중국어 이해 연구 생태계를 활성화**하고, 관련 기술이 학술적 탐구를 넘어 실질적인 응용 분야로 나아가는 데 중요한 발판이 될 것입니다.\n\n---\n\n## 6. 결론 및 향후 과제 (Conclusion and Future Work)\n\n본 연구 제안서는 대규모 언어 모델(LLM) 시대에 고전 중국어 이해(CCU) 연구의 중요성을 재확인하고, 기존 평가 방법의 명백한 한계를 지적하며, 이를 극복하기 위한 새로운 솔루션으로 **C3Bench**를 제안했습니다.\n\nC3Bench는 5가지 핵심 과업과 10개 분야를 아우르는 **최초의 포괄적인 CCU 벤치마크**로서, LLM의 능력을 다각적이고 심층적으로 평가할 수 있는 표준화된 기틀을 마련한다는 점에서 큰 의의를 가집니다.\n\n### 향후 과제 (Future Work)\n\n물론 본 연구에도 몇 가지 한계점이 존재하며, 이는 향후 연구를 위한 중요한 과제가 될 것입니다:\n\n#### 1) 과업 범위 확장\n\nC3Bench는 5개의 대표적인 과업에 집중하였으나, **고문서 요약(summary)**과 같이 CCU 분야의 모든 과업 유형을 포함하지는 못했습니다. 향후 더 다양한 과업을 포함하여 벤치마크를 확장하는 연구가 필요합니다.\n\n#### 2) 다양한 프롬프팅 전략 탐색\n\n본 평가는 모델의 **제로샷(zero-shot) 능력**에 초점을 맞추었으며, 소수의 예시를 제공했을 때의 성능을 측정하는 **퓨샷(few-shot) 시나리오**는 탐색하지 않았습니다. 다양한 프롬프팅 전략에 따른 성능 변화를 분석하는 것도 의미 있는 후속 연구가 될 것입니다.\n\n---\n\n### 최종 요약\n\n궁극적으로 C3Bench는 LLM 기반 CCU 연구에 다음을 제공함으로써 해당 분야의 학술적, 기술적 성장을 가속화하는 핵심적인 초석이 될 것입니다:\n\n- 필수적인 표준 벤치마크\n- 포괄적인 베이스라인\n- 미래 연구를 위한 심층적 통찰력"
  },
  {
   "cell_type": "markdown",
   "id": "4cc9f391",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}