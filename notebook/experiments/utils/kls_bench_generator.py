"""
KLSBench: Korean Classical Literature Understanding Benchmark Generator
í•œêµ­ ê³ ì „ ë¬¸í—Œ ì´í•´ ë²¤ì¹˜ë§ˆí¬ ìƒì„±ê¸°

C3Benchë¥¼ ì°¸ê³ í•˜ì—¬ 5ê°€ì§€ í•µì‹¬ íƒœìŠ¤í¬ë¡œ êµ¬ì„±:
1. Classification (ë¶„ë¥˜): ë¬¸ì²´ ë¶„ë¥˜
2. Retrieval (ê²€ìƒ‰): ì¶œì²˜ ì‹ë³„
3. Punctuation (êµ¬ë‘ì ): ë°±ë¬¸ì— êµ¬ë‘ì  ë³µì›
4. NLI/STS: ìì—°ì–¸ì–´ì¶”ë¡  ë° ì˜ë¯¸ ìœ ì‚¬ë„
5. Translation (ë²ˆì—­): í•œë¬¸-í•œê¸€-ì˜ë¬¸ ë²ˆì—­

Target: ì´ 10,000ê°œ í•­ëª© (ê° íƒœìŠ¤í¬ë‹¹ 2,000ê°œ)
"""

import pandas as pd
import json
import random
import re
from pathlib import Path
from typing import List, Dict, Tuple
from collections import Counter
import numpy as np

# ëœë¤ ì‹œë“œ ê³ ì •
random.seed(42)
np.random.seed(42)


class KLSBenchGenerator:
    """í•œêµ­ ê³ ì „ ë¬¸í—Œ ë²¤ì¹˜ë§ˆí¬ ìƒì„±ê¸°"""

    def __init__(self,
                 translated_csv_path: str,
                 external_csv_path: str,
                 nli_examples_path: str,
                 output_dir: str):
        """
        Args:
            translated_csv_path: ê³¼ê±°ì‹œí—˜ ë²ˆì—­ ë°ì´í„° ê²½ë¡œ
            external_csv_path: ì‚¬ì„œ ì›ë¬¸ ë°ì´í„° ê²½ë¡œ
            nli_examples_path: NLI ì˜ˆì‹œ ë°ì´í„° ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.translated_csv_path = translated_csv_path
        self.external_csv_path = external_csv_path
        self.nli_examples_path = nli_examples_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ë°ì´í„° ë¡œë“œ
        self.load_data()

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")

        # ê³¼ê±°ì‹œí—˜ ë°ì´í„°
        self.translated_df = pd.read_csv(self.translated_csv_path)
        print(f"  - ê³¼ê±°ì‹œí—˜ ë°ì´í„°: {len(self.translated_df)} í•­ëª©")

        # ì‚¬ì„œ ë°ì´í„°
        self.external_df = pd.read_csv(self.external_csv_path)
        print(f"  - ì‚¬ì„œ ë°ì´í„°: {len(self.external_df)} í•­ëª©")

        # NLI ì˜ˆì‹œ
        with open(self.nli_examples_path, 'r', encoding='utf-8') as f:
            nli_data = json.load(f)
            self.nli_examples = nli_data.get('examples', [])
        print(f"  - NLI ì˜ˆì‹œ: {len(self.nli_examples)} í•­ëª©")

        # ë°ì´í„° ì „ì²˜ë¦¬
        self.preprocess_data()

    def preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        print("\nğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

        # ê³¼ê±°ì‹œí—˜ ë°ì´í„°: ë¹ˆ ê°’ ì œê±°
        self.translated_df = self.translated_df.dropna(subset=['category'])
        self.translated_df = self.translated_df[
            (self.translated_df['abstract'].notna()) |
            (self.translated_df['content'].notna())
        ]

        # ì‚¬ì„œ ë°ì´í„°: ë¹ˆ ê°’ ì œê±°
        self.external_df = self.external_df.dropna(subset=['Original', 'Book'])

        print(f"  - ì „ì²˜ë¦¬ í›„ ê³¼ê±°ì‹œí—˜ ë°ì´í„°: {len(self.translated_df)} í•­ëª©")
        print(f"  - ì „ì²˜ë¦¬ í›„ ì‚¬ì„œ ë°ì´í„°: {len(self.external_df)} í•­ëª©")

    def generate_classification_task(self, target_size: int = 2000) -> List[Dict]:
        """
        Task 1: ë¶„ë¥˜ (Classification)
        ë¬¸ì²´(è³¦/è©©/ç–‘/ç¾©) ë¶„ë¥˜ íƒœìŠ¤í¬ ìƒì„±
        """
        print(f"\nğŸ“‹ [1/5] ë¶„ë¥˜(Classification) íƒœìŠ¤í¬ ìƒì„± ì¤‘... (ëª©í‘œ: {target_size}ê°œ)")

        task_data = []

        # ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œë§
        categories = self.translated_df['category'].unique()
        category_counts = self.translated_df['category'].value_counts()

        print(f"  ì¹´í…Œê³ ë¦¬: {list(categories)}")
        print(f"  ë¶„í¬: {dict(category_counts)}")

        # ê· ë“± ìƒ˜í”Œë§ì„ ìœ„í•œ ê³„ì‚°
        samples_per_category = target_size // len(categories)

        for category in categories:
            category_df = self.translated_df[self.translated_df['category'] == category]

            # ìƒ˜í”Œ ìˆ˜ ê²°ì •
            n_samples = min(samples_per_category, len(category_df))
            sampled_df = category_df.sample(n=n_samples, random_state=42)

            for idx, row in sampled_df.iterrows():
                # ì…ë ¥ í…ìŠ¤íŠ¸: abstract ë˜ëŠ” content ì‚¬ìš©
                input_text = row['abstract'] if pd.notna(row['abstract']) and row['abstract'].strip() else row['content']

                if pd.notna(input_text) and input_text.strip():
                    task_data.append({
                        'task': 'classification',
                        'id': f"cls_{len(task_data)+1:04d}",
                        'input': input_text.strip(),
                        'label': category,
                        'question_id': row.get('question_id', ''),
                        'metadata': {
                            'has_korean': pd.notna(row.get('abstract_ko')) or pd.notna(row.get('content_ko')),
                            'has_english': pd.notna(row.get('abstract_en')) or pd.notna(row.get('content_en'))
                        }
                    })

        print(f"  âœ“ ìƒì„± ì™„ë£Œ: {len(task_data)} í•­ëª©")
        return task_data[:target_size]

    def generate_retrieval_task(self, target_size: int = 2000) -> List[Dict]:
        """
        Task 2: ê²€ìƒ‰ (Retrieval)
        ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ì¶œì²˜(Book/Chapter) ì‹ë³„ íƒœìŠ¤í¬ ìƒì„±
        """
        print(f"\nğŸ” [2/5] ê²€ìƒ‰(Retrieval) íƒœìŠ¤í¬ ìƒì„± ì¤‘... (ëª©í‘œ: {target_size}ê°œ)")

        task_data = []

        # ì‚¬ì„œ ë°ì´í„°ì—ì„œ ì¶”ì¶œ
        available_books = self.external_df['Book'].unique()
        print(f"  ì±… ëª©ë¡: {list(available_books)}")

        # ì±…ë³„ ê· ë“± ìƒ˜í”Œë§
        samples_per_book = target_size // len(available_books)

        for book in available_books:
            book_df = self.external_df[self.external_df['Book'] == book]

            # ìƒ˜í”Œ ìˆ˜ ê²°ì •
            n_samples = min(samples_per_book, len(book_df))
            sampled_df = book_df.sample(n=n_samples, random_state=42)

            for idx, row in sampled_df.iterrows():
                original_text = row['Original']

                if pd.notna(original_text) and original_text.strip():
                    # ì •ë‹µ: Book + Chapter
                    answer = f"{row['Book']}"
                    if pd.notna(row.get('Chapter')):
                        answer += f" - {row['Chapter']}"

                    task_data.append({
                        'task': 'retrieval',
                        'id': f"ret_{len(task_data)+1:04d}",
                        'input': original_text.strip(),
                        'answer': answer,
                        'book': row['Book'],
                        'chapter': row.get('Chapter', ''),
                        'volume': row.get('Volume', ''),
                        'metadata': {
                            'has_translation': pd.notna(row.get('Original_trans')),
                            'has_comment': pd.notna(row.get('Comment'))
                        }
                    })

        print(f"  âœ“ ìƒì„± ì™„ë£Œ: {len(task_data)} í•­ëª©")
        return task_data[:target_size]

    def generate_punctuation_task(self, target_size: int = 2000) -> List[Dict]:
        """
        Task 3: êµ¬ë‘ì  ì°ê¸° (Punctuation)
        ë°±ë¬¸(êµ¬ë‘ì  ì—†ëŠ” í•œë¬¸)ì— êµ¬ë‘ì  ë³µì› íƒœìŠ¤í¬ ìƒì„±

        Original (êµ¬ë‘ì  ì—†ëŠ” í•œë¬¸) â†’ Original_quotation (êµ¬ë‘ì  ìˆëŠ” í•œë¬¸)
        Comment (êµ¬ë‘ì  ì—†ëŠ” ì£¼ì„) â†’ Comment_quotation (êµ¬ë‘ì  ìˆëŠ” ì£¼ì„)
        """
        print(f"\nâœï¸  [3/5] êµ¬ë‘ì (Punctuation) íƒœìŠ¤í¬ ìƒì„± ì¤‘... (ëª©í‘œ: {target_size}ê°œ)")

        task_data = []

        # 1) ì‚¬ì„œ ë°ì´í„°: Original (ë°±ë¬¸) â†’ Original_quotation (êµ¬ë‘ì  ìˆëŠ” í•œë¬¸)
        external_with_quotation = self.external_df[
            (self.external_df['Original'].notna()) &
            (self.external_df['Original_quotation'].notna())
        ]

        for idx, row in external_with_quotation.iterrows():
            if len(task_data) >= target_size * 0.5:  # ì „ì²´ì˜ 50%
                break

            original_text = row['Original'].strip()
            quotation_text = row['Original_quotation'].strip()

            # ë°±ë¬¸ê³¼ êµ¬ë‘ì ë³¸ì´ ë‹¤ë¥¸ ê²½ìš°ë§Œ ì‚¬ìš© (êµ¬ë‘ì ì´ ì‹¤ì œë¡œ ì¶”ê°€ëœ ê²½ìš°)
            if original_text and quotation_text and original_text != quotation_text:
                if len(original_text) > 10:  # ìµœì†Œ ê¸¸ì´ ì œí•œ
                    task_data.append({
                        'task': 'punctuation',
                        'id': f"pun_{len(task_data)+1:04d}",
                        'input': original_text,
                        'answer': quotation_text,
                        'language': 'classical_chinese',
                        'text_type': 'original',
                        'source': row.get('Book', ''),
                        'metadata': {
                            'chapter': row.get('Chapter', ''),
                            'korean_translation': row.get('Original_trans', ''),
                            'original_length': len(original_text),
                            'punctuated_length': len(quotation_text)
                        }
                    })

        # 2) ì‚¬ì„œ ë°ì´í„°: Comment (ë°±ë¬¸ ì£¼ì„) â†’ Comment_quotation (êµ¬ë‘ì  ìˆëŠ” ì£¼ì„)
        external_with_comment_quotation = self.external_df[
            (self.external_df['Comment'].notna()) &
            (self.external_df['Comment_quotation'].notna())
        ]

        for idx, row in external_with_comment_quotation.iterrows():
            if len(task_data) >= target_size:
                break

            comment_text = row['Comment'].strip()
            comment_quotation_text = row['Comment_quotation'].strip()

            # ë°±ë¬¸ê³¼ êµ¬ë‘ì ë³¸ì´ ë‹¤ë¥¸ ê²½ìš°ë§Œ ì‚¬ìš©
            if comment_text and comment_quotation_text and comment_text != comment_quotation_text:
                if len(comment_text) > 10:
                    task_data.append({
                        'task': 'punctuation',
                        'id': f"pun_{len(task_data)+1:04d}",
                        'input': comment_text,
                        'answer': comment_quotation_text,
                        'language': 'classical_chinese',
                        'text_type': 'comment',
                        'source': row.get('Book', ''),
                        'metadata': {
                            'chapter': row.get('Chapter', ''),
                            'korean_translation': row.get('Comment_trans', ''),
                            'original_length': len(comment_text),
                            'punctuated_length': len(comment_quotation_text)
                        }
                    })

        print(f"  âœ“ ìƒì„± ì™„ë£Œ: {len(task_data)} í•­ëª©")
        return task_data[:target_size]

    def generate_nli_sts_task(self, target_size: int = 2000) -> List[Dict]:
        """
        Task 4: NLI/STS (Natural Language Inference / Semantic Textual Similarity)
        ìì—°ì–¸ì–´ì¶”ë¡  ë° ì˜ë¯¸ ìœ ì‚¬ë„ íƒœìŠ¤í¬ ìƒì„±
        """
        print(f"\nğŸ§  [4/5] NLI/STS íƒœìŠ¤í¬ ìƒì„± ì¤‘... (ëª©í‘œ: {target_size}ê°œ)")

        task_data = []

        # ê¸°ì¡´ NLI ì˜ˆì‹œë¥¼ í…œí”Œë¦¿ìœ¼ë¡œ í™œìš©
        print(f"  ê¸°ì¡´ NLI ì˜ˆì‹œ {len(self.nli_examples)}ê°œë¥¼ í…œí”Œë¦¿ìœ¼ë¡œ í™œìš©")

        # 1) ê¸°ì¡´ ì˜ˆì‹œ ì¶”ê°€ (15ê°œ)
        for example in self.nli_examples:
            task_data.append({
                'task': 'nli',
                'id': f"nli_{len(task_data)+1:04d}",
                'premise': example['premise'],
                'hypothesis': example['hypothesis'],
                'label': example['label'],
                'source': example.get('source', ''),
                'difficulty': example.get('difficulty', 'medium'),
                'category': example.get('category', ''),
                'explanation': example.get('explanation', '')
            })

        # 2) ì‚¬ì„œ ë°ì´í„°ë¡œ NLI ìŒ ìƒì„±
        # Entailment: ì›ë¬¸ â†’ ë²ˆì—­ ê´€ê³„
        entailment_samples = self.external_df[
            self.external_df['Original_trans'].notna()
        ].sample(n=min(650, len(self.external_df)), random_state=42)

        for idx, row in entailment_samples.iterrows():
            if len(task_data) >= target_size:
                break

            task_data.append({
                'task': 'nli',
                'id': f"nli_{len(task_data)+1:04d}",
                'premise': row['Original'].strip(),
                'hypothesis': row['Original_trans'].strip(),
                'label': 'entailment',
                'source': row.get('Book', ''),
                'difficulty': 'easy',
                'category': 'translation_equivalence',
                'explanation': 'ì›ë¬¸ê³¼ ë²ˆì—­ì´ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼'
            })

        # 3) ê³¼ê±°ì‹œí—˜ ë°ì´í„°ë¡œ ë²ˆì—­ ê´€ê³„ NLI ìƒì„±
        for idx, row in self.translated_df.iterrows():
            if len(task_data) >= target_size * 0.66:  # ì „ì²´ì˜ 66%ê¹Œì§€
                break

            # í•œë¬¸ â†’ í•œê¸€ ë²ˆì—­
            if pd.notna(row['abstract']) and pd.notna(row['abstract_ko']):
                task_data.append({
                    'task': 'nli',
                    'id': f"nli_{len(task_data)+1:04d}",
                    'premise': row['abstract'].strip(),
                    'hypothesis': row['abstract_ko'].strip(),
                    'label': 'entailment',
                    'source': 'ê³¼ê±°ì‹œí—˜',
                    'difficulty': 'easy',
                    'category': 'translation_equivalence',
                    'explanation': 'í•œë¬¸ê³¼ í•œê¸€ ë²ˆì—­ì´ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼'
                })

            if len(task_data) >= target_size:
                break

            # í•œê¸€ â†’ ì˜ë¬¸ ë²ˆì—­
            if pd.notna(row['abstract_ko']) and pd.notna(row['abstract_en']):
                task_data.append({
                    'task': 'nli',
                    'id': f"nli_{len(task_data)+1:04d}",
                    'premise': row['abstract_ko'].strip(),
                    'hypothesis': row['abstract_en'].strip(),
                    'label': 'entailment',
                    'source': 'ê³¼ê±°ì‹œí—˜',
                    'difficulty': 'medium',
                    'category': 'cross_lingual_entailment',
                    'explanation': 'í•œê¸€ê³¼ ì˜ì–´ ë²ˆì—­ì´ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼'
                })

        # 4) Neutral/Contradiction ì˜ˆì‹œ ìƒì„± (ê°œì„ ëœ íœ´ë¦¬ìŠ¤í‹±)
        # Neutral: ë‹¤ë¥¸ ì±…ì˜ ë¬¸ì¥ë¼ë¦¬ ë§¤ì¹­
        books = self.external_df['Book'].unique()
        neutral_target = int(target_size * 0.20)  # ì „ì²´ì˜ 20%

        # ì±…ë³„ í˜ì–´ ìƒì„±
        book_pairs = []
        for i, book1 in enumerate(books):
            for book2 in books[i+1:]:
                book_pairs.append((book1, book2))

        for book1, book2 in book_pairs:
            if len([item for item in task_data if item['label'] == 'neutral']) >= neutral_target:
                break

            book1_samples = self.external_df[self.external_df['Book'] == book1].sample(
                n=min(100, len(self.external_df[self.external_df['Book'] == book1])), random_state=42
            )
            book2_samples = self.external_df[self.external_df['Book'] == book2].sample(
                n=min(100, len(self.external_df[self.external_df['Book'] == book2])), random_state=42
            )

            for (idx1, row1), (idx2, row2) in zip(book1_samples.iterrows(), book2_samples.iterrows()):
                if len([item for item in task_data if item['label'] == 'neutral']) >= neutral_target:
                    break

                if pd.notna(row1['Original_trans']) and pd.notna(row2['Original_trans']):
                    task_data.append({
                        'task': 'nli',
                        'id': f"nli_{len(task_data)+1:04d}",
                        'premise': row1['Original_trans'].strip(),
                        'hypothesis': row2['Original_trans'].strip(),
                        'label': 'neutral',
                        'source': f"{row1['Book']} vs {row2['Book']}",
                        'difficulty': 'medium',
                        'category': 'cross_text_relation',
                        'explanation': 'ë‹¤ë¥¸ ë¬¸í—Œì˜ ë¬¸ì¥ìœ¼ë¡œ ê´€ê³„ ë¶ˆëª…'
                    })

        # 5) Contradiction ì˜ˆì‹œ ìƒì„±
        # ë¶€ì •ë¬¸ ìƒì„± (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        contradiction_target = int(target_size * 0.15)  # ì „ì²´ì˜ 15%

        # ê³¼ê±°ì‹œí—˜ ë°ì´í„°ì—ì„œ contradiction ìŒ ìƒì„±
        # ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¼ë¦¬ ë§¤ì¹­ â†’ contradiction ê°€ëŠ¥ì„± ë†’ìŒ
        categories = self.translated_df['category'].unique()

        for cat1, cat2 in zip(categories[:-1], categories[1:]):
            if len([item for item in task_data if item['label'] == 'contradiction']) >= contradiction_target:
                break

            cat1_df = self.translated_df[
                (self.translated_df['category'] == cat1) &
                (self.translated_df['abstract_ko'].notna())
            ]
            cat2_df = self.translated_df[
                (self.translated_df['category'] == cat2) &
                (self.translated_df['abstract_ko'].notna())
            ]

            if len(cat1_df) == 0 or len(cat2_df) == 0:
                continue

            cat1_samples = cat1_df.sample(n=min(50, len(cat1_df)), random_state=42)
            cat2_samples = cat2_df.sample(n=min(50, len(cat2_df)), random_state=42)

            for (idx1, row1), (idx2, row2) in zip(cat1_samples.iterrows(), cat2_samples.iterrows()):
                if len([item for item in task_data if item['label'] == 'contradiction']) >= contradiction_target:
                    break

                # ë¶€ì • í‘œí˜„ ì¶”ê°€ë¡œ contradiction ìƒì„±
                premise = row1['abstract_ko'].strip()
                hypothesis = f"{row2['abstract_ko'].strip()}ì´ ì•„ë‹ˆë‹¤"  # ë¶€ì • ì¶”ê°€

                task_data.append({
                    'task': 'nli',
                    'id': f"nli_{len(task_data)+1:04d}",
                    'premise': premise,
                    'hypothesis': hypothesis,
                    'label': 'contradiction',
                    'source': 'ê³¼ê±°ì‹œí—˜ (ìƒì„±)',
                    'difficulty': 'medium',
                    'category': 'negation_based',
                    'explanation': 'ë¶€ì • í‘œí˜„ì„ í†µí•œ ëª¨ìˆœ ê´€ê³„'
                })

        print(f"  âœ“ ìƒì„± ì™„ë£Œ: {len(task_data)} í•­ëª©")
        print(f"    - Label ë¶„í¬: {Counter([item['label'] for item in task_data])}")
        return task_data[:target_size]

    def generate_translation_task(self, target_size: int = 2000) -> List[Dict]:
        """
        Task 5: ë²ˆì—­ (Translation)
        í•œë¬¸ â†” í•œê¸€ â†” ì˜ë¬¸ ë²ˆì—­ íƒœìŠ¤í¬ ìƒì„±
        """
        print(f"\nğŸŒ [5/5] ë²ˆì—­(Translation) íƒœìŠ¤í¬ ìƒì„± ì¤‘... (ëª©í‘œ: {target_size}ê°œ)")

        task_data = []

        # 1) ì‚¬ì„œ ë°ì´í„°: í•œë¬¸ â†’ í•œê¸€
        external_with_trans = self.external_df[
            self.external_df['Original_trans'].notna()
        ].sample(n=min(800, len(self.external_df)), random_state=42)

        for idx, row in external_with_trans.iterrows():
            task_data.append({
                'task': 'translation',
                'id': f"trans_{len(task_data)+1:04d}",
                'source_text': row['Original'].strip(),
                'target_text': row['Original_trans'].strip(),
                'source_lang': 'classical_chinese',
                'target_lang': 'korean',
                'book': row.get('Book', ''),
                'metadata': {
                    'has_comment': pd.notna(row.get('Comment'))
                }
            })

        # 2) ê³¼ê±°ì‹œí—˜ ë°ì´í„°: í•œë¬¸ â†’ í•œê¸€
        for idx, row in self.translated_df.iterrows():
            if len(task_data) >= target_size * 0.66:
                break

            # abstract: í•œë¬¸ â†’ í•œê¸€
            if pd.notna(row['abstract']) and pd.notna(row['abstract_ko']):
                task_data.append({
                    'task': 'translation',
                    'id': f"trans_{len(task_data)+1:04d}",
                    'source_text': row['abstract'].strip(),
                    'target_text': row['abstract_ko'].strip(),
                    'source_lang': 'classical_chinese',
                    'target_lang': 'korean',
                    'category': row.get('category', ''),
                    'metadata': {}
                })

            if len(task_data) >= target_size:
                break

            # content: í•œë¬¸ â†’ í•œê¸€
            if pd.notna(row['content']) and pd.notna(row['content_ko']):
                task_data.append({
                    'task': 'translation',
                    'id': f"trans_{len(task_data)+1:04d}",
                    'source_text': row['content'].strip(),
                    'target_text': row['content_ko'].strip(),
                    'source_lang': 'classical_chinese',
                    'target_lang': 'korean',
                    'category': row.get('category', ''),
                    'metadata': {}
                })

        # 3) ê³¼ê±°ì‹œí—˜ ë°ì´í„°: í•œê¸€ â†’ ì˜ë¬¸
        for idx, row in self.translated_df.iterrows():
            if len(task_data) >= target_size:
                break

            if pd.notna(row['abstract_ko']) and pd.notna(row['abstract_en']):
                task_data.append({
                    'task': 'translation',
                    'id': f"trans_{len(task_data)+1:04d}",
                    'source_text': row['abstract_ko'].strip(),
                    'target_text': row['abstract_en'].strip(),
                    'source_lang': 'korean',
                    'target_lang': 'english',
                    'category': row.get('category', ''),
                    'metadata': {}
                })

            if len(task_data) >= target_size:
                break

            if pd.notna(row['content_ko']) and pd.notna(row['content_en']):
                task_data.append({
                    'task': 'translation',
                    'id': f"trans_{len(task_data)+1:04d}",
                    'source_text': row['content_ko'].strip(),
                    'target_text': row['content_en'].strip(),
                    'source_lang': 'korean',
                    'target_lang': 'english',
                    'category': row.get('category', ''),
                    'metadata': {}
                })

        # 4) í•œë¬¸ â†’ ì˜ë¬¸ (ê°„ì ‘)
        for idx, row in self.translated_df.iterrows():
            if len(task_data) >= target_size:
                break

            if pd.notna(row['abstract']) and pd.notna(row['abstract_en']):
                task_data.append({
                    'task': 'translation',
                    'id': f"trans_{len(task_data)+1:04d}",
                    'source_text': row['abstract'].strip(),
                    'target_text': row['abstract_en'].strip(),
                    'source_lang': 'classical_chinese',
                    'target_lang': 'english',
                    'category': row.get('category', ''),
                    'metadata': {'indirect': True}
                })

        print(f"  âœ“ ìƒì„± ì™„ë£Œ: {len(task_data)} í•­ëª©")
        print(f"    - ì–¸ì–´ ìŒ ë¶„í¬:")
        lang_pairs = Counter([f"{item['source_lang']} â†’ {item['target_lang']}" for item in task_data])
        for pair, count in lang_pairs.items():
            print(f"      {pair}: {count}")

        return task_data[:target_size]

    def generate_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ìƒì„±"""
        print("\n" + "="*70)
        print("ğŸš€ KLSBench ë²¤ì¹˜ë§ˆí¬ ìƒì„± ì‹œì‘")
        print("="*70)

        # ê° íƒœìŠ¤í¬ ìƒì„±
        classification_data = self.generate_classification_task(2000)
        retrieval_data = self.generate_retrieval_task(2000)
        punctuation_data = self.generate_punctuation_task(2000)
        nli_sts_data = self.generate_nli_sts_task(2000)
        translation_data = self.generate_translation_task(2000)

        # ë²¤ì¹˜ë§ˆí¬ í†µí•©
        benchmark = {
            'benchmark_info': {
                'name': 'KLSBench',
                'full_name': 'Korean Classical Literature Understanding Benchmark',
                'version': '1.0',
                'description': 'C3Benchë¥¼ ì°¸ê³ í•˜ì—¬ ê°œë°œí•œ í•œêµ­ ê³ ì „ ë¬¸í—Œ ì´í•´ ë²¤ì¹˜ë§ˆí¬',
                'tasks': ['classification', 'retrieval', 'punctuation', 'nli', 'translation'],
                'total_size': len(classification_data) + len(retrieval_data) +
                             len(punctuation_data) + len(nli_sts_data) + len(translation_data),
                'languages': ['Classical Chinese', 'Korean', 'English'],
                'data_sources': ['ê³¼ê±°ì‹œí—˜ ë°ì´í„°', 'ì‚¬ì„œ(å››æ›¸) ë°ì´í„°', 'NLI ì˜ˆì‹œ']
            },
            'tasks': {
                'classification': {
                    'description': 'ì£¼ì–´ì§„ ê³ ì „ ë¬¸í—Œì˜ ë¬¸ì²´(è³¦/è©©/ç–‘/ç¾©)ë¥¼ ë¶„ë¥˜',
                    'size': len(classification_data),
                    'metric': 'Accuracy',
                    'data': classification_data
                },
                'retrieval': {
                    'description': 'ì£¼ì–´ì§„ ë¬¸ì¥ì´ ìœ ë˜í•œ ì›ë¬¸ì˜ ì¶œì²˜(Book/Chapter)ë¥¼ ì‹ë³„',
                    'size': len(retrieval_data),
                    'metric': 'Accuracy',
                    'data': retrieval_data
                },
                'punctuation': {
                    'description': 'êµ¬ë‘ì ì´ ì—†ëŠ” ë°±ë¬¸(ç™½æ–‡)ì— ì ì ˆí•œ êµ¬ë‘ì ì„ ë³µì›',
                    'size': len(punctuation_data),
                    'metric': 'F1 Score',
                    'data': punctuation_data
                },
                'nli': {
                    'description': 'ë‘ ë¬¸ì¥ ê°„ì˜ ë…¼ë¦¬ì  ê´€ê³„(entailment/contradiction/neutral)ë¥¼ íŒë‹¨',
                    'size': len(nli_sts_data),
                    'metric': 'Accuracy',
                    'data': nli_sts_data
                },
                'translation': {
                    'description': 'í•œë¬¸, í•œê¸€, ì˜ë¬¸ ê°„ì˜ ë²ˆì—­ ìˆ˜í–‰',
                    'size': len(translation_data),
                    'metric': 'BLEU Score',
                    'data': translation_data
                }
            }
        }

        # ì €ì¥
        self.save_benchmark(benchmark)

        # í†µê³„ ì¶œë ¥
        self.print_statistics(benchmark)

        return benchmark

    def save_benchmark(self, benchmark: Dict):
        """ë²¤ì¹˜ë§ˆí¬ ì €ì¥"""
        print("\n" + "="*70)
        print("ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ì €ì¥ ì¤‘...")
        print("="*70)

        # 1) ì „ì²´ ë²¤ì¹˜ë§ˆí¬ JSON ì €ì¥
        full_output_path = self.output_dir / 'kls_bench_full.json'
        with open(full_output_path, 'w', encoding='utf-8') as f:
            json.dump(benchmark, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ ì „ì²´ ë²¤ì¹˜ë§ˆí¬: {full_output_path}")

        # 2) íƒœìŠ¤í¬ë³„ ê°œë³„ ì €ì¥
        for task_name, task_info in benchmark['tasks'].items():
            task_output_path = self.output_dir / f'kls_bench_{task_name}.json'
            with open(task_output_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'task': task_name,
                    'description': task_info['description'],
                    'size': task_info['size'],
                    'metric': task_info['metric'],
                    'data': task_info['data']
                }, f, ensure_ascii=False, indent=2)
            print(f"  âœ“ {task_name}: {task_output_path}")

        # 3) CSV í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (ë¶„ì„ í¸ì˜)
        for task_name, task_info in benchmark['tasks'].items():
            df = pd.DataFrame(task_info['data'])
            csv_output_path = self.output_dir / f'kls_bench_{task_name}.csv'
            df.to_csv(csv_output_path, index=False, encoding='utf-8-sig')
            print(f"  âœ“ {task_name} (CSV): {csv_output_path}")

        # 4) README ìƒì„±
        self.generate_readme(benchmark)

    def generate_readme(self, benchmark: Dict):
        """README ë¬¸ì„œ ìƒì„±"""
        readme_path = self.output_dir / 'README.md'

        readme_content = f"""# KLSBench: Korean Classical Literature Understanding Benchmark

í•œêµ­ ê³ ì „ ë¬¸í—Œ ì´í•´ë¥¼ ìœ„í•œ í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬

## ğŸ“‹ ê°œìš”

**KLSBench**ëŠ” C3Benchë¥¼ ì°¸ê³ í•˜ì—¬ ê°œë°œëœ í•œêµ­ ê³ ì „ ë¬¸í—Œ ì´í•´ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í•œêµ­ ê³ ì „ í•œë¬¸ ë° ì‚¬ì„œ ë°ì´í„°ì— ëŒ€í•œ ì´í•´ ëŠ¥ë ¥ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

- **ë²„ì „**: {benchmark['benchmark_info']['version']}
- **ì´ í•­ëª© ìˆ˜**: {benchmark['benchmark_info']['total_size']:,}ê°œ
- **íƒœìŠ¤í¬ ìˆ˜**: {len(benchmark['benchmark_info']['tasks'])}ê°œ
- **ì§€ì› ì–¸ì–´**: {', '.join(benchmark['benchmark_info']['languages'])}

## ğŸ¯ íƒœìŠ¤í¬ êµ¬ì„±

| íƒœìŠ¤í¬ | ì„¤ëª… | í•­ëª© ìˆ˜ | í‰ê°€ ì§€í‘œ |
|:---|:---|---:|:---|
"""

        for task_name, task_info in benchmark['tasks'].items():
            readme_content += f"| **{task_name}** | {task_info['description']} | {task_info['size']:,} | {task_info['metric']} |\n"

        readme_content += f"""
## ğŸ“Š ë°ì´í„° í†µê³„

### 1. Classification (ë¶„ë¥˜)

ë¬¸ì²´ë³„ ë¶„í¬:
"""

        # Classification í†µê³„
        cls_labels = [item['label'] for item in benchmark['tasks']['classification']['data']]
        cls_counts = Counter(cls_labels)
        for label, count in sorted(cls_counts.items()):
            readme_content += f"- **{label}**: {count:,}ê°œ\n"

        readme_content += """
### 2. Retrieval (ê²€ìƒ‰)

ì±…ë³„ ë¶„í¬:
"""

        # Retrieval í†µê³„
        ret_books = [item['book'] for item in benchmark['tasks']['retrieval']['data']]
        ret_counts = Counter(ret_books)
        for book, count in sorted(ret_counts.items(), key=lambda x: -x[1]):
            readme_content += f"- **{book}**: {count:,}ê°œ\n"

        readme_content += """
### 3. Punctuation (êµ¬ë‘ì )

í‰ê·  ë¬¸ì¥ ê¸¸ì´ ë° í†µê³„ëŠ” ë°ì´í„° ë¡œë”© í›„ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### 4. NLI (ìì—°ì–¸ì–´ì¶”ë¡ )

ë ˆì´ë¸” ë¶„í¬:
"""

        # NLI í†µê³„
        nli_labels = [item['label'] for item in benchmark['tasks']['nli']['data']]
        nli_counts = Counter(nli_labels)
        for label, count in sorted(nli_counts.items()):
            readme_content += f"- **{label}**: {count:,}ê°œ\n"

        readme_content += """
### 5. Translation (ë²ˆì—­)

ì–¸ì–´ ìŒ ë¶„í¬:
"""

        # Translation í†µê³„
        trans_pairs = [f"{item['source_lang']} â†’ {item['target_lang']}"
                       for item in benchmark['tasks']['translation']['data']]
        trans_counts = Counter(trans_pairs)
        for pair, count in sorted(trans_counts.items(), key=lambda x: -x[1]):
            readme_content += f"- **{pair}**: {count:,}ê°œ\n"

        readme_content += """
## ğŸš€ ì‚¬ìš© ë°©ë²•

### Pythonì—ì„œ ë¡œë“œ

```python
import json

# ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ
with open('kls_bench_full.json', 'r', encoding='utf-8') as f:
    benchmark = json.load(f)

# íŠ¹ì • íƒœìŠ¤í¬ë§Œ ë¡œë“œ
with open('kls_bench_classification.json', 'r', encoding='utf-8') as f:
    classification_task = json.load(f)

# ë°ì´í„° ì ‘ê·¼
for item in classification_task['data']:
    print(f"Input: {item['input']}")
    print(f"Label: {item['label']}")
```

### Pandasë¡œ ë¶„ì„

```python
import pandas as pd

# CSVë¡œ ë¡œë“œ
df = pd.read_csv('kls_bench_classification.csv')
print(df.head())
print(df['label'].value_counts())
```

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
kls_bench/
â”œâ”€â”€ kls_bench_full.json          # ì „ì²´ ë²¤ì¹˜ë§ˆí¬ (ëª¨ë“  íƒœìŠ¤í¬ í¬í•¨)
â”œâ”€â”€ kls_bench_classification.json # ë¶„ë¥˜ íƒœìŠ¤í¬
â”œâ”€â”€ kls_bench_retrieval.json     # ê²€ìƒ‰ íƒœìŠ¤í¬
â”œâ”€â”€ kls_bench_punctuation.json   # êµ¬ë‘ì  íƒœìŠ¤í¬
â”œâ”€â”€ kls_bench_nli.json           # NLI íƒœìŠ¤í¬
â”œâ”€â”€ kls_bench_translation.json   # ë²ˆì—­ íƒœìŠ¤í¬
â”œâ”€â”€ kls_bench_classification.csv # ë¶„ë¥˜ íƒœìŠ¤í¬ (CSV)
â”œâ”€â”€ kls_bench_retrieval.csv      # ê²€ìƒ‰ íƒœìŠ¤í¬ (CSV)
â”œâ”€â”€ kls_bench_punctuation.csv    # êµ¬ë‘ì  íƒœìŠ¤í¬ (CSV)
â”œâ”€â”€ kls_bench_nli.csv            # NLI íƒœìŠ¤í¬ (CSV)
â”œâ”€â”€ kls_bench_translation.csv    # ë²ˆì—­ íƒœìŠ¤í¬ (CSV)
â””â”€â”€ README.md                          # ë³¸ ë¬¸ì„œ
```

## ğŸ“ ë°ì´í„° ì¶œì²˜

1. **ê³¼ê±°ì‹œí—˜ ë°ì´í„°**: í•œêµ­ ê³¼ê±°ì‹œí—˜ ë¬¸ì œ ë° ë‹µì•ˆ (ë¬¸ì²´ ë¶„ë¥˜ í¬í•¨)
2. **ì‚¬ì„œ(å››æ›¸) ë°ì´í„°**: ë…¼ì–´, ë§¹ì, ëŒ€í•™, ì¤‘ìš© ë“± ìœ êµ ê²½ì „
3. **NLI ì˜ˆì‹œ**: ìì—°ì–¸ì–´ì¶”ë¡  í…œí”Œë¦¿ ë° ì˜ˆì‹œ

## ğŸ“œ ë¼ì´ì„ ìŠ¤ ë° ì¸ìš©

ì´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì—°êµ¬ì— ì‚¬ìš©í•˜ì‹œëŠ” ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ì¸ìš©í•´ ì£¼ì„¸ìš”:

```bibtex
@misc{{kls_bench_2024,
  title={{KLSBench: Korean Classical Literature Understanding Benchmark}},
  author={{Your Name}},
  year={{2024}},
  note={{Inspired by C3Bench}}
}}
```

## ğŸ”— ì°¸ê³  ìë£Œ

- **C3Bench**: [ë…¼ë¬¸ ë§í¬]
- **ê´€ë ¨ ì—°êµ¬**: ê³ ì „ í•œë¬¸ ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬

## ğŸ“§ ë¬¸ì˜

ë²¤ì¹˜ë§ˆí¬ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì€ ì´ë©”ì¼ë¡œ ì—°ë½ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

---

Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)

        print(f"  âœ“ README: {readme_path}")

    def print_statistics(self, benchmark: Dict):
        """ë²¤ì¹˜ë§ˆí¬ í†µê³„ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ í†µê³„")
        print("="*70)

        print(f"\nğŸ¯ ì „ì²´ ìš”ì•½")
        print(f"  - ë²¤ì¹˜ë§ˆí¬ ì´ë¦„: {benchmark['benchmark_info']['name']}")
        print(f"  - ì´ í•­ëª© ìˆ˜: {benchmark['benchmark_info']['total_size']:,}ê°œ")
        print(f"  - íƒœìŠ¤í¬ ìˆ˜: {len(benchmark['benchmark_info']['tasks'])}ê°œ")

        print(f"\nğŸ“‹ íƒœìŠ¤í¬ë³„ í†µê³„:")
        for task_name, task_info in benchmark['tasks'].items():
            print(f"\n  [{task_name.upper()}]")
            print(f"    - í•­ëª© ìˆ˜: {task_info['size']:,}ê°œ")
            print(f"    - í‰ê°€ ì§€í‘œ: {task_info['metric']}")

            # ì¶”ê°€ í†µê³„
            if task_name == 'classification':
                labels = [item['label'] for item in task_info['data']]
                label_counts = Counter(labels)
                print(f"    - ë ˆì´ë¸” ë¶„í¬: {dict(label_counts)}")

            elif task_name == 'nli':
                labels = [item['label'] for item in task_info['data']]
                label_counts = Counter(labels)
                print(f"    - ë ˆì´ë¸” ë¶„í¬: {dict(label_counts)}")

            elif task_name == 'translation':
                pairs = [f"{item['source_lang']}â†’{item['target_lang']}"
                        for item in task_info['data']]
                pair_counts = Counter(pairs)
                print(f"    - ì–¸ì–´ ìŒ ë¶„í¬:")
                for pair, count in pair_counts.most_common():
                    print(f"      â€¢ {pair}: {count:,}ê°œ")

        print("\n" + "="*70)
        print("âœ… ë²¤ì¹˜ë§ˆí¬ ìƒì„± ì™„ë£Œ!")
        print("="*70)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    translated_csv_path = "/Users/songhune/Workspace/korean_eda/notebook/experiments/graphs/translated_full_20251020_212605.csv"
    external_csv_path = "/Users/songhune/Workspace/korean_eda/data/External_raw.csv"
    nli_examples_path = "/Users/songhune/Workspace/korean_eda/examples/nli_examples.json"
    output_dir = "/Users/songhune/Workspace/korean_eda/benchmark/kls_bench"

    # ë²¤ì¹˜ë§ˆí¬ ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = KLSBenchGenerator(
        translated_csv_path=translated_csv_path,
        external_csv_path=external_csv_path,
        nli_examples_path=nli_examples_path,
        output_dir=output_dir
    )

    # ë²¤ì¹˜ë§ˆí¬ ìƒì„±
    benchmark = generator.generate_benchmark()

    print(f"\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")


if __name__ == "__main__":
    main()
