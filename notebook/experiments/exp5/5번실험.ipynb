{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì‹¤í—˜ 5: K-ClassicBench ë²¤ì¹˜ë§ˆí¬ í‰ê°€\n",
    "\n",
    "**ëª©í‘œ**: C3Benchë¥¼ ì°¸ê³ í•˜ì—¬ ê°œë°œí•œ K-ClassicBenchë¡œ ë‹¤ì–‘í•œ LLM í‰ê°€\n",
    "\n",
    "## í‰ê°€ ëª¨ë¸ ì¢…ë¥˜\n",
    "\n",
    "### 1. ë¹„ê³µê°œ API ëª¨ë¸\n",
    "- GPT-4 Turbo\n",
    "- GPT-3.5 Turbo\n",
    "- Claude 3.5 Sonnet\n",
    "- Claude 3 Opus\n",
    "\n",
    "### 2. ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸\n",
    "- Llama 3.1 (8B, 70B)\n",
    "- Qwen 2.5 (7B, 14B, 72B)\n",
    "- EXAONE 3.0 (7.8B)\n",
    "\n",
    "### 3. ì§€ë„í•™ìŠµ ëª¨ë¸\n",
    "- GwenBert\n",
    "- Tongu\n",
    "\n",
    "## í‰ê°€ íƒœìŠ¤í¬\n",
    "\n",
    "1. **Classification**: ë¬¸ì²´ ë¶„ë¥˜ (è³¦/è©©/ç–‘/ç¾© ë“±)\n",
    "2. **Retrieval**: ì¶œì²˜ ì‹ë³„ (è«–èª/å­Ÿå­/å¤§å­¸/ä¸­åº¸)\n",
    "3. **Punctuation**: êµ¬ë‘ì  ë³µì› (ë°±ë¬¸ â†’ êµ¬ë‘ì ë³¸)\n",
    "4. **NLI**: ìì—°ì–¸ì–´ì¶”ë¡  (entailment/contradiction/neutral)\n",
    "5. **Translation**: ë²ˆì—­ (í•œë¬¸â†”í•œê¸€â†”ì˜ë¬¸)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "BENCHMARK_PATH = '/Users/songhune/Workspace/korean_eda/benchmark/k_classic_bench/k_classic_bench_full.json'\n",
    "RESULTS_DIR = '/Users/songhune/Workspace/korean_eda/benchmark/results'\n",
    "\n",
    "# ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¤ì¹˜ë§ˆí¬ ë¡œë“œ\n",
    "with open(BENCHMARK_PATH, 'r', encoding='utf-8') as f:\n",
    "    benchmark = json.load(f)\n",
    "\n",
    "print(f\"ğŸ“Š {benchmark['benchmark_info']['name']}\")\n",
    "print(f\"ë²„ì „: {benchmark['benchmark_info']['version']}\")\n",
    "print(f\"ì´ í•­ëª© ìˆ˜: {benchmark['benchmark_info']['total_size']:,}ê°œ\")\n",
    "print(f\"\\níƒœìŠ¤í¬:\")\n",
    "\n",
    "for task_name, task_data in benchmark['tasks'].items():\n",
    "    print(f\"  - {task_name}: {task_data['size']:,}ê°œ ({task_data['metric']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° íƒœìŠ¤í¬ ì˜ˆì‹œ í™•ì¸\n",
    "for task_name, task_data in benchmark['tasks'].items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{task_name.upper()}] ì˜ˆì‹œ\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    example = task_data['data'][0]\n",
    "    \n",
    "    if task_name == 'classification':\n",
    "        print(f\"ì…ë ¥: {example['input']}\")\n",
    "        print(f\"ë ˆì´ë¸”: {example['label']}\")\n",
    "    \n",
    "    elif task_name == 'retrieval':\n",
    "        print(f\"ì…ë ¥: {example['input'][:100]}...\")\n",
    "        print(f\"ì •ë‹µ: {example['answer']}\")\n",
    "    \n",
    "    elif task_name == 'punctuation':\n",
    "        print(f\"ì…ë ¥ (ë°±ë¬¸): {example['input'][:80]}...\")\n",
    "        print(f\"ì •ë‹µ (êµ¬ë‘ì ): {example['answer'][:80]}...\")\n",
    "    \n",
    "    elif task_name == 'nli':\n",
    "        print(f\"ì „ì œ: {example['premise'][:80]}...\")\n",
    "        print(f\"ê°€ì„¤: {example['hypothesis'][:80]}...\")\n",
    "        print(f\"ë ˆì´ë¸”: {example['label']}\")\n",
    "    \n",
    "    elif task_name == 'translation':\n",
    "        print(f\"ì›ë¬¸ ({example['source_lang']}): {example['source_text'][:80]}...\")\n",
    "        print(f\"ë²ˆì—­ ({example['target_lang']}): {example['target_text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í‰ê°€ ì‹¤í–‰\n",
    "\n",
    "### 3.1 API ëª¨ë¸ í‰ê°€ (GPT-4, Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')  # ë˜ëŠ” ì§ì ‘ ì…ë ¥\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')  # ë˜ëŠ” ì§ì ‘ ì…ë ¥\n",
    "\n",
    "# API í‚¤ í™•ì¸\n",
    "print(f\"OpenAI API Key: {'ì„¤ì •ë¨' if OPENAI_API_KEY else 'ë¯¸ì„¤ì •'}\")\n",
    "print(f\"Anthropic API Key: {'ì„¤ì •ë¨' if ANTHROPIC_API_KEY else 'ë¯¸ì„¤ì •'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 í‰ê°€ (ìƒ˜í”Œ í…ŒìŠ¤íŠ¸: ê° íƒœìŠ¤í¬ë‹¹ 10ê°œ)\n",
    "!python exp5_benchmark_evaluation.py \\\n",
    "    --model-type api \\\n",
    "    --model-name gpt-4-turbo \\\n",
    "    --max-samples 10 \\\n",
    "    --api-key $OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet í‰ê°€ (ìƒ˜í”Œ í…ŒìŠ¤íŠ¸)\n",
    "!python exp5_benchmark_evaluation.py \\\n",
    "    --model-type api \\\n",
    "    --model-name claude-3-5-sonnet-20241022 \\\n",
    "    --max-samples 10 \\\n",
    "    --api-key $ANTHROPIC_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.1 8B í‰ê°€ (ìƒ˜í”Œ í…ŒìŠ¤íŠ¸)\n",
    "# ì£¼ì˜: GPU ë©”ëª¨ë¦¬ í•„ìš”\n",
    "!python exp5_benchmark_evaluation.py \\\n",
    "    --model-type opensource \\\n",
    "    --model-name meta-llama/Llama-3.1-8B-Instruct \\\n",
    "    --max-samples 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen 2.5 7B í‰ê°€ (ìƒ˜í”Œ í…ŒìŠ¤íŠ¸)\n",
    "!python exp5_benchmark_evaluation.py \\\n",
    "    --model-type opensource \\\n",
    "    --model-name Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --max-samples 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ì§€ë„í•™ìŠµ ëª¨ë¸ í‰ê°€ (GwenBert, Tongu)\n",
    "\n",
    "**ì£¼ì˜**: ì´ ë¶€ë¶„ì€ ëª¨ë¸ë³„ êµ¬í˜„ì´ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tongu ëª¨ë¸ í‰ê°€\n",
    "# TODO: Tongu ëª¨ë¸ ê²½ë¡œ ì§€ì • ë° ë˜í¼ êµ¬í˜„ í•„ìš”\n",
    "# !python exp5_benchmark_evaluation.py \\\n",
    "#     --model-type supervised \\\n",
    "#     --model-name tongu \\\n",
    "#     --max-samples 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ íŒŒì¼ ë¡œë“œ\n",
    "result_files = list(Path(RESULTS_DIR).glob('summary_*.csv'))\n",
    "\n",
    "print(f\"ğŸ“ ë°œê²¬ëœ ê²°ê³¼ íŒŒì¼: {len(result_files)}ê°œ\")\n",
    "for f in result_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  ê²°ê³¼ ë³‘í•©\n",
    "all_results = []\n",
    "\n",
    "for file in result_files:\n",
    "    df = pd.read_csv(file)\n",
    "    all_results.append(df)\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"âœ… ì´ {len(results_df)} ê°œì˜ ê²°ê³¼ ë¡œë“œë¨\")\n",
    "    display(results_df.head())\n",
    "else:\n",
    "    print(\"âš ï¸  ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 íƒœìŠ¤í¬ë³„ ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # íƒœìŠ¤í¬ë³„ ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ\n",
    "    metric_map = {\n",
    "        'classification': 'accuracy',\n",
    "        'retrieval': 'accuracy',\n",
    "        'punctuation': 'rougeL_f1',\n",
    "        'nli': 'accuracy',\n",
    "        'translation': 'bleu'\n",
    "    }\n",
    "    \n",
    "    # í”¼ë²— í…Œì´ë¸” ìƒì„±\n",
    "    pivot_data = []\n",
    "    \n",
    "    for task, metric in metric_map.items():\n",
    "        task_data = results_df[results_df['task'] == task]\n",
    "        for _, row in task_data.iterrows():\n",
    "            pivot_data.append({\n",
    "                'model': row['model'],\n",
    "                'task': task,\n",
    "                'score': row.get(metric, 0)\n",
    "            })\n",
    "    \n",
    "    pivot_df = pd.DataFrame(pivot_data)\n",
    "    pivot_table = pivot_df.pivot(index='model', columns='task', values='score')\n",
    "    \n",
    "    print(\"\\nğŸ“Š íƒœìŠ¤í¬ë³„ ì„±ëŠ¥ (ì£¼ìš” ë©”íŠ¸ë¦­)\")\n",
    "    display(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # íƒœìŠ¤í¬ë³„ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('K-ClassicBench: íƒœìŠ¤í¬ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    tasks = list(metric_map.keys())\n",
    "    \n",
    "    for idx, task in enumerate(tasks):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        task_df = pivot_df[pivot_df['task'] == task]\n",
    "        \n",
    "        if not task_df.empty:\n",
    "            task_df = task_df.sort_values('score', ascending=True)\n",
    "            \n",
    "            ax.barh(task_df['model'], task_df['score'], color='skyblue')\n",
    "            ax.set_xlabel('Score')\n",
    "            ax.set_title(f'{task.upper()} ({metric_map[task]})')\n",
    "            ax.set_xlim(0, 1.0)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # ë¹ˆ subplot ì œê±°\n",
    "    if len(tasks) < 6:\n",
    "        for idx in range(len(tasks), 6):\n",
    "            fig.delaxes(axes[idx // 3, idx % 3])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # íˆíŠ¸ë§µ\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Score'})\n",
    "    plt.title('ëª¨ë¸ë³„ íƒœìŠ¤í¬ ì„±ëŠ¥ íˆíŠ¸ë§µ', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('íƒœìŠ¤í¬')\n",
    "    plt.ylabel('ëª¨ë¸')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ëª¨ë¸ë³„ ì¢…í•© ì ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # í‰ê·  ì ìˆ˜ ê³„ì‚°\n",
    "    model_avg = pivot_table.mean(axis=1).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nğŸ† ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥ (ì „ì²´ íƒœìŠ¤í¬)\")\n",
    "    print(\"=\"*50)\n",
    "    for idx, (model, score) in enumerate(model_avg.items(), 1):\n",
    "        print(f\"{idx}. {model}: {score:.4f}\")\n",
    "    \n",
    "    # ê·¸ë˜í”„\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model_avg.plot(kind='barh', color='coral')\n",
    "    plt.xlabel('í‰ê·  ì ìˆ˜')\n",
    "    plt.title('ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/overall_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ìƒì„¸ ë¶„ì„\n",
    "\n",
    "### 5.1 Classification: ë¬¸ì²´ë³„ ì •í™•ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification ìƒì„¸ ê²°ê³¼ ë¡œë“œ\n",
    "# TODO: ê° ëª¨ë¸ì˜ JSON ê²°ê³¼ íŒŒì¼ì—ì„œ ì˜ˆì¸¡ê°’ ì¶”ì¶œí•˜ì—¬ ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ì˜¤ë¥˜ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜¤ë‹µ ì¼€ì´ìŠ¤ ë¶„ì„\n",
    "# TODO: JSON ê²°ê³¼ì—ì„œ ì˜ˆì¸¡ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¶”ì¶œ ë° ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ê²°ë¡  ë° ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "### ì£¼ìš” ë°œê²¬ì‚¬í•­\n",
    "\n",
    "1. **ëª¨ë¸ë³„ ì„±ëŠ¥**\n",
    "   - ë¹„ê³µê°œ API ëª¨ë¸ (GPT-4, Claude)ì˜ ì „ë°˜ì ì¸ ìš°ìˆ˜ì„±\n",
    "   - ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì˜ íƒœìŠ¤í¬ë³„ í¸ì°¨\n",
    "   - ì§€ë„í•™ìŠµ ëª¨ë¸ì˜ íŠ¹ì • íƒœìŠ¤í¬ ê°•ì \n",
    "\n",
    "2. **íƒœìŠ¤í¬ë³„ ë‚œì´ë„**\n",
    "   - ê°€ì¥ ì‰¬ìš´ íƒœìŠ¤í¬: [ë¶„ì„ í•„ìš”]\n",
    "   - ê°€ì¥ ì–´ë ¤ìš´ íƒœìŠ¤í¬: [ë¶„ì„ í•„ìš”]\n",
    "\n",
    "3. **ê°œì„  ë°©í–¥**\n",
    "   - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
    "   - Few-shot learning ì ìš©\n",
    "   - Fine-tuning ê°€ëŠ¥ì„±\n",
    "\n",
    "### C3Benchì™€ì˜ ë¹„êµ\n",
    "\n",
    "- [ë¶„ì„ í•„ìš”]\n",
    "\n",
    "### í–¥í›„ ì—°êµ¬ ë°©í–¥\n",
    "\n",
    "1. ë” ë§ì€ ëª¨ë¸ í‰ê°€\n",
    "2. Few-shot ì„±ëŠ¥ ë¹„êµ\n",
    "3. ë„ë©”ì¸ íŠ¹í™” Fine-tuning\n",
    "4. ë²¤ì¹˜ë§ˆí¬ í™•ì¥ (ë” ë§ì€ íƒœìŠ¤í¬, ë°ì´í„°)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë¶€ë¡: ì „ì²´ í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸\n",
    "\n",
    "### ë°°ì¹˜ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
    "api_models = [\n",
    "    ('gpt-4-turbo', 'openai'),\n",
    "    ('gpt-3.5-turbo', 'openai'),\n",
    "    ('claude-3-5-sonnet-20241022', 'anthropic'),\n",
    "]\n",
    "\n",
    "opensource_models = [\n",
    "    'meta-llama/Llama-3.1-8B-Instruct',\n",
    "    'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct',\n",
    "]\n",
    "\n",
    "# API ëª¨ë¸ í‰ê°€\n",
    "for model_name, provider in api_models:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"í‰ê°€ ì¤‘: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    api_key = OPENAI_API_KEY if provider == 'openai' else ANTHROPIC_API_KEY\n",
    "    \n",
    "    !python exp5_benchmark_evaluation.py \\\n",
    "        --model-type api \\\n",
    "        --model-name {model_name} \\\n",
    "        --api-key {api_key}\n",
    "\n",
    "# ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ í‰ê°€\n",
    "for model_name in opensource_models:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"í‰ê°€ ì¤‘: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    !python exp5_benchmark_evaluation.py \\\n",
    "        --model-type opensource \\\n",
    "        --model-name {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
