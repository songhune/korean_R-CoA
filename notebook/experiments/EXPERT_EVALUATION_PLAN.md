# KLSBench Expert Evaluation Plan

A comprehensive plan for obtaining validation from classical Chinese literature experts (í•œë¬¸í•™ ì „ê³µì).

## Table of Contents
1. [Objectives](#objectives)
2. [Target Experts](#target-experts)
3. [Evaluation Phases](#evaluation-phases)
4. [Materials to Prepare](#materials-to-prepare)
5. [Evaluation Methodology](#evaluation-methodology)
6. [Timeline](#timeline)
7. [Success Metrics](#success-metrics)

---

## Objectives

### Primary Goals
1. **Validate benchmark quality**: Confirm that tasks and data accurately reflect classical Chinese literature understanding
2. **Verify label correctness**: Validate classification labels (19 classes) and task annotations
3. **Assess task difficulty**: Determine if tasks appropriately measure expertise levels
4. **Identify improvements**: Gather feedback for benchmark refinement

### Secondary Goals
1. Establish credibility for academic publication
2. Build network with domain experts
3. Gather expert-level baseline performance data
4. Identify culturally/historically important edge cases

---

## Target Experts

### Profile Requirements

**Primary Target** (5-10 experts):
- PhD in Classical Chinese Literature, Korean Classical Literature, or East Asian Studies
- Active researchers or professors
- Specialization in:
  - Joseon Dynasty literature (ì¡°ì„ ì‹œëŒ€ ë¬¸í•™)
  - Four Books (å››æ›¸: è«–èª, å­Ÿå­, å¤§å­¸, ä¸­åº¸)
  - Gwageo examination system (ê³¼ê±°ì‹œí—˜)
  - Classical Chinese poetry and prose (í•œì‹œ, ì‚°ë¬¸)

**Secondary Target** (10-20 experts):
- Graduate students (MA/PhD) in relevant fields
- Teachers of classical Chinese (í•œë¬¸ êµì‚¬)
- Researchers at institutes (í•œêµ­í•™ì¤‘ì•™ì—°êµ¬ì›, í•œêµ­ê³ ì „ë²ˆì—­ì›)

### Recruitment Strategy

**Academic Institutions**:
- ì„œìš¸ëŒ€í•™êµ í•œë¬¸í•™ê³¼, ì¤‘ì–´ì¤‘ë¬¸í•™ê³¼
- ê³ ë ¤ëŒ€í•™êµ, ì—°ì„¸ëŒ€í•™êµ í•œë¬¸í•™ê³¼
- ì„±ê· ê´€ëŒ€í•™êµ í•œë¬¸í•™ê³¼ (ì „í†µì  ê°•ì )
- í•œêµ­í•™ì¤‘ì•™ì—°êµ¬ì›
- í•œêµ­ê³ ì „ë²ˆì—­ì›

**Approach**:
1. Email to department chairs requesting participation
2. Present at relevant conferences/seminars
3. Leverage existing academic networks
4. Offer co-authorship on validation paper for key contributors
5. Provide honorarium for extensive evaluation (50-100k KRW per expert)

---

## Evaluation Phases

### Phase 1: Pilot Evaluation (2-3 experts, 2 weeks)

**Objectives**:
- Test evaluation methodology
- Identify major issues early
- Refine evaluation materials

**Tasks**:
- Sample evaluation (50 items per task)
- Interview for qualitative feedback
- Iterative refinement

**Deliverables**:
- Revised evaluation protocol
- Initial feedback report
- Refined benchmark (if needed)

### Phase 2: Full Expert Evaluation (5-10 experts, 1 month)

**Objectives**:
- Comprehensive validation
- Inter-annotator agreement analysis
- Statistical validation

**Tasks**:
- Full task evaluation (stratified sample: 10-30% per task)
- Structured questionnaire
- Difficulty rating
- Error analysis

**Deliverables**:
- Expert validation report
- Inter-annotator agreement scores (Fleiss' kappa)
- Benchmark quality metrics

### Phase 3: Expert Baseline (3-5 experts, 2 weeks)

**Objectives**:
- Establish human expert performance
- Compare LLM vs. human performance
- Identify challenging items

**Tasks**:
- Timed evaluation on test set
- Think-aloud protocol (optional)
- Error pattern analysis

**Deliverables**:
- Human baseline scores
- Human vs. LLM comparison
- Qualitative insights on AI limitations

---

## Materials to Prepare

### 1. Executive Summary (Korean)

**Content** (2-3 pages):
```markdown
# KLSBench: í•œêµ­ ê³ ì „ ë¬¸í—Œ ì´í•´ë„ ë²¤ì¹˜ë§ˆí¬

## ê°œìš”
- ì—°êµ¬ ë°°ê²½ ë° ëª©ì 
- ë²¤ì¹˜ë§ˆí¬ êµ¬ì„± (5ê°œ íƒœìŠ¤í¬, 7,871 í•­ëª©)
- ë°ì´í„° ì¶œì²˜ (ê³¼ê±°ì‹œí—˜, ì‚¬ì„œ)
- AI ëª¨ë¸ í‰ê°€ ê²°ê³¼ ìš”ì•½

## í‰ê°€ ì˜ë¢° ì‚¬í•­
- ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- ë¶„ë¥˜ ì²´ê³„ íƒ€ë‹¹ì„± í‰ê°€
- ë‚œì´ë„ ì ì ˆì„± í‰ê°€
- ê°œì„  ì œì•ˆ
```

### 2. Detailed Task Description (Korean)

**For each task**, provide:

**Classification (ë¬¸ì²´ ë¶„ë¥˜)**:
```markdown
### íƒœìŠ¤í¬ ì„¤ëª…
ì£¼ì–´ì§„ í•œë¬¸ í…ìŠ¤íŠ¸ì˜ ë¬¸ì²´ë¥¼ 19ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜

### ë¶„ë¥˜ ì²´ê³„ (19ê°œ ë¼ë²¨)
- ê· í˜• í´ë˜ìŠ¤ (95ê°œ): è³¦, è©©, ç–‘, ç¾©, ç­–, è¡¨
- ê¸°íƒ€ í´ë˜ìŠ¤: è«–(53), éŠ˜(53), ç®‹(49), é Œ(24), ç¦®ç¾©(13), ç®´(12), æ˜“ç¾©(9), è©©ç¾©(7), æ›¸ç¾©(6), è©”(5), åˆ¶(3), è¬›(2), æ“¬(2)

### í‰ê°€ ìš”ì²­ ì‚¬í•­
1. ë¼ë²¨ ì²´ê³„ê°€ ì ì ˆí•œê°€?
2. ê° ìƒ˜í”Œì˜ ë¼ë²¨ì´ ì •í™•í•œê°€?
3. í˜¼ë™í•˜ê¸° ì‰¬ìš´ ì¹´í…Œê³ ë¦¬ëŠ”?
4. ì œì•ˆ ì‚¬í•­
```

**Retrieval (ì¶œì²˜ ì‹ë³„)**:
```markdown
### íƒœìŠ¤í¬ ì„¤ëª…
ì‚¬ì„œ(è«–èª, å­Ÿå­, å¤§å­¸, ä¸­åº¸) ë¬¸ì¥ì˜ ì¶œì²˜ ì‹ë³„

### í‰ê°€ ìš”ì²­ ì‚¬í•­
1. ì¶œì²˜ ì •ë³´ê°€ ì •í™•í•œê°€?
2. ë‚œì´ë„ê°€ ì ì ˆí•œê°€?
3. ì˜¤ë¥˜ê°€ ìˆëŠ” í•­ëª©ì€?
```

**Similar for other tasks...**

### 3. Sample Data for Review

**Stratified Sample**:
```python
# Per task sampling strategy
classification: 80 items (10% of 808)
  - Balanced classes: 10 each (60 items)
  - Other classes: 2-5 each (20 items)

retrieval: 120 items (10% of 1,209)
  - By source book: 30 each (è«–èª, å­Ÿå­, å¤§å­¸, ä¸­åº¸)

punctuation: 200 items (10% of 2,000)
  - By difficulty: Easy(50), Medium(100), Hard(50)

nli: 180 items (10% of 1,854)
  - By label: 60 each (entailment, contradiction, neutral)

translation: 200 items (10% of 2,000)
  - By direction: Classical Chineseâ†’Korean(100), Koreanâ†’English(50), etc.
```

### 4. Evaluation Interface

**Option A: Web-based Interface** (Recommended)
```
Features:
- Clean, intuitive UI
- Task-by-task evaluation
- Save/resume capability
- Real-time validation
- Export results

Technology:
- Streamlit or Gradio (Python)
- Simple deployment (Hugging Face Spaces)
```

**Option B: Excel/Google Sheets**
```
Pros:
- Familiar interface
- Easy to distribute
- Offline capability

Cons:
- Manual data processing
- Less user-friendly
```

**Option C: PDF + Response Form**
```
Use for:
- Small pilot group
- Quick feedback
```

### 5. Evaluation Questionnaire

**Per Task**:
```markdown
## 1. ë°ì´í„° í’ˆì§ˆ (5-point Likert scale)
- ë°ì´í„°ì˜ ì •í™•ì„±: 1(ë§¤ìš° ë¶€ì •í™•) - 5(ë§¤ìš° ì •í™•)
- ë¼ë²¨ì˜ ì¼ê´€ì„±: 1(ì¼ê´€ì„± ì—†ìŒ) - 5(ë§¤ìš° ì¼ê´€ì )
- ë°ì´í„°ì˜ ëŒ€í‘œì„±: 1(ë¹„ëŒ€í‘œì ) - 5(ë§¤ìš° ëŒ€í‘œì )

## 2. ë‚œì´ë„ í‰ê°€
- ì „ì²´ ë‚œì´ë„: 1(ë§¤ìš° ì‰¬ì›€) - 5(ë§¤ìš° ì–´ë ¤ì›€)
- ì „ê³µìì—ê²Œ ì ì ˆí•œê°€?: ì˜ˆ/ì•„ë‹ˆì˜¤
- ì¼ë°˜ì¸ì—ê²ŒëŠ”?: 1(ë¶ˆê°€ëŠ¥) - 5(ê°€ëŠ¥)

## 3. ì˜¤ë¥˜ ì§€ì 
í•­ëª© ID, ì˜¤ë¥˜ ë‚´ìš©, ì œì•ˆ ì‚¬í•­

## 4. ë¶„ë¥˜ ì²´ê³„ (Classificationë§Œ í•´ë‹¹)
- 19ê°œ ì¹´í…Œê³ ë¦¬ê°€ ì ì ˆí•œê°€?: ì˜ˆ/ì•„ë‹ˆì˜¤
- ì¶”ê°€/ì œê±°/ë³‘í•©í•  ì¹´í…Œê³ ë¦¬: (ììœ  ê¸°ìˆ )

## 5. ì¢…í•© ì˜ê²¬
- ê°•ì 
- ì•½ì 
- ê°œì„  ì œì•ˆ
- ì¶”ê°€ ì œì•ˆ
```

### 6. IRB/Ethics Approval

**If collecting personal data**:
- Consent form
- Data protection plan
- IRB approval (if affiliated with institution)

---

## Evaluation Methodology

### Quantitative Metrics

**1. Inter-Annotator Agreement**
```python
# Fleiss' kappa for multiple annotators
# Target: Îº > 0.75 (substantial agreement)

from statsmodels.stats.inter_rater import fleiss_kappa

# For each task
kappa_classification = fleiss_kappa(ratings_matrix)
kappa_nli = fleiss_kappa(ratings_matrix)
# ...
```

**2. Label Accuracy Validation**
```python
# Expert consensus vs. current labels
# Threshold: 95% agreement

accuracy = correct_labels / total_labels
if accuracy < 0.95:
    # Identify problematic items
    # Revise benchmark
```

**3. Difficulty Rating**
```python
# Average difficulty score (1-5 scale)
# Expected: 3.0-4.0 (moderate to challenging)

mean_difficulty = np.mean(difficulty_ratings)
std_difficulty = np.std(difficulty_ratings)
```

### Qualitative Analysis

**1. Thematic Coding**
```
Open-ended responses â†’ Code categories:
- Data quality issues
- Label disagreements
- Task design suggestions
- Domain-specific insights
```

**2. Error Pattern Analysis**
```
Common error types:
- Mislabeling patterns
- Ambiguous cases
- Historical context issues
- Translation inconsistencies
```

**3. Expert Interviews**
```
Semi-structured interviews (30-60 min):
- Overall impressions
- Specific concerns
- Suggestions for improvement
- Potential use cases
```

---

## Timeline

### Month 1: Preparation
- Week 1-2: Prepare materials (executive summary, samples, interface)
- Week 3: Recruit pilot experts (2-3)
- Week 4: Pilot evaluation

### Month 2: Pilot & Revision
- Week 1-2: Analyze pilot results
- Week 2-3: Revise benchmark based on feedback
- Week 4: Recruit main evaluation experts (5-10)

### Month 3: Main Evaluation
- Week 1-3: Expert evaluation period
- Week 4: Data collection and initial analysis

### Month 4: Analysis & Reporting
- Week 1-2: Statistical analysis
- Week 2-3: Qualitative analysis
- Week 4: Write validation report

**Total: 4 months**

---

## Success Metrics

### Tier 1: Essential (Must Achieve)
âœ… **Inter-annotator agreement**: Îº > 0.70 (substantial)
âœ… **Label accuracy**: >90% expert consensus
âœ… **Expert participation**: 5+ PhD-level experts
âœ… **Sample coverage**: >10% per task evaluated

### Tier 2: Desired (Should Achieve)
â­ **Inter-annotator agreement**: Îº > 0.80 (almost perfect)
â­ **Label accuracy**: >95% expert consensus
â­ **Expert participation**: 8+ experts
â­ **Qualitative feedback**: Rich insights for improvement

### Tier 3: Aspirational (Nice to Have)
ğŸ¯ **Inter-annotator agreement**: Îº > 0.85
ğŸ¯ **Expert participation**: 10+ experts
ğŸ¯ **Publication**: Co-authored validation paper
ğŸ¯ **Baseline**: Human expert performance data

---

## Budget Estimate

### Honorarium (50-100k KRW per expert)
- Pilot (3 experts Ã— 50k): 150,000 KRW
- Main evaluation (8 experts Ã— 100k): 800,000 KRW
- Baseline (3 experts Ã— 150k): 450,000 KRW
- **Subtotal**: 1,400,000 KRW (~$1,000 USD)

### Interface Development
- Web interface (Streamlit/Gradio): 500,000 KRW (or free if DIY)
- **Subtotal**: 500,000 KRW

### Miscellaneous
- Transcription/translation: 300,000 KRW
- Meeting expenses: 200,000 KRW
- **Subtotal**: 500,000 KRW

### **Total: ~2,400,000 KRW (~$1,800 USD)**

---

## Risk Mitigation

### Risk 1: Low Expert Participation
**Mitigation**:
- Start recruitment early
- Offer appropriate compensation
- Leverage institutional connections
- Present at conferences

### Risk 2: Major Quality Issues Found
**Mitigation**:
- Pilot evaluation catches issues early
- Iterative refinement process
- Budget time for revisions

### Risk 3: Low Inter-Annotator Agreement
**Mitigation**:
- Clear annotation guidelines
- Training session before evaluation
- Allow discussion and consensus building
- Refine ambiguous items

### Risk 4: Timeline Delays
**Mitigation**:
- Buffer time in schedule (4 months total)
- Rolling recruitment
- Flexible evaluation deadlines

---

## Deliverables

### Academic Deliverables
1. **Expert Validation Report** (Korean + English)
   - Methodology
   - Quantitative results (IAA, accuracy)
   - Qualitative findings
   - Recommendations

2. **Revised Benchmark** (if needed)
   - Corrected labels
   - Removed/modified problematic items
   - Enhanced documentation

3. **Technical Paper** (for publication)
   - "KLSBench: A Validated Benchmark for Classical Chinese Understanding"
   - Include expert validation as key contribution
   - Target venues: ACL, EMNLP, LREC, or domain-specific journals

### Community Deliverables
1. **Public Dataset** with validation metadata
2. **Expert Evaluation Tool** (open-source)
3. **Best Practices Guide** for benchmark validation

---

## Next Steps (Immediate Actions)

### Week 1-2: Material Preparation
- [ ] Write executive summary (Korean)
- [ ] Prepare detailed task descriptions
- [ ] Create sample dataset (stratified)
- [ ] Draft evaluation questionnaire
- [ ] Design evaluation interface (prototype)

### Week 3: Expert Outreach
- [ ] Identify target experts (list 20+)
- [ ] Draft recruitment email
- [ ] Prepare presentation slides
- [ ] Contact department chairs

### Week 4: Pilot Setup
- [ ] Finalize pilot materials
- [ ] Recruit 2-3 pilot experts
- [ ] Schedule pilot evaluation
- [ ] Prepare consent forms

---

## Contact & Follow-up

### For Experts
**Email template** (Korean):
```
ì œëª©: [í˜‘ì¡° ìš”ì²­] í•œêµ­ ê³ ì „ ë¬¸í—Œ AI ë²¤ì¹˜ë§ˆí¬ ê²€ì¦ ì°¸ì—¬

ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ,

ì €í¬ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ í•œêµ­ ê³ ì „ ë¬¸í—Œ ì´í•´ ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ
KLSBench ë²¤ì¹˜ë§ˆí¬ë¥¼ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤.

ë³¸ ì—°êµ¬ì˜ í•™ìˆ ì  íƒ€ë‹¹ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•´ í•œë¬¸í•™ ì „ê³µ êµìˆ˜ë‹˜ë“¤ì˜
ì „ë¬¸ì  ê²€ì¦ì„ ìš”ì²­ë“œë¦½ë‹ˆë‹¤.

- í‰ê°€ ëŒ€ìƒ: ê³¼ê±°ì‹œí—˜ ë° ì‚¬ì„œ(å››æ›¸) ê¸°ë°˜ 5ê°œ íƒœìŠ¤í¬
- ì†Œìš” ì‹œê°„: 2-3ì‹œê°„ (ì˜¨ë¼ì¸ í‰ê°€)
- ì‚¬ë¡€ê¸ˆ: 10ë§Œì›
- ê¸°ì—¬ ì¸ì •: ë…¼ë¬¸ ê°ì‚¬ì˜ ê¸€ ë˜ëŠ” ê³µë™ì €ì (ê¸°ì—¬ë„ì— ë”°ë¼)

ê´€ì‹¬ ìˆìœ¼ì‹  ê²½ìš°, ìƒì„¸ ìë£Œë¥¼ ë³´ë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ê°ì‚¬í•©ë‹ˆë‹¤.
```

### For Follow-up
- Weekly progress updates
- Thank you notes after evaluation
- Share results with participants
- Acknowledge contributions in publications

---

**Document Version**: 1.0
**Last Updated**: 2025-10-30
**Status**: Planning Phase
