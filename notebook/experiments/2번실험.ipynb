{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750a2e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FONT] 한글/한자 폰트 설정: AppleGothic\n",
      "[BASE] /Users/songhune/Workspace/korean_eda\n",
      "[IN_JSONL] /Users/songhune/Workspace/korean_eda/notebook/eda_outputs/1번실험/links_fix/triples_no_answer.jsonl\n",
      "[OUT_DIR] /Users/songhune/Workspace/korean_eda/notebook/experiments/graphs\n",
      "[LOAD] 3348 rows\n",
      "[GRAPH] nodes=5469 edges=6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/cj4b9h512gs7by1fyqkz5j5c0000gn/T/ipykernel_12436/3327524171.py:171: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Figures saved in /Users/songhune/Workspace/korean_eda/notebook/experiments/graphs\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Entity graph (Exam–Question–Time) + Temporal flow visualization (Korean font fixed)\n",
    "\"\"\"\n",
    "import os, json\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import networkx as nx\n",
    "import platform\n",
    "\n",
    "# ===== 한글/한자 폰트 자동 설정 =====\n",
    "def setup_korean_fonts():\n",
    "    \"\"\"macOS/Windows/Linux에서 한글/한자를 지원하는 폰트 자동 설정\"\"\"\n",
    "    system = platform.system()\n",
    "    \n",
    "    # 시스템별 한글/한자 폰트 우선순위\n",
    "    font_candidates = []\n",
    "    if system == 'Darwin':  # macOS\n",
    "        font_candidates = [\n",
    "            'AppleGothic',           # 한글 (macOS 기본)\n",
    "            'Apple SD Gothic Neo',   # 한글 (macOS)\n",
    "            'Arial Unicode MS',      # 한글+한자\n",
    "            'Nanum Gothic',          # 나눔고딕\n",
    "        ]\n",
    "    elif system == 'Windows':\n",
    "        font_candidates = [\n",
    "            'Malgun Gothic',    # 맑은고딕 (Windows 기본)\n",
    "            'Gulim',           # 굴림\n",
    "            'Batang',          # 바탕\n",
    "            'NanumGothic',     # 나눔고딕\n",
    "        ]\n",
    "    else:  # Linux\n",
    "        font_candidates = [\n",
    "            'NanumGothic',\n",
    "            'Noto Sans CJK KR',\n",
    "            'Noto Sans KR',\n",
    "            'DejaVu Sans',\n",
    "        ]\n",
    "    \n",
    "    # 사용 가능한 폰트 찾기\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            print(f\"[FONT] 한글/한자 폰트 설정: {font}\")\n",
    "            plt.rcParams['font.family'] = font\n",
    "            plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "            return font\n",
    "    \n",
    "    # 폰트를 찾지 못한 경우 경고\n",
    "    print(\"[WARNING] 한글/한자 지원 폰트를 찾지 못했습니다. 텍스트가 깨질 수 있습니다.\")\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    return None\n",
    "\n",
    "# 폰트 설정 적용\n",
    "setup_korean_fonts()\n",
    "\n",
    "# ===== Path setup (상대 경로 사용) =====\n",
    "# 노트북이 있는 디렉토리 기준으로 프로젝트 루트 찾기\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE = NOTEBOOK_DIR.parent.parent  # korean_eda 디렉토리\n",
    "\n",
    "IN_JSONL = BASE / \"notebook\" / \"eda_outputs\" / \"1번실험\" / \"links_fix\" / \"triples_no_answer.jsonl\"\n",
    "OUT_DIR = BASE / \"notebook\" / \"experiments\" / \"graphs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[BASE] {BASE}\")\n",
    "print(f\"[IN_JSONL] {IN_JSONL}\")\n",
    "print(f\"[OUT_DIR] {OUT_DIR}\")\n",
    "\n",
    "def infer_type(nid):\n",
    "    if not nid: return \"Unknown\"\n",
    "    return {\"E\":\"Exam\",\"Q\":\"Question\",\"T\":\"Time\"}.get(nid[0].upper(),\"Unknown\")\n",
    "\n",
    "# ===== Load =====\n",
    "rows=[]\n",
    "with open(IN_JSONL,encoding=\"utf-8\") as f:\n",
    "    for l in f:\n",
    "        if l.strip(): rows.append(json.loads(l))\n",
    "print(f\"[LOAD] {len(rows)} rows\")\n",
    "\n",
    "# ===== Parse nodes/edges =====\n",
    "G = nx.MultiDiGraph()\n",
    "exam_to_time, q_to_exam, time_year = {}, {}, {}\n",
    "q_cat, q_sub = {}, {}\n",
    "exam_stage, exam_typeA, exam_kind = {}, {}, {}\n",
    "\n",
    "for r in rows:\n",
    "    for key in (\"exam\",\"question\",\"time\"):\n",
    "        if key in r:\n",
    "            nid = r[key].get(\"id\")\n",
    "            if nid:\n",
    "                G.add_node(nid, type=infer_type(nid), name=r[key].get(\"name\",\"\"))\n",
    "    for t in r[\"triples\"]:\n",
    "        s,p,o,o_type = t[\"s\"], t[\"p\"], t[\"o\"], t.get(\"o_type\",\"\")\n",
    "        if o_type==\"id\":\n",
    "            G.add_node(s,type=infer_type(s))\n",
    "            G.add_node(o,type=infer_type(o))\n",
    "            G.add_edge(s,o,predicate=p)\n",
    "            if p==\"isHeldOn\": exam_to_time[s]=o\n",
    "            if p==\"isPartOf\": q_to_exam[s]=o\n",
    "        else:\n",
    "            # literals as attributes\n",
    "            if infer_type(s)==\"Time\" and p==\"year\":\n",
    "                try: time_year[s]=int(float(o))\n",
    "                except: pass\n",
    "            if infer_type(s)==\"Question\":\n",
    "                if p==\"hasCategory\": q_cat[s]=o\n",
    "                if p==\"hasSubcategory\": q_sub[s]=o\n",
    "            if infer_type(s)==\"Exam\":\n",
    "                if p==\"hasStage\": exam_stage[s]=o\n",
    "                if p==\"hasTypeA\": exam_typeA[s]=o\n",
    "                if p==\"hasCategory\": exam_kind[s]=o\n",
    "\n",
    "print(f\"[GRAPH] nodes={G.number_of_nodes()} edges={G.number_of_edges()}\")\n",
    "\n",
    "# ===== Predicate counts =====\n",
    "pred_counts = Counter(d.get(\"predicate\",\"\") for _,_,d in G.edges(data=True))\n",
    "pred_df = pd.DataFrame(sorted(pred_counts.items(), key=lambda x:-x[1]), columns=[\"predicate\",\"count\"])\n",
    "pred_df.to_csv(OUT_DIR / \"edge_density.csv\", index=False)\n",
    "\n",
    "# ===== Temporal base table =====\n",
    "records=[]\n",
    "for q,e in q_to_exam.items():\n",
    "    t=exam_to_time.get(e)\n",
    "    y=time_year.get(t)\n",
    "    if y is None: continue\n",
    "    records.append({\n",
    "        \"year\":y,\n",
    "        \"exam_id\":e,\n",
    "        \"question_id\":q,\n",
    "        \"stage\":exam_stage.get(e,\"\"),\n",
    "        \"typeA\":exam_typeA.get(e,\"\"),\n",
    "        \"examKind\":exam_kind.get(e,\"\"),\n",
    "        \"qCategory\":q_cat.get(q,\"\"),\n",
    "        \"qSubcategory\":q_sub.get(q,\"\")\n",
    "    })\n",
    "flow_df=pd.DataFrame(records)\n",
    "flow_df.to_csv(OUT_DIR / \"temporal_flow_base.csv\", index=False)\n",
    "\n",
    "# ===== Figure A: schema subset =====\n",
    "def draw_schema(year_min=1393,year_max=1410):\n",
    "    keep_T={t for t,y in time_year.items() if year_min<=y<=year_max}\n",
    "    keep_E={e for e,t in exam_to_time.items() if t in keep_T}\n",
    "    keep_Q={q for q,e in q_to_exam.items() if e in keep_E}\n",
    "    H=nx.MultiDiGraph()\n",
    "    for n in list(keep_T)+list(keep_E)+list(keep_Q):\n",
    "        H.add_node(n,type=infer_type(n))\n",
    "    for s,t,d in G.edges(data=True):\n",
    "        if s in H and t in H and d[\"predicate\"] in (\"isHeldOn\",\"isPartOf\"):\n",
    "            H.add_edge(s,t,predicate=d[\"predicate\"])\n",
    "    pos={}\n",
    "    def layer(nodes,y):\n",
    "        for i,n in enumerate(sorted(nodes)): pos[n]=(i,y)\n",
    "    Tn=[n for n in H if infer_type(n)==\"Time\"]\n",
    "    En=[n for n in H if infer_type(n)==\"Exam\"]\n",
    "    Qn=[n for n in H if infer_type(n)==\"Question\"]\n",
    "    layer(Tn,2); layer(En,1); layer(Qn,0)\n",
    "    plt.figure(figsize=(14,9))\n",
    "    nx.draw(H,pos,with_labels=False,node_size=400,arrows=True)\n",
    "    labs={n:H.nodes[n].get(\"name\",\"\")[:12] for n in H}\n",
    "    nx.draw_networkx_labels(H,pos,labs,font_size=10)\n",
    "    e_labels={(u,v):d[\"predicate\"] for u,v,d in H.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(H,pos,edge_labels=e_labels,font_size=9)\n",
    "    plt.title(f\"Exam–Question–Time schema (subset {year_min}-{year_max})\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"A_schema_small_1393_1410.pdf\", dpi=200, bbox_inches='tight', format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "draw_schema()\n",
    "\n",
    "# ===== Figure B: edge density =====\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(pred_df[\"predicate\"],pred_df[\"count\"],color=\"steelblue\")\n",
    "plt.xticks(rotation=45,ha='right',fontsize=12)\n",
    "plt.ylabel(\"Edge count\",fontsize=14)\n",
    "plt.title(\"Edge density by predicate\",fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"B_edge_density.pdf\", dpi=200, bbox_inches='tight', format='pdf')\n",
    "plt.close()\n",
    "\n",
    "# ===== Figure C: temporal flow =====\n",
    "annual=flow_df.groupby(\"year\")[\"question_id\"].nunique().reset_index(name=\"n_questions\")\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(annual[\"year\"],annual[\"n_questions\"],color=\"darkslateblue\",lw=1.8)\n",
    "plt.xlabel(\"Year\",fontsize=14); plt.ylabel(\"Number of questions\",fontsize=14)\n",
    "plt.title(\"Number of questions per year\",fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"C1_questions_per_year.pdf\", dpi=200, bbox_inches='tight', format='pdf')\n",
    "plt.close()\n",
    "\n",
    "# Stacked area by stage\n",
    "top_stages=flow_df[\"stage\"].value_counts().head(5).index.tolist()\n",
    "tmp=flow_df.copy()\n",
    "tmp[\"stage_top\"]=np.where(tmp[\"stage\"].isin(top_stages),tmp[\"stage\"],\"Others\")\n",
    "pivot=(tmp.groupby([\"year\",\"stage_top\"])[\"question_id\"]\n",
    "        .nunique().reset_index()\n",
    "        .pivot(index=\"year\",columns=\"stage_top\",values=\"question_id\").fillna(0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.stackplot(pivot.index,*[pivot[c] for c in pivot.columns],labels=pivot.columns)\n",
    "plt.legend(loc=\"upper left\",fontsize=11)\n",
    "plt.xlabel(\"Year\",fontsize=14); plt.ylabel(\"Number of questions\",fontsize=14)\n",
    "plt.title(\"Questions by stage over time\",fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"C2_stage_over_time.pdf\", dpi=200, bbox_inches='tight', format='pdf')\n",
    "plt.close()\n",
    "\n",
    "# Stacked area by category\n",
    "top_cats=flow_df[\"qCategory\"].value_counts().head(5).index.tolist()\n",
    "tmp=flow_df.copy()\n",
    "tmp[\"cat_top\"]=np.where(tmp[\"qCategory\"].isin(top_cats),tmp[\"qCategory\"],\"Others\")\n",
    "pivot=(tmp.groupby([\"year\",\"cat_top\"])[\"question_id\"]\n",
    "        .nunique().reset_index()\n",
    "        .pivot(index=\"year\",columns=\"cat_top\",values=\"question_id\").fillna(0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.stackplot(pivot.index,*[pivot[c] for c in pivot.columns],labels=pivot.columns)\n",
    "plt.legend(loc=\"upper left\",fontsize=11)\n",
    "plt.xlabel(\"Year\",fontsize=14); plt.ylabel(\"Number of questions\",fontsize=14)\n",
    "plt.title(\"Questions by category over time\",fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"C3_category_over_time.pdf\", dpi=200, bbox_inches='tight', format='pdf')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n✅ Figures saved in {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00a4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FONT] 한글/한자 폰트 설정: AppleGothic\n",
      "[FONT] 한글/한자 폰트 설정: AppleGothic\n",
      "[BASE] /Users/songhune/Workspace/korean_eda\n",
      "[IN_JSONL] /Users/songhune/Workspace/korean_eda/notebook/eda_outputs/1번실험/links_fix/triples_no_answer.jsonl\n",
      "[OUT_DIR] /Users/songhune/Workspace/korean_eda/notebook/experiments/graphs\n",
      "✅ Saved:\n",
      "   /Users/songhune/Workspace/korean_eda/notebook/experiments/graphs/A_prime_tripartite_1393_1410.pdf\n",
      "   /Users/songhune/Workspace/korean_eda/notebook/experiments/graphs/B_prime_link_completeness.pdf\n",
      "   /Users/songhune/Workspace/korean_eda/notebook/experiments/graphs/B_double_prime_attr_coverage.pdf\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Useful diagnostics & visualization (Korean font fixed + Improved labels)\n",
    "- A' Tripartite network (with informative styling)\n",
    "- B' Link completeness funnel (Q→E, E→T, Q→E→T)\n",
    "- B'' Attribute coverage heatmap\n",
    "\"\"\"\n",
    "import os, json, math, re\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import platform\n",
    "\n",
    "# ===== 한글/한자 폰트 자동 설정 (개선 버전) =====\n",
    "def setup_korean_fonts():\n",
    "    \"\"\"macOS/Windows/Linux에서 한글/한자를 지원하는 폰트 자동 설정\"\"\"\n",
    "    system = platform.system()\n",
    "    \n",
    "    # 시스템별 한글/한자 폰트 우선순위\n",
    "    font_candidates = []\n",
    "    if system == 'Darwin':  # macOS\n",
    "        font_candidates = [\n",
    "            'AppleGothic',           # 한글 (macOS 기본)\n",
    "            'Apple SD Gothic Neo',   # 한글 (macOS)\n",
    "            'Arial Unicode MS',      # 한글+한자\n",
    "            'Nanum Gothic',          # 나눔고딕\n",
    "        ]\n",
    "    elif system == 'Windows':\n",
    "        font_candidates = [\n",
    "            'Malgun Gothic',    # 맑은고딕 (Windows 기본)\n",
    "            'Gulim',           # 굴림\n",
    "            'Batang',          # 바탕\n",
    "            'NanumGothic',     # 나눔고딕\n",
    "        ]\n",
    "    else:  # Linux\n",
    "        font_candidates = [\n",
    "            'NanumGothic',\n",
    "            'Noto Sans CJK KR',\n",
    "            'Noto Sans KR',\n",
    "            'DejaVu Sans',\n",
    "        ]\n",
    "    \n",
    "    # 사용 가능한 폰트 찾기\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    selected_font = None\n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            selected_font = font\n",
    "            break\n",
    "    \n",
    "    if not selected_font:\n",
    "        print(\"[WARNING] 한글/한자 지원 폰트를 찾지 못했습니다. 텍스트가 깨질 수 있습니다.\")\n",
    "        return None\n",
    "    \n",
    "    # 강제 설정 (matplotlib + pyplot 모두)\n",
    "    matplotlib.rcParams['font.family'] = selected_font\n",
    "    matplotlib.rcParams['font.sans-serif'] = [selected_font]\n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    plt.rcParams['font.family'] = selected_font\n",
    "    plt.rcParams['font.sans-serif'] = [selected_font]\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    plt.rc('font', family=selected_font)\n",
    "    \n",
    "    print(f\"[FONT] 한글/한자 폰트 설정: {selected_font}\")\n",
    "    return selected_font\n",
    "\n",
    "# 폰트 설정 적용\n",
    "setup_korean_fonts()\n",
    "\n",
    "# seaborn 설정 (이게 폰트를 리셋할 수 있음!)\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# seaborn 설정 후 폰트 다시 적용 (중요!)\n",
    "setup_korean_fonts()\n",
    "\n",
    "# ========= Path setup (상대 경로 사용) =========\n",
    "# 노트북이 있는 디렉토리 기준으로 프로젝트 루트 찾기\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE = NOTEBOOK_DIR.parent.parent  # korean_eda 디렉토리\n",
    "\n",
    "IN_JSONL = BASE / \"notebook\" / \"eda_outputs\" / \"1번실험\" / \"links_fix\" / \"triples_no_answer.jsonl\"\n",
    "OUT_DIR = BASE / \"notebook\" / \"experiments\" / \"graphs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[BASE] {BASE}\")\n",
    "print(f\"[IN_JSONL] {IN_JSONL}\")\n",
    "print(f\"[OUT_DIR] {OUT_DIR}\")\n",
    "\n",
    "# ========= Helpers =========\n",
    "def infer_type(nid: str) -> str:\n",
    "    if not nid: return \"Unknown\"\n",
    "    return {\"E\":\"Exam\",\"Q\":\"Question\",\"T\":\"Time\"}.get(nid[0].upper(),\"Unknown\")\n",
    "\n",
    "def safe_int(x):\n",
    "    try: return int(float(x))\n",
    "    except: return None\n",
    "\n",
    "def smart_truncate(text: str, max_len: int = 8, node_type: str = \"Unknown\") -> str:\n",
    "    \"\"\"\n",
    "    스마트 텍스트 축약\n",
    "    - Time: 연도만 표시\n",
    "    - Exam: 연도 + 핵심단어\n",
    "    - Question: 첫 단어 + 말줄임\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Time 노드: 연도만 추출\n",
    "    if node_type == \"Time\":\n",
    "        year_match = re.search(r'(\\d{4})', text)\n",
    "        if year_match:\n",
    "            return year_match.group(1)\n",
    "        return text[:max_len]\n",
    "    \n",
    "    # Exam 노드: 연도 + 첫 단어\n",
    "    if node_type == \"Exam\":\n",
    "        year_match = re.search(r'(\\d{4})', text)\n",
    "        year = year_match.group(1) if year_match else \"\"\n",
    "        # 연도 제거 후 첫 단어 추출\n",
    "        text_without_year = re.sub(r'\\d{4}년?_?', '', text)\n",
    "        first_word = text_without_year.split('_')[0] if '_' in text_without_year else text_without_year\n",
    "        first_word = first_word[:6]\n",
    "        return f\"{year}\\n{first_word}\" if year else first_word\n",
    "    \n",
    "    # Question 노드: 첫 부분만\n",
    "    if node_type == \"Question\":\n",
    "        # 연도 제거\n",
    "        text_clean = re.sub(r'\\d{4}년?_?', '', text)\n",
    "        parts = text_clean.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[0][:4]}\\n{parts[1][:4]}\"\n",
    "        return text_clean[:max_len] + \"…\"\n",
    "    \n",
    "    # 기본: 단순 자르기\n",
    "    return text[:max_len] + (\"…\" if len(text) > max_len else \"\")\n",
    "\n",
    "# ========= Load =========\n",
    "rows = []\n",
    "with open(IN_JSONL, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            rows.append(json.loads(line))\n",
    "\n",
    "# ========= Parse triples into structures =========\n",
    "# Nodes\n",
    "node_name = {}\n",
    "node_type = {}\n",
    "\n",
    "# Edges (only id→id)\n",
    "edges = []  # (s, p, o)\n",
    "\n",
    "# Literal attributes per node\n",
    "lit = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for r in rows:\n",
    "    for key in (\"exam\", \"question\", \"time\"):\n",
    "        if key in r and isinstance(r[key], dict):\n",
    "            nid = r[key].get(\"id\")\n",
    "            if nid:\n",
    "                node_type[nid] = infer_type(nid)\n",
    "                # prefer non-empty name\n",
    "                nm = r[key].get(\"name\") or \"\"\n",
    "                if nm:\n",
    "                    node_name[nid] = nm\n",
    "    for t in r.get(\"triples\", []):\n",
    "        s, p, o = t.get(\"s\"), t.get(\"p\"), t.get(\"o\")\n",
    "        o_type = t.get(\"o_type\", \"\")\n",
    "        if not s or not p or o is None: \n",
    "            continue\n",
    "        if o_type == \"id\":\n",
    "            edges.append((s, p, o))\n",
    "            # register node types if missing\n",
    "            if s not in node_type: node_type[s] = infer_type(s)\n",
    "            if o not in node_type: node_type[o] = infer_type(o)\n",
    "        else:\n",
    "            lit[s][p].append(o)\n",
    "\n",
    "# Quick dictionaries for key relations\n",
    "exam_to_time = {}      # E -> T\n",
    "question_to_exam = {}  # Q -> E\n",
    "for s, p, o in edges:\n",
    "    if p == \"isHeldOn\" and node_type.get(s) == \"Exam\" and node_type.get(o) == \"Time\":\n",
    "        exam_to_time[s] = o\n",
    "    if p == \"isPartOf\" and node_type.get(s) == \"Question\" and node_type.get(o) == \"Exam\":\n",
    "        question_to_exam[s] = o\n",
    "\n",
    "# Useful literal maps\n",
    "time_year = {t: safe_int(vals[0]) for t,vals in ((tid, lit[tid].get(\"year\", [\"\"])) for tid,typ in node_type.items() if typ==\"Time\") if vals}\n",
    "q_cat     = {q: vals[0] for q,vals in ((q, lit[q].get(\"hasCategory\", [\"\"])) for q,typ in node_type.items() if typ==\"Question\") if vals and vals[0]}\n",
    "exam_stage= {e: vals[0] for e,vals in ((eid, lit[eid].get(\"hasStage\", [\"\"])) for eid,typ in node_type.items() if typ==\"Exam\") if vals and vals[0]}\n",
    "\n",
    "# ========= A' Tripartite (informative) =========\n",
    "def draw_tripartite_subset(year_min=1393, year_max=1410, max_questions_per_exam=12):\n",
    "    # pick subset by year\n",
    "    keep_time = {t for t,y in time_year.items() if y is not None and year_min <= y <= year_max}\n",
    "    keep_exam = {e for e,t in exam_to_time.items() if t in keep_time}\n",
    "    keep_q    = {q for q,e in question_to_exam.items() if e in keep_exam}\n",
    "\n",
    "    # downsample per exam to avoid overplot\n",
    "    if max_questions_per_exam is not None:\n",
    "        by_exam = defaultdict(list)\n",
    "        for q in keep_q:\n",
    "            by_exam[question_to_exam[q]].append(q)\n",
    "        keep_q = set(sum([qs[:max_questions_per_exam] for qs in by_exam.values()], []))\n",
    "\n",
    "    H = nx.MultiDiGraph()\n",
    "    for n in list(keep_time) + list(keep_exam) + list(keep_q):\n",
    "        H.add_node(n, t=node_type.get(n,\"Unknown\"))\n",
    "\n",
    "    for s,p,o in edges:\n",
    "        if s in H and o in H and p in (\"isHeldOn\",\"isPartOf\"):\n",
    "            H.add_edge(s,o,predicate=p)\n",
    "\n",
    "    # positions: layered layout with more spacing\n",
    "    pos = {}\n",
    "    def place(layer_nodes, y, spacing=1.5):\n",
    "        sorted_nodes = sorted(layer_nodes)\n",
    "        for i, n in enumerate(sorted_nodes):\n",
    "            pos[n] = (i * spacing, y)\n",
    "\n",
    "    T_nodes = [n for n in H if H.nodes[n]['t']==\"Time\"]\n",
    "    E_nodes = [n for n in H if H.nodes[n]['t']==\"Exam\"]\n",
    "    Q_nodes = [n for n in H if H.nodes[n]['t']==\"Question\"]\n",
    "\n",
    "    place(T_nodes, 2.5, spacing=2.0)   # Time: 넓은 간격\n",
    "    place(E_nodes, 1.5, spacing=1.8)   # Exam: 중간 간격\n",
    "    place(Q_nodes, 0.5, spacing=1.2)   # Question: 좁은 간격\n",
    "\n",
    "    # node style\n",
    "    color_map = {\"Time\":\"#5B8FF9\", \"Exam\":\"#61DDAA\", \"Question\":\"#F6BD16\", \"Unknown\":\"#999999\"}\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    for n in H.nodes():\n",
    "        deg = H.degree(n)\n",
    "        size = 150 + 60*deg  # 노드 크기 약간 축소\n",
    "        node_sizes.append(size)\n",
    "        node_colors.append(color_map.get(H.nodes[n]['t'],\"#999999\"))\n",
    "\n",
    "    # edge style\n",
    "    widths = []\n",
    "    alphas = []\n",
    "    for u,v,k in H.edges(keys=True):\n",
    "        p = H.get_edge_data(u,v,k).get(\"predicate\",\"\")\n",
    "        if p == \"isPartOf\":\n",
    "            widths.append(1.2)\n",
    "            alphas.append(0.25)\n",
    "        elif p == \"isHeldOn\":\n",
    "            widths.append(2.0)\n",
    "            alphas.append(0.4)\n",
    "        else:\n",
    "            widths.append(0.8)\n",
    "            alphas.append(0.2)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    # draw edges first\n",
    "    for (u,v,k), w, a in zip(H.edges(keys=True), widths, alphas):\n",
    "        nx.draw_networkx_edges(H, pos, edgelist=[(u,v)], width=w, alpha=a, \n",
    "                              arrows=True, arrowstyle='-|>', arrowsize=8)\n",
    "\n",
    "    # draw nodes\n",
    "    nx.draw_networkx_nodes(H, pos, node_color=node_colors, node_size=node_sizes, \n",
    "                          linewidths=0.5, edgecolors=\"#333333\")\n",
    "    \n",
    "    # draw labels with smart truncation\n",
    "    short_labels = {}\n",
    "    for n in H.nodes():\n",
    "        nm = node_name.get(n,\"\")\n",
    "        ntype = H.nodes[n]['t']\n",
    "        \n",
    "        if not nm:\n",
    "            if ntype == \"Time\":\n",
    "                y = time_year.get(n, \"\")\n",
    "                nm = str(y) if y else \"T\"\n",
    "            else:\n",
    "                nm = ntype[:1]\n",
    "        \n",
    "        short_labels[n] = smart_truncate(nm, max_len=10, node_type=ntype)\n",
    "    \n",
    "    # 폰트 크기 증가 (article용)\n",
    "    nx.draw_networkx_labels(H, pos, short_labels, font_size=9)\n",
    "\n",
    "    # Legend with larger font\n",
    "    from matplotlib.lines import Line2D\n",
    "    leg_elems = [\n",
    "        Line2D([0],[0], marker='o', color='w', label='Time', \n",
    "               markerfacecolor=color_map[\"Time\"], markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Exam', \n",
    "               markerfacecolor=color_map[\"Exam\"], markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Question', \n",
    "               markerfacecolor=color_map[\"Question\"], markersize=10),\n",
    "        Line2D([0],[0], color='#333333', lw=2.0, label='isHeldOn (Exam→Time)'),\n",
    "        Line2D([0],[0], color='#333333', lw=1.2, label='isPartOf (Question→Exam)'),\n",
    "    ]\n",
    "    plt.legend(handles=leg_elems, loc='upper left', frameon=True, fontsize=12)\n",
    "\n",
    "    # Title with larger font\n",
    "    plt.title(f\"Tripartite network (subset {year_min}-{year_max})\\n\"\n",
    "              f\"Nodes sized by degree; edges styled by predicate\\n\"\n",
    "              f\"Labels: Time=year, Exam=year+keyword, Question=abbreviated\", \n",
    "              fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    outpath = OUT_DIR / f\"A_prime_tripartite_{year_min}_{year_max}.pdf\"\n",
    "    plt.savefig(outpath, dpi=200, bbox_inches='tight', format='pdf')\n",
    "    plt.close()\n",
    "    return outpath\n",
    "\n",
    "# ========= B' Link completeness funnel =========\n",
    "def draw_link_funnel():\n",
    "    all_Q = {n for n,t in node_type.items() if t==\"Question\"}\n",
    "    all_E = {n for n,t in node_type.items() if t==\"Exam\"}\n",
    "\n",
    "    q_has_e = len(question_to_exam)\n",
    "    e_has_t = len(exam_to_time)\n",
    "\n",
    "    # Q→E→T chain completeness\n",
    "    q_chain = sum(1 for q in all_Q if q in question_to_exam and question_to_exam[q] in exam_to_time)\n",
    "\n",
    "    q_total = len(all_Q)\n",
    "    e_total = len(all_E)\n",
    "\n",
    "    metrics = pd.DataFrame([\n",
    "        {\"stage\":\"Question→Exam\", \"value\": 100.0 * (q_has_e / q_total if q_total else 0), \"num\": q_has_e, \"den\": q_total},\n",
    "        {\"stage\":\"Exam→Time\",    \"value\": 100.0 * (e_has_t / e_total if e_total else 0), \"num\": e_has_t, \"den\": e_total},\n",
    "        {\"stage\":\"Q→E→T chain\",  \"value\": 100.0 * (q_chain / q_total if q_total else 0), \"num\": q_chain, \"den\": q_total},\n",
    "    ])\n",
    "\n",
    "    plt.figure(figsize=(9,5))\n",
    "    ax = sns.barplot(data=metrics, x=\"stage\", y=\"value\")\n",
    "    ax.bar_label(ax.containers[0], labels=[f\"{v:.1f}% ({n}/{d})\" for v,n,d in zip(metrics['value'], metrics['num'], metrics['den'])],\n",
    "                 padding=3, fontsize=12)\n",
    "    ax.set_xlabel(\"\", fontsize=14)\n",
    "    ax.set_ylabel(\"Completeness (%)\", fontsize=14)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "    plt.ylim(0,100)\n",
    "    plt.title(\"Link completeness across the core chain\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    outpath = OUT_DIR / \"B_prime_link_completeness.pdf\"\n",
    "    plt.savefig(outpath, dpi=200, bbox_inches='tight', format='pdf')\n",
    "    plt.close()\n",
    "    return outpath\n",
    "\n",
    "# ========= B'' Attribute coverage heatmap =========\n",
    "Q_ATTRS = [(\"hasCategory\",\"Category\"), (\"hasSubcategory\",\"Subcategory\"),\n",
    "           (\"hasAbstract\",\"Abstract\"), (\"hasContent\",\"Content\"),\n",
    "           (\"hasSource\",\"Source\"), (\"hasSourceURL\",\"SourceURL\")]\n",
    "E_ATTRS = [(\"hasTypeA\",\"TypeA\"), (\"hasTypeB\",\"TypeB\"), (\"hasCategory\",\"ExamKind\"),\n",
    "           (\"hasStage\",\"Stage\"), (\"hasRound\",\"Round\"),\n",
    "           (\"isRecordedIn\",\"RecordTitle\"), (\"hasRecordURL\",\"RecordURL\")]\n",
    "T_ATTRS = [(\"year\",\"Year\"), (\"month\",\"Month\"), (\"day\",\"Day\"),\n",
    "           (\"sexagenaryKR\",\"KR Sexagenary\"), (\"sexagenaryCN\",\"CN Sexagenary\")]\n",
    "\n",
    "def coverage_for(entity_type, attrs):\n",
    "    nodes = [n for n,t in node_type.items() if t==entity_type]\n",
    "    total = len(nodes)\n",
    "    cov = []\n",
    "    for p,pretty in attrs:\n",
    "        count = 0\n",
    "        for n in nodes:\n",
    "            vals = lit[n].get(p, [])\n",
    "            if vals:\n",
    "                if any(str(v).strip() for v in vals):\n",
    "                    count += 1\n",
    "        cov.append(100.0 * (count / total if total else 0))\n",
    "    return [round(x,1) for x in cov]\n",
    "\n",
    "def draw_attr_heatmap():\n",
    "    data = []\n",
    "    idx = []\n",
    "    for etype, attrs in [(\"Question\", Q_ATTRS), (\"Exam\", E_ATTRS), (\"Time\", T_ATTRS)]:\n",
    "        idx.append(etype)\n",
    "        data.append(coverage_for(etype, attrs))\n",
    "    \n",
    "    cols = [pretty for _,pretty in Q_ATTRS] + [pretty for _,pretty in E_ATTRS] + [pretty for _,pretty in T_ATTRS]\n",
    "    mat = np.zeros((3, len(cols))) * np.nan\n",
    "    start = 0\n",
    "    for i,(attrs,etype) in enumerate([(Q_ATTRS,\"Question\"),(E_ATTRS,\"Exam\"),(T_ATTRS,\"Time\")]):\n",
    "        for j,(_,pretty) in enumerate(attrs):\n",
    "            mat[i, start+j] = coverage_for(etype, attrs)[j]\n",
    "        start += len(attrs)\n",
    "    df = pd.DataFrame(mat, index=[\"Question\",\"Exam\",\"Time\"], columns=cols)\n",
    "\n",
    "    plt.figure(figsize=(max(12, len(cols)*0.6), 5))\n",
    "    ax = sns.heatmap(df, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", vmin=0, vmax=100, \n",
    "                     cbar_kws={\"label\":\"Coverage (%)\"},\n",
    "                     linewidths=0.5, linecolor=\"#EEEEEE\", annot_kws={\"size\":12})\n",
    "    ax.set_xlabel(\"\", fontsize=14)\n",
    "    ax.set_ylabel(\"\", fontsize=14)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "    plt.title(\"Attribute coverage by entity\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    outpath = OUT_DIR / \"B_double_prime_attr_coverage.pdf\"\n",
    "    plt.savefig(outpath, dpi=200, bbox_inches='tight', format='pdf')\n",
    "    plt.close()\n",
    "    return outpath\n",
    "\n",
    "# ========= Run & save =========\n",
    "a_path = draw_tripartite_subset(1393, 1410, max_questions_per_exam=10)\n",
    "b1_path = draw_link_funnel()\n",
    "b2_path = draw_attr_heatmap()\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\"  \", a_path)\n",
    "print(\"  \", b1_path)\n",
    "print(\"  \", b2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ad4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f238f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
