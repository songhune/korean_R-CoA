{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4299f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, hashlib\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import os\n",
    "\n",
    "# ===== 한글/한자 폰트 자동 설정 =====\n",
    "def setup_korean_fonts():\n",
    "    \"\"\"macOS/Windows/Linux에서 한글/한자를 지원하는 폰트 자동 설정\"\"\"\n",
    "    system = platform.system()\n",
    "    \n",
    "    # 시스템별 한글/한자 폰트 우선순위\n",
    "    font_candidates = []\n",
    "    if system == 'Darwin':  # macOS\n",
    "        font_candidates = [\n",
    "            'AppleGothic',           # 한글 (macOS 기본)\n",
    "            'Apple SD Gothic Neo',   # 한글 (macOS)\n",
    "            'Arial Unicode MS',      # 한글+한자\n",
    "            'Nanum Gothic',          # 나눔고딕\n",
    "        ]\n",
    "    elif system == 'Windows':\n",
    "        font_candidates = [\n",
    "            'Malgun Gothic',    # 맑은고딕 (Windows 기본)\n",
    "            'Gulim',           # 굴림\n",
    "            'Batang',          # 바탕\n",
    "            'NanumGothic',     # 나눔고딕\n",
    "        ]\n",
    "    else:  # Linux\n",
    "        font_candidates = [\n",
    "            'NanumGothic',\n",
    "            'Noto Sans CJK KR',\n",
    "            'Noto Sans KR',\n",
    "            'DejaVu Sans',\n",
    "        ]\n",
    "    \n",
    "    # 사용 가능한 폰트 찾기\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            print(f\"[FONT] 한글/한자 폰트 설정: {font}\")\n",
    "            plt.rcParams['font.family'] = font\n",
    "            plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "            return font\n",
    "    \n",
    "    # 폰트를 찾지 못한 경우 경고\n",
    "    print(\"[WARNING] 한글/한자 지원 폰트를 찾지 못했습니다. 텍스트가 깨질 수 있습니다.\")\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    return None\n",
    "\n",
    "# 폰트 설정 적용\n",
    "setup_korean_fonts()\n",
    "\n",
    "PATH = \"gwashi.csv\"                      # 입력\n",
    "ENC  = \"utf-8\"                           # 필요시 \"cp949\"\n",
    "N    = 12                                # 해시 길이(접두사 제외)\n",
    "\n",
    "# === 매핑: 각자 컬럼명에 맞게 수정 ===\n",
    "MAP = {\n",
    "    \"person\": dict(prefix=\"PE\", fields=[\"writer\"]),                    # 예: 작자명\n",
    "    \"answer\": dict(prefix=\"A\",  fields=[\"writer\",\"q_id\",\"year\"]),      # 예: (작자, 문제, 연도)\n",
    "    \"question\": dict(prefix=\"Q\", fields=[\"q_name\",\"category1\",\"category2\"]),\n",
    "}\n",
    "\n",
    "def sha(prefix,*fs,n=N):\n",
    "    s = \"||\".join(\"\" if pd.isna(x) else str(x).strip() for x in fs)\n",
    "    return f\"{prefix}{hashlib.sha1(s.encode()).hexdigest()[:n]}\"\n",
    "\n",
    "def build_id_series(df, prefix, fields, n=N):\n",
    "    return [sha(prefix, *[df[f].iloc[i] if f in df.columns else \"\" for f in fields], n=n) for i in range(len(df))]\n",
    "\n",
    "def stability_and_uniqueness(df, key_name, prefix, fields):\n",
    "    v1 = build_id_series(df, prefix, fields)\n",
    "    v2 = build_id_series(df, prefix, fields)           # 동일 입력 재계산\n",
    "    stab = (pd.Series(v1) == pd.Series(v2)).mean()     # 재현성(=1.0이 정상)\n",
    "    dup  = pd.Series(v1).value_counts()\n",
    "    dup_tbl = dup[dup>1].reset_index().rename(columns={\"index\":key_name,0:\"cnt\"})\n",
    "    return stab, dup_tbl\n",
    "\n",
    "df = pd.read_csv(PATH, encoding=ENC, engine=\"python\")\n",
    "\n",
    "reports = {}\n",
    "for ent, cfg in MAP.items():\n",
    "    stab, dup_tbl = stability_and_uniqueness(df, f\"{ent}_id\", cfg[\"prefix\"], cfg[\"fields\"])\n",
    "    reports[ent] = {\"stability\": float(stab), \"dup_cnt\": int(len(dup_tbl)), \"dup_table\": dup_tbl.head(20)}\n",
    "\n",
    "# 출력 요약\n",
    "for ent, r in reports.items():\n",
    "    print(f\"[{ent.upper()}] 재현성={r['stability']:.4f}  중복key개수={r['dup_cnt']}\")\n",
    "    if r[\"dup_cnt\"]>0:\n",
    "        print(r[\"dup_table\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750a2e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FONT] 한글/한자 폰트 설정: AppleGothic\n",
      "[LOAD] 3348 rows\n",
      "[GRAPH] nodes=5469 edges=6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/cj4b9h512gs7by1fyqkz5j5c0000gn/T/ipykernel_24167/4118093376.py:163: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Figures saved in /Users/songhune/Library/Mobile Documents/com~apple~CloudDocs/Workspace/korean_eda/notebook/experiments/graphs\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Entity graph (Exam–Question–Time) + Temporal flow visualization (Korean font fixed)\n",
    "\"\"\"\n",
    "import os, json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import networkx as nx\n",
    "import platform\n",
    "\n",
    "# ===== 한글/한자 폰트 자동 설정 =====\n",
    "def setup_korean_fonts():\n",
    "    \"\"\"macOS/Windows/Linux에서 한글/한자를 지원하는 폰트 자동 설정\"\"\"\n",
    "    system = platform.system()\n",
    "    \n",
    "    # 시스템별 한글/한자 폰트 우선순위\n",
    "    font_candidates = []\n",
    "    if system == 'Darwin':  # macOS\n",
    "        font_candidates = [\n",
    "            'AppleGothic',           # 한글 (macOS 기본)\n",
    "            'Apple SD Gothic Neo',   # 한글 (macOS)\n",
    "            'Arial Unicode MS',      # 한글+한자\n",
    "            'Nanum Gothic',          # 나눔고딕\n",
    "        ]\n",
    "    elif system == 'Windows':\n",
    "        font_candidates = [\n",
    "            'Malgun Gothic',    # 맑은고딕 (Windows 기본)\n",
    "            'Gulim',           # 굴림\n",
    "            'Batang',          # 바탕\n",
    "            'NanumGothic',     # 나눔고딕\n",
    "        ]\n",
    "    else:  # Linux\n",
    "        font_candidates = [\n",
    "            'NanumGothic',\n",
    "            'Noto Sans CJK KR',\n",
    "            'Noto Sans KR',\n",
    "            'DejaVu Sans',\n",
    "        ]\n",
    "    \n",
    "    # 사용 가능한 폰트 찾기\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            print(f\"[FONT] 한글/한자 폰트 설정: {font}\")\n",
    "            plt.rcParams['font.family'] = font\n",
    "            plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "            return font\n",
    "    \n",
    "    # 폰트를 찾지 못한 경우 경고\n",
    "    print(\"[WARNING] 한글/한자 지원 폰트를 찾지 못했습니다. 텍스트가 깨질 수 있습니다.\")\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    return None\n",
    "\n",
    "# 폰트 설정 적용\n",
    "setup_korean_fonts()\n",
    "\n",
    "# ===== Path setup =====\n",
    "BASE = \"/Users/songhune/Library/Mobile Documents/com~apple~CloudDocs/Workspace/korean_eda\"\n",
    "IN_JSONL = os.path.join(BASE, \"notebook\", \"eda_outputs\", \"1번실험\", \"triples_no_answer.jsonl\")\n",
    "OUT_DIR  = os.path.join(BASE, \"notebook\", \"experiments\", \"graphs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def infer_type(nid):\n",
    "    if not nid: return \"Unknown\"\n",
    "    return {\"E\":\"Exam\",\"Q\":\"Question\",\"T\":\"Time\"}.get(nid[0].upper(),\"Unknown\")\n",
    "\n",
    "# ===== Load =====\n",
    "rows=[]\n",
    "with open(IN_JSONL,encoding=\"utf-8\") as f:\n",
    "    for l in f:\n",
    "        if l.strip(): rows.append(json.loads(l))\n",
    "print(f\"[LOAD] {len(rows)} rows\")\n",
    "\n",
    "# ===== Parse nodes/edges =====\n",
    "G = nx.MultiDiGraph()\n",
    "exam_to_time, q_to_exam, time_year = {}, {}, {}\n",
    "q_cat, q_sub = {}, {}\n",
    "exam_stage, exam_typeA, exam_kind = {}, {}, {}\n",
    "\n",
    "for r in rows:\n",
    "    for key in (\"exam\",\"question\",\"time\"):\n",
    "        if key in r:\n",
    "            nid = r[key].get(\"id\")\n",
    "            if nid:\n",
    "                G.add_node(nid, type=infer_type(nid), name=r[key].get(\"name\",\"\"))\n",
    "    for t in r[\"triples\"]:\n",
    "        s,p,o,o_type = t[\"s\"], t[\"p\"], t[\"o\"], t.get(\"o_type\",\"\")\n",
    "        if o_type==\"id\":\n",
    "            G.add_node(s,type=infer_type(s))\n",
    "            G.add_node(o,type=infer_type(o))\n",
    "            G.add_edge(s,o,predicate=p)\n",
    "            if p==\"isHeldOn\": exam_to_time[s]=o\n",
    "            if p==\"isPartOf\": q_to_exam[s]=o\n",
    "        else:\n",
    "            # literals as attributes\n",
    "            if infer_type(s)==\"Time\" and p==\"year\":\n",
    "                try: time_year[s]=int(float(o))\n",
    "                except: pass\n",
    "            if infer_type(s)==\"Question\":\n",
    "                if p==\"hasCategory\": q_cat[s]=o\n",
    "                if p==\"hasSubcategory\": q_sub[s]=o\n",
    "            if infer_type(s)==\"Exam\":\n",
    "                if p==\"hasStage\": exam_stage[s]=o\n",
    "                if p==\"hasTypeA\": exam_typeA[s]=o\n",
    "                if p==\"hasCategory\": exam_kind[s]=o\n",
    "\n",
    "print(f\"[GRAPH] nodes={G.number_of_nodes()} edges={G.number_of_edges()}\")\n",
    "\n",
    "# ===== Predicate counts =====\n",
    "pred_counts = Counter(d.get(\"predicate\",\"\") for _,_,d in G.edges(data=True))\n",
    "pred_df = pd.DataFrame(sorted(pred_counts.items(), key=lambda x:-x[1]), columns=[\"predicate\",\"count\"])\n",
    "pred_df.to_csv(os.path.join(OUT_DIR,\"edge_density.csv\"),index=False)\n",
    "\n",
    "# ===== Temporal base table =====\n",
    "records=[]\n",
    "for q,e in q_to_exam.items():\n",
    "    t=exam_to_time.get(e)\n",
    "    y=time_year.get(t)\n",
    "    if y is None: continue\n",
    "    records.append({\n",
    "        \"year\":y,\n",
    "        \"exam_id\":e,\n",
    "        \"question_id\":q,\n",
    "        \"stage\":exam_stage.get(e,\"\"),\n",
    "        \"typeA\":exam_typeA.get(e,\"\"),\n",
    "        \"examKind\":exam_kind.get(e,\"\"),\n",
    "        \"qCategory\":q_cat.get(q,\"\"),\n",
    "        \"qSubcategory\":q_sub.get(q,\"\")\n",
    "    })\n",
    "flow_df=pd.DataFrame(records)\n",
    "flow_df.to_csv(os.path.join(OUT_DIR,\"temporal_flow_base.csv\"),index=False)\n",
    "\n",
    "# ===== Figure A: schema subset =====\n",
    "def draw_schema(year_min=1393,year_max=1410):\n",
    "    keep_T={t for t,y in time_year.items() if year_min<=y<=year_max}\n",
    "    keep_E={e for e,t in exam_to_time.items() if t in keep_T}\n",
    "    keep_Q={q for q,e in q_to_exam.items() if e in keep_E}\n",
    "    H=nx.MultiDiGraph()\n",
    "    for n in list(keep_T)+list(keep_E)+list(keep_Q):\n",
    "        H.add_node(n,type=infer_type(n))\n",
    "    for s,t,d in G.edges(data=True):\n",
    "        if s in H and t in H and d[\"predicate\"] in (\"isHeldOn\",\"isPartOf\"):\n",
    "            H.add_edge(s,t,predicate=d[\"predicate\"])\n",
    "    pos={}\n",
    "    def layer(nodes,y):\n",
    "        for i,n in enumerate(sorted(nodes)): pos[n]=(i,y)\n",
    "    Tn=[n for n in H if infer_type(n)==\"Time\"]\n",
    "    En=[n for n in H if infer_type(n)==\"Exam\"]\n",
    "    Qn=[n for n in H if infer_type(n)==\"Question\"]\n",
    "    layer(Tn,2); layer(En,1); layer(Qn,0)\n",
    "    plt.figure(figsize=(14,9))\n",
    "    nx.draw(H,pos,with_labels=False,node_size=400,arrows=True)\n",
    "    labs={n:H.nodes[n].get(\"name\",\"\")[:12] for n in H}\n",
    "    nx.draw_networkx_labels(H,pos,labs,font_size=8)\n",
    "    e_labels={(u,v):d[\"predicate\"] for u,v,d in H.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(H,pos,edge_labels=e_labels,font_size=8)\n",
    "    plt.title(f\"Exam–Question–Time schema (subset {year_min}-{year_max})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR,\"A_schema_small_1393_1410.png\"),dpi=200,bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "draw_schema()\n",
    "\n",
    "# ===== Figure B: edge density =====\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(pred_df[\"predicate\"],pred_df[\"count\"],color=\"steelblue\")\n",
    "plt.xticks(rotation=45,ha='right')\n",
    "plt.ylabel(\"Edge count\")\n",
    "plt.title(\"Edge density by predicate\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR,\"B_edge_density.png\"),dpi=200,bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ===== Figure C: temporal flow =====\n",
    "annual=flow_df.groupby(\"year\")[\"question_id\"].nunique().reset_index(name=\"n_questions\")\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(annual[\"year\"],annual[\"n_questions\"],color=\"darkslateblue\",lw=1.8)\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Number of questions\")\n",
    "plt.title(\"Number of questions per year\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR,\"C1_questions_per_year.png\"),dpi=200,bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Stacked area by stage\n",
    "top_stages=flow_df[\"stage\"].value_counts().head(5).index.tolist()\n",
    "tmp=flow_df.copy()\n",
    "tmp[\"stage_top\"]=np.where(tmp[\"stage\"].isin(top_stages),tmp[\"stage\"],\"Others\")\n",
    "pivot=(tmp.groupby([\"year\",\"stage_top\"])[\"question_id\"]\n",
    "        .nunique().reset_index()\n",
    "        .pivot(index=\"year\",columns=\"stage_top\",values=\"question_id\").fillna(0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.stackplot(pivot.index,*[pivot[c] for c in pivot.columns],labels=pivot.columns)\n",
    "plt.legend(loc=\"upper left\",fontsize=9)\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Number of questions\")\n",
    "plt.title(\"Questions by stage over time\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR,\"C2_stage_over_time.png\"),dpi=200,bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Stacked area by category\n",
    "top_cats=flow_df[\"qCategory\"].value_counts().head(5).index.tolist()\n",
    "tmp=flow_df.copy()\n",
    "tmp[\"cat_top\"]=np.where(tmp[\"qCategory\"].isin(top_cats),tmp[\"qCategory\"],\"Others\")\n",
    "pivot=(tmp.groupby([\"year\",\"cat_top\"])[\"question_id\"]\n",
    "        .nunique().reset_index()\n",
    "        .pivot(index=\"year\",columns=\"cat_top\",values=\"question_id\").fillna(0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.stackplot(pivot.index,*[pivot[c] for c in pivot.columns],labels=pivot.columns)\n",
    "plt.legend(loc=\"upper left\",fontsize=9)\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Number of questions\")\n",
    "plt.title(\"Questions by category over time\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR,\"C3_category_over_time.png\"),dpi=200,bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n✅ Figures saved in {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00a4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FONT] 한글/한자 폰트 설정: AppleGothic\n",
      "[FONT] 한글/한자 폰트 설정: AppleGothic\n",
      "✅ Saved:\n",
      "   /Users/songhune/Library/Mobile Documents/com~apple~CloudDocs/Workspace/korean_eda/notebook/experiments/graphs/A_prime_tripartite_1393_1410.png\n",
      "   /Users/songhune/Library/Mobile Documents/com~apple~CloudDocs/Workspace/korean_eda/notebook/experiments/graphs/B_prime_link_completeness.png\n",
      "   /Users/songhune/Library/Mobile Documents/com~apple~CloudDocs/Workspace/korean_eda/notebook/experiments/graphs/B_double_prime_attr_coverage.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Useful diagnostics & visualization (Korean font fixed + Improved labels)\n",
    "- A' Tripartite network (with informative styling)\n",
    "- B' Link completeness funnel (Q→E, E→T, Q→E→T)\n",
    "- B'' Attribute coverage heatmap\n",
    "\"\"\"\n",
    "import os, json, math, re\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import platform\n",
    "\n",
    "# ===== 한글/한자 폰트 자동 설정 (개선 버전) =====\n",
    "def setup_korean_fonts():\n",
    "    \"\"\"macOS/Windows/Linux에서 한글/한자를 지원하는 폰트 자동 설정\"\"\"\n",
    "    system = platform.system()\n",
    "    \n",
    "    # 시스템별 한글/한자 폰트 우선순위\n",
    "    font_candidates = []\n",
    "    if system == 'Darwin':  # macOS\n",
    "        font_candidates = [\n",
    "            'AppleGothic',           # 한글 (macOS 기본)\n",
    "            'Apple SD Gothic Neo',   # 한글 (macOS)\n",
    "            'Arial Unicode MS',      # 한글+한자\n",
    "            'Nanum Gothic',          # 나눔고딕\n",
    "        ]\n",
    "    elif system == 'Windows':\n",
    "        font_candidates = [\n",
    "            'Malgun Gothic',    # 맑은고딕 (Windows 기본)\n",
    "            'Gulim',           # 굴림\n",
    "            'Batang',          # 바탕\n",
    "            'NanumGothic',     # 나눔고딕\n",
    "        ]\n",
    "    else:  # Linux\n",
    "        font_candidates = [\n",
    "            'NanumGothic',\n",
    "            'Noto Sans CJK KR',\n",
    "            'Noto Sans KR',\n",
    "            'DejaVu Sans',\n",
    "        ]\n",
    "    \n",
    "    # 사용 가능한 폰트 찾기\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    selected_font = None\n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            selected_font = font\n",
    "            break\n",
    "    \n",
    "    if not selected_font:\n",
    "        print(\"[WARNING] 한글/한자 지원 폰트를 찾지 못했습니다. 텍스트가 깨질 수 있습니다.\")\n",
    "        return None\n",
    "    \n",
    "    # 강제 설정 (matplotlib + pyplot 모두)\n",
    "    matplotlib.rcParams['font.family'] = selected_font\n",
    "    matplotlib.rcParams['font.sans-serif'] = [selected_font]\n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    plt.rcParams['font.family'] = selected_font\n",
    "    plt.rcParams['font.sans-serif'] = [selected_font]\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    plt.rc('font', family=selected_font)\n",
    "    \n",
    "    print(f\"[FONT] 한글/한자 폰트 설정: {selected_font}\")\n",
    "    return selected_font\n",
    "\n",
    "# 폰트 설정 적용\n",
    "setup_korean_fonts()\n",
    "\n",
    "# seaborn 설정 (이게 폰트를 리셋할 수 있음!)\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# seaborn 설정 후 폰트 다시 적용 (중요!)\n",
    "setup_korean_fonts()\n",
    "\n",
    "# ========= Paths =========\n",
    "BASE = \"/Users/songhune/Library/Mobile Documents/com~apple~CloudDocs/Workspace/korean_eda\"\n",
    "IN_JSONL = os.path.join(BASE, \"notebook\", \"eda_outputs\", \"1번실험\",\"triples_no_answer.jsonl\")\n",
    "OUT_DIR  = os.path.join(BASE, \"notebook\", \"experiments\", \"graphs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ========= Helpers =========\n",
    "def infer_type(nid: str) -> str:\n",
    "    if not nid: return \"Unknown\"\n",
    "    return {\"E\":\"Exam\",\"Q\":\"Question\",\"T\":\"Time\"}.get(nid[0].upper(),\"Unknown\")\n",
    "\n",
    "def safe_int(x):\n",
    "    try: return int(float(x))\n",
    "    except: return None\n",
    "\n",
    "def smart_truncate(text: str, max_len: int = 8, node_type: str = \"Unknown\") -> str:\n",
    "    \"\"\"\n",
    "    스마트 텍스트 축약\n",
    "    - Time: 연도만 표시\n",
    "    - Exam: 연도 + 핵심단어\n",
    "    - Question: 첫 단어 + 말줄임\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Time 노드: 연도만 추출\n",
    "    if node_type == \"Time\":\n",
    "        year_match = re.search(r'(\\d{4})', text)\n",
    "        if year_match:\n",
    "            return year_match.group(1)\n",
    "        return text[:max_len]\n",
    "    \n",
    "    # Exam 노드: 연도 + 첫 단어\n",
    "    if node_type == \"Exam\":\n",
    "        year_match = re.search(r'(\\d{4})', text)\n",
    "        year = year_match.group(1) if year_match else \"\"\n",
    "        # 연도 제거 후 첫 단어 추출\n",
    "        text_without_year = re.sub(r'\\d{4}년?_?', '', text)\n",
    "        first_word = text_without_year.split('_')[0] if '_' in text_without_year else text_without_year\n",
    "        first_word = first_word[:6]\n",
    "        return f\"{year}\\n{first_word}\" if year else first_word\n",
    "    \n",
    "    # Question 노드: 첫 부분만\n",
    "    if node_type == \"Question\":\n",
    "        # 연도 제거\n",
    "        text_clean = re.sub(r'\\d{4}년?_?', '', text)\n",
    "        parts = text_clean.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[0][:4]}\\n{parts[1][:4]}\"\n",
    "        return text_clean[:max_len] + \"…\"\n",
    "    \n",
    "    # 기본: 단순 자르기\n",
    "    return text[:max_len] + (\"…\" if len(text) > max_len else \"\")\n",
    "\n",
    "# ========= Load =========\n",
    "rows = []\n",
    "with open(IN_JSONL, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            rows.append(json.loads(line))\n",
    "\n",
    "# ========= Parse triples into structures =========\n",
    "# Nodes\n",
    "node_name = {}\n",
    "node_type = {}\n",
    "\n",
    "# Edges (only id→id)\n",
    "edges = []  # (s, p, o)\n",
    "\n",
    "# Literal attributes per node\n",
    "lit = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for r in rows:\n",
    "    for key in (\"exam\", \"question\", \"time\"):\n",
    "        if key in r and isinstance(r[key], dict):\n",
    "            nid = r[key].get(\"id\")\n",
    "            if nid:\n",
    "                node_type[nid] = infer_type(nid)\n",
    "                # prefer non-empty name\n",
    "                nm = r[key].get(\"name\") or \"\"\n",
    "                if nm:\n",
    "                    node_name[nid] = nm\n",
    "    for t in r.get(\"triples\", []):\n",
    "        s, p, o = t.get(\"s\"), t.get(\"p\"), t.get(\"o\")\n",
    "        o_type = t.get(\"o_type\", \"\")\n",
    "        if not s or not p or o is None: \n",
    "            continue\n",
    "        if o_type == \"id\":\n",
    "            edges.append((s, p, o))\n",
    "            # register node types if missing\n",
    "            if s not in node_type: node_type[s] = infer_type(s)\n",
    "            if o not in node_type: node_type[o] = infer_type(o)\n",
    "        else:\n",
    "            lit[s][p].append(o)\n",
    "\n",
    "# Quick dictionaries for key relations\n",
    "exam_to_time = {}      # E -> T\n",
    "question_to_exam = {}  # Q -> E\n",
    "for s, p, o in edges:\n",
    "    if p == \"isHeldOn\" and node_type.get(s) == \"Exam\" and node_type.get(o) == \"Time\":\n",
    "        exam_to_time[s] = o\n",
    "    if p == \"isPartOf\" and node_type.get(s) == \"Question\" and node_type.get(o) == \"Exam\":\n",
    "        question_to_exam[s] = o\n",
    "\n",
    "# Useful literal maps\n",
    "time_year = {t: safe_int(vals[0]) for t,vals in ((tid, lit[tid].get(\"year\", [\"\"])) for tid,typ in node_type.items() if typ==\"Time\") if vals}\n",
    "q_cat     = {q: vals[0] for q,vals in ((q, lit[q].get(\"hasCategory\", [\"\"])) for q,typ in node_type.items() if typ==\"Question\") if vals and vals[0]}\n",
    "exam_stage= {e: vals[0] for e,vals in ((eid, lit[eid].get(\"hasStage\", [\"\"])) for eid,typ in node_type.items() if typ==\"Exam\") if vals and vals[0]}\n",
    "\n",
    "# ========= A' Tripartite (informative) =========\n",
    "def draw_tripartite_subset(year_min=1393, year_max=1410, max_questions_per_exam=12):\n",
    "    # pick subset by year\n",
    "    keep_time = {t for t,y in time_year.items() if y is not None and year_min <= y <= year_max}\n",
    "    keep_exam = {e for e,t in exam_to_time.items() if t in keep_time}\n",
    "    keep_q    = {q for q,e in question_to_exam.items() if e in keep_exam}\n",
    "\n",
    "    # downsample per exam to avoid overplot\n",
    "    if max_questions_per_exam is not None:\n",
    "        by_exam = defaultdict(list)\n",
    "        for q in keep_q:\n",
    "            by_exam[question_to_exam[q]].append(q)\n",
    "        keep_q = set(sum([qs[:max_questions_per_exam] for qs in by_exam.values()], []))\n",
    "\n",
    "    H = nx.MultiDiGraph()\n",
    "    for n in list(keep_time) + list(keep_exam) + list(keep_q):\n",
    "        H.add_node(n, t=node_type.get(n,\"Unknown\"))\n",
    "\n",
    "    for s,p,o in edges:\n",
    "        if s in H and o in H and p in (\"isHeldOn\",\"isPartOf\"):\n",
    "            H.add_edge(s,o,predicate=p)\n",
    "\n",
    "    # positions: layered layout with more spacing\n",
    "    pos = {}\n",
    "    def place(layer_nodes, y, spacing=1.5):\n",
    "        sorted_nodes = sorted(layer_nodes)\n",
    "        for i, n in enumerate(sorted_nodes):\n",
    "            pos[n] = (i * spacing, y)\n",
    "\n",
    "    T_nodes = [n for n in H if H.nodes[n]['t']==\"Time\"]\n",
    "    E_nodes = [n for n in H if H.nodes[n]['t']==\"Exam\"]\n",
    "    Q_nodes = [n for n in H if H.nodes[n]['t']==\"Question\"]\n",
    "\n",
    "    place(T_nodes, 2.5, spacing=2.0)   # Time: 넓은 간격\n",
    "    place(E_nodes, 1.5, spacing=1.8)   # Exam: 중간 간격\n",
    "    place(Q_nodes, 0.5, spacing=1.2)   # Question: 좁은 간격\n",
    "\n",
    "    # node style\n",
    "    color_map = {\"Time\":\"#5B8FF9\", \"Exam\":\"#61DDAA\", \"Question\":\"#F6BD16\", \"Unknown\":\"#999999\"}\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    for n in H.nodes():\n",
    "        deg = H.degree(n)\n",
    "        size = 150 + 60*deg  # 노드 크기 약간 축소\n",
    "        node_sizes.append(size)\n",
    "        node_colors.append(color_map.get(H.nodes[n]['t'],\"#999999\"))\n",
    "\n",
    "    # edge style\n",
    "    widths = []\n",
    "    alphas = []\n",
    "    for u,v,k in H.edges(keys=True):\n",
    "        p = H.get_edge_data(u,v,k).get(\"predicate\",\"\")\n",
    "        if p == \"isPartOf\":\n",
    "            widths.append(1.2)\n",
    "            alphas.append(0.25)\n",
    "        elif p == \"isHeldOn\":\n",
    "            widths.append(2.0)\n",
    "            alphas.append(0.4)\n",
    "        else:\n",
    "            widths.append(0.8)\n",
    "            alphas.append(0.2)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    # draw edges first\n",
    "    for (u,v,k), w, a in zip(H.edges(keys=True), widths, alphas):\n",
    "        nx.draw_networkx_edges(H, pos, edgelist=[(u,v)], width=w, alpha=a, \n",
    "                              arrows=True, arrowstyle='-|>', arrowsize=8)\n",
    "\n",
    "    # draw nodes\n",
    "    nx.draw_networkx_nodes(H, pos, node_color=node_colors, node_size=node_sizes, \n",
    "                          linewidths=0.5, edgecolors=\"#333333\")\n",
    "    \n",
    "    # draw labels with smart truncation\n",
    "    short_labels = {}\n",
    "    for n in H.nodes():\n",
    "        nm = node_name.get(n,\"\")\n",
    "        ntype = H.nodes[n]['t']\n",
    "        \n",
    "        if not nm:\n",
    "            if ntype == \"Time\":\n",
    "                y = time_year.get(n, \"\")\n",
    "                nm = str(y) if y else \"T\"\n",
    "            else:\n",
    "                nm = ntype[:1]\n",
    "        \n",
    "        short_labels[n] = smart_truncate(nm, max_len=10, node_type=ntype)\n",
    "    \n",
    "    # 폰트 크기를 더 작게\n",
    "    nx.draw_networkx_labels(H, pos, short_labels, font_size=6)\n",
    "\n",
    "    # Legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    leg_elems = [\n",
    "        Line2D([0],[0], marker='o', color='w', label='Time', \n",
    "               markerfacecolor=color_map[\"Time\"], markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Exam', \n",
    "               markerfacecolor=color_map[\"Exam\"], markersize=10),\n",
    "        Line2D([0],[0], marker='o', color='w', label='Question', \n",
    "               markerfacecolor=color_map[\"Question\"], markersize=10),\n",
    "        Line2D([0],[0], color='#333333', lw=2.0, label='isHeldOn (Exam→Time)'),\n",
    "        Line2D([0],[0], color='#333333', lw=1.2, label='isPartOf (Question→Exam)'),\n",
    "    ]\n",
    "    plt.legend(handles=leg_elems, loc='upper left', frameon=True, fontsize=10)\n",
    "\n",
    "    # Title\n",
    "    plt.title(f\"Tripartite network (subset {year_min}-{year_max})\\n\"\n",
    "              f\"Nodes sized by degree; edges styled by predicate\\n\"\n",
    "              f\"Labels: Time=year, Exam=year+keyword, Question=abbreviated\", \n",
    "              fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    outpath = os.path.join(OUT_DIR, f\"A_prime_tripartite_{year_min}_{year_max}.png\")\n",
    "    plt.savefig(outpath, dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return outpath\n",
    "\n",
    "# ========= B' Link completeness funnel =========\n",
    "def draw_link_funnel():\n",
    "    all_Q = {n for n,t in node_type.items() if t==\"Question\"}\n",
    "    all_E = {n for n,t in node_type.items() if t==\"Exam\"}\n",
    "\n",
    "    q_has_e = len(question_to_exam)\n",
    "    e_has_t = len(exam_to_time)\n",
    "\n",
    "    # Q→E→T chain completeness\n",
    "    q_chain = sum(1 for q in all_Q if q in question_to_exam and question_to_exam[q] in exam_to_time)\n",
    "\n",
    "    q_total = len(all_Q)\n",
    "    e_total = len(all_E)\n",
    "\n",
    "    metrics = pd.DataFrame([\n",
    "        {\"stage\":\"Question→Exam\", \"value\": 100.0 * (q_has_e / q_total if q_total else 0), \"num\": q_has_e, \"den\": q_total},\n",
    "        {\"stage\":\"Exam→Time\",    \"value\": 100.0 * (e_has_t / e_total if e_total else 0), \"num\": e_has_t, \"den\": e_total},\n",
    "        {\"stage\":\"Q→E→T chain\",  \"value\": 100.0 * (q_chain / q_total if q_total else 0), \"num\": q_chain, \"den\": q_total},\n",
    "    ])\n",
    "\n",
    "    plt.figure(figsize=(9,5))\n",
    "    ax = sns.barplot(data=metrics, x=\"stage\", y=\"value\")\n",
    "    ax.bar_label(ax.containers[0], labels=[f\"{v:.1f}% ({n}/{d})\" for v,n,d in zip(metrics['value'], metrics['num'], metrics['den'])],\n",
    "                 padding=3, fontsize=11)\n",
    "    plt.ylim(0,100)\n",
    "    plt.ylabel(\"Completeness (%)\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.title(\"Link completeness across the core chain\")\n",
    "    plt.tight_layout()\n",
    "    outpath = os.path.join(OUT_DIR, \"B_prime_link_completeness.png\")\n",
    "    plt.savefig(outpath, dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return outpath\n",
    "\n",
    "# ========= B'' Attribute coverage heatmap =========\n",
    "Q_ATTRS = [(\"hasCategory\",\"Category\"), (\"hasSubcategory\",\"Subcategory\"),\n",
    "           (\"hasAbstract\",\"Abstract\"), (\"hasContent\",\"Content\"),\n",
    "           (\"hasSource\",\"Source\"), (\"hasSourceURL\",\"SourceURL\")]\n",
    "E_ATTRS = [(\"hasTypeA\",\"TypeA\"), (\"hasTypeB\",\"TypeB\"), (\"hasCategory\",\"ExamKind\"),\n",
    "           (\"hasStage\",\"Stage\"), (\"hasRound\",\"Round\"),\n",
    "           (\"isRecordedIn\",\"RecordTitle\"), (\"hasRecordURL\",\"RecordURL\")]\n",
    "T_ATTRS = [(\"year\",\"Year\"), (\"month\",\"Month\"), (\"day\",\"Day\"),\n",
    "           (\"sexagenaryKR\",\"KR Sexagenary\"), (\"sexagenaryCN\",\"CN Sexagenary\")]\n",
    "\n",
    "def coverage_for(entity_type, attrs):\n",
    "    nodes = [n for n,t in node_type.items() if t==entity_type]\n",
    "    total = len(nodes)\n",
    "    cov = []\n",
    "    for p,pretty in attrs:\n",
    "        count = 0\n",
    "        for n in nodes:\n",
    "            vals = lit[n].get(p, [])\n",
    "            if vals:\n",
    "                if any(str(v).strip() for v in vals):\n",
    "                    count += 1\n",
    "        cov.append(100.0 * (count / total if total else 0))\n",
    "    return [round(x,1) for x in cov]\n",
    "\n",
    "def draw_attr_heatmap():\n",
    "    data = []\n",
    "    idx = []\n",
    "    for etype, attrs in [(\"Question\", Q_ATTRS), (\"Exam\", E_ATTRS), (\"Time\", T_ATTRS)]:\n",
    "        idx.append(etype)\n",
    "        data.append(coverage_for(etype, attrs))\n",
    "    \n",
    "    cols = [pretty for _,pretty in Q_ATTRS] + [pretty for _,pretty in E_ATTRS] + [pretty for _,pretty in T_ATTRS]\n",
    "    mat = np.zeros((3, len(cols))) * np.nan\n",
    "    start = 0\n",
    "    for i,(attrs,etype) in enumerate([(Q_ATTRS,\"Question\"),(E_ATTRS,\"Exam\"),(T_ATTRS,\"Time\")]):\n",
    "        for j,(_,pretty) in enumerate(attrs):\n",
    "            mat[i, start+j] = coverage_for(etype, attrs)[j]\n",
    "        start += len(attrs)\n",
    "    df = pd.DataFrame(mat, index=[\"Question\",\"Exam\",\"Time\"], columns=cols)\n",
    "\n",
    "    plt.figure(figsize=(max(12, len(cols)*0.6), 5))\n",
    "    ax = sns.heatmap(df, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", vmin=0, vmax=100, \n",
    "                     cbar_kws={\"label\":\"Coverage (%)\"},\n",
    "                     linewidths=0.5, linecolor=\"#EEEEEE\", annot_kws={\"size\":10})\n",
    "    plt.title(\"Attribute coverage by entity\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    outpath = os.path.join(OUT_DIR, \"B_double_prime_attr_coverage.png\")\n",
    "    plt.savefig(outpath, dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return outpath\n",
    "\n",
    "# ========= Run & save =========\n",
    "a_path = draw_tripartite_subset(1393, 1410, max_questions_per_exam=10)\n",
    "b1_path = draw_link_funnel()\n",
    "b2_path = draw_attr_heatmap()\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\"  \", a_path)\n",
    "print(\"  \", b1_path)\n",
    "print(\"  \", b2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ad4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
