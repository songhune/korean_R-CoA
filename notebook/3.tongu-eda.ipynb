{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# ACCN-INS Dataset Exploratory Data Analysis (EDA)\n",
    "\n",
    "이 노트북은 `/home/work/songhune/ACCN-INS.json` 데이터셋에 대한 종합적인 탐색적 데이터 분석을 수행합니다.\n",
    "\n",
    "**데이터셋 개요:**\n",
    "- 총 4,018,092개의 레코드\n",
    "- 주요 태스크: 고전 중국어-현대 중국어 번역 (99.55%)\n",
    "- 21개의 다양한 NLP 태스크 포함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": "# 필요한 라이브러리 임포트\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter, defaultdict\nimport re\nimport random\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 중국어/한자 지원을 위한 폰트 설정\nimport matplotlib.font_manager as fm\n\n# matplotlib 폰트 캐시 새로고침\nfm.fontManager.__init__()\n\n# 시스템에서 사용 가능한 모든 폰트 확인\navailable_fonts = [f.name for f in fm.fontManager.ttflist]\n\n# 한자를 지원할 수 있는 폰트 우선순위 설정\nfont_candidates = [\n    'NanumBarunGothic YetHangul',\n    'NanumMyeongjo YetHangul', \n    'Liberation Sans',\n    'DejaVu Sans',\n    'sans-serif'  # 시스템 기본 폰트\n]\n\n# 실제 사용할 폰트 선택\nselected_font = 'DejaVu Sans'  # 기본값\nfor font in font_candidates:\n    if font in available_fonts:\n        selected_font = font\n        break\n\n# 폰트 설정 - fallback 체인 설정으로 한자 지원 최대화\nplt.rcParams['font.family'] = font_candidates\nplt.rcParams['axes.unicode_minus'] = False\nplt.rcParams['font.size'] = 10\n\n# seaborn 테마 설정 (폰트 포함)\nsns.set_theme(style=\"whitegrid\", font=font_candidates[0] if font_candidates[0] in available_fonts else 'DejaVu Sans')\n\n# matplotlib 백엔드 설정으로 유니코드 지원 향상\nplt.rcParams['svg.fonttype'] = 'none'  # SVG에서 텍스트를 텍스트로 유지\n\n# 시드 설정\nrandom.seed(42)\nnp.random.seed(42)\n\nprint(f\"라이브러리 로딩 완료\")\nprint(f\"선택된 주 폰트: {selected_font}\")\nprint(f\"폰트 fallback 체인: {font_candidates[:3]}\")\nprint(\"한자 표시 테스트: 高等法院 翻译 现代文 古文 解释\")\n\n# 한자 지원 여부 간단 테스트\ntry:\n    test_fig, test_ax = plt.subplots(figsize=(4, 1))\n    test_ax.text(0.5, 0.5, '测试中文 Test Chinese', ha='center', va='center', fontsize=12)\n    test_ax.set_xlim(0, 1)\n    test_ax.set_ylim(0, 1)\n    test_ax.axis('off')\n    plt.close(test_fig)\n    print(\"✓ 한자 표시 테스트 통과\")\nexcept Exception as e:\n    print(f\"⚠ 한자 표시에 일부 제한이 있을 수 있음: {str(e)[:100]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. 데이터 로딩 및 기본 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩 (메모리 효율성을 위해 샘플링)\n",
    "def load_sample_data(file_path, sample_size=50000):\n",
    "    \"\"\"대용량 JSON 파일에서 샘플 데이터 로딩\"\"\"\n",
    "    print(f\"전체 데이터에서 {sample_size:,}개 샘플 로딩 중...\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        full_data = json.load(f)\n",
    "    \n",
    "    print(f\"전체 데이터 크기: {len(full_data):,}개\")\n",
    "    \n",
    "    # 랜덤 샘플링\n",
    "    if len(full_data) > sample_size:\n",
    "        sample_data = random.sample(full_data, sample_size)\n",
    "    else:\n",
    "        sample_data = full_data\n",
    "    \n",
    "    return full_data, sample_data\n",
    "\n",
    "# 데이터 로딩\n",
    "file_path = '/home/work/songhune/ACCN-INS.json'\n",
    "full_data, sample_data = load_sample_data(file_path, 50000)\n",
    "\n",
    "print(f\"분석용 샘플 데이터 크기: {len(sample_data):,}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 구조 분석\n",
    "print(\"=== 데이터 구조 분석 ===\")\n",
    "print(f\"첫 번째 레코드 키: {list(sample_data[0].keys())}\")\n",
    "print(f\"data 필드의 키: {list(sample_data[0]['data'].keys())}\")\n",
    "\n",
    "# 샘플 데이터 출력\n",
    "print(\"\\n=== 샘플 데이터 ===\")\n",
    "for i in range(3):\n",
    "    item = sample_data[i]\n",
    "    print(f\"\\n레코드 {i+1}:\")\n",
    "    print(f\"Task: {item['task']}\")\n",
    "    print(f\"Instruction: {item['data']['instruction'][:100]}...\")\n",
    "    print(f\"Output: {item['data']['output'][:100]}...\")\n",
    "    print(f\"History length: {len(item['data']['history'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. 태스크 분포 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터에서 태스크 분포 계산\n",
    "def analyze_task_distribution(data):\n",
    "    \"\"\"태스크 분포 분석\"\"\"\n",
    "    task_counts = Counter([item['task'] for item in data])\n",
    "    total_count = len(data)\n",
    "    \n",
    "    task_df = pd.DataFrame([\n",
    "        {\n",
    "            'task': task,\n",
    "            'count': count,\n",
    "            'percentage': count / total_count * 100\n",
    "        }\n",
    "        for task, count in task_counts.items()\n",
    "    ]).sort_values('count', ascending=False)\n",
    "    \n",
    "    return task_df\n",
    "\n",
    "# 전체 데이터 태스크 분포\n",
    "full_task_df = analyze_task_distribution(full_data)\n",
    "print(\"=== 전체 데이터 태스크 분포 ===\")\n",
    "print(full_task_df.to_string(index=False))\n",
    "\n",
    "# 샘플 데이터 태스크 분포\n",
    "sample_task_df = analyze_task_distribution(sample_data)\n",
    "print(f\"\\n=== 샘플 데이터 태스크 분포 (n={len(sample_data):,}) ===\")\n",
    "print(sample_task_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태스크 분포 시각화\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# 전체 분포 (상위 15개)\n",
    "top_15_tasks = full_task_df.head(15)\n",
    "bars1 = ax1.barh(range(len(top_15_tasks)), top_15_tasks['count'], \n",
    "                color=plt.cm.viridis(np.linspace(0, 1, len(top_15_tasks))))\n",
    "ax1.set_yticks(range(len(top_15_tasks)))\n",
    "ax1.set_yticklabels(top_15_tasks['task'], fontsize=10)\n",
    "ax1.set_xlabel('Count (log scale)')\n",
    "ax1.set_title('Full Dataset Task Distribution (Top 15)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 값 표시\n",
    "for i, (count, pct) in enumerate(zip(top_15_tasks['count'], top_15_tasks['percentage'])):\n",
    "    ax1.text(count, i, f'{count:,} ({pct:.2f}%)', \n",
    "            va='center', ha='left', fontsize=8)\n",
    "\n",
    "# 비주요 태스크 분포 (Classical Chinese to Modern Chinese 제외)\n",
    "minor_tasks = full_task_df[full_task_df['task'] != 'Classical Chinese to Modern Chinese'].head(10)\n",
    "bars2 = ax2.barh(range(len(minor_tasks)), minor_tasks['count'],\n",
    "                color=plt.cm.plasma(np.linspace(0, 1, len(minor_tasks))))\n",
    "ax2.set_yticks(range(len(minor_tasks)))\n",
    "ax2.set_yticklabels(minor_tasks['task'], fontsize=10)\n",
    "ax2.set_xlabel('Count')\n",
    "ax2.set_title('Minor Tasks Distribution (Excluding Main Task)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 값 표시\n",
    "for i, (count, pct) in enumerate(zip(minor_tasks['count'], minor_tasks['percentage'])):\n",
    "    ax2.text(count, i, f'{count:,}', va='center', ha='left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. 텍스트 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 길이 분석\n",
    "def analyze_text_lengths(data):\n",
    "    \"\"\"instruction과 output의 텍스트 길이 분석\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for item in data:\n",
    "        task = item['task']\n",
    "        instruction = item['data']['instruction']\n",
    "        output = item['data']['output']\n",
    "        input_text = item['data']['input']\n",
    "        history_len = len(item['data']['history'])\n",
    "        \n",
    "        results.append({\n",
    "            'task': task,\n",
    "            'instruction_len': len(instruction),\n",
    "            'output_len': len(output),\n",
    "            'input_len': len(input_text),\n",
    "            'history_count': history_len,\n",
    "            'has_input': len(input_text) > 0,\n",
    "            'has_history': history_len > 0\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 텍스트 길이 분석\n",
    "length_df = analyze_text_lengths(sample_data)\n",
    "\n",
    "print(\"=== 텍스트 길이 기본 통계 ===\")\n",
    "print(length_df[['instruction_len', 'output_len', 'input_len', 'history_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 길이 분포 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Instruction 길이 분포\n",
    "axes[0,0].hist(length_df['instruction_len'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].set_xlabel('Instruction Length (characters)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('Distribution of Instruction Length', fontweight='bold')\n",
    "axes[0,0].axvline(length_df['instruction_len'].mean(), color='red', linestyle='--', \n",
    "                 label=f'Mean: {length_df[\"instruction_len\"].mean():.0f}')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Output 길이 분포\n",
    "axes[0,1].hist(length_df['output_len'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0,1].set_xlabel('Output Length (characters)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Distribution of Output Length', fontweight='bold')\n",
    "axes[0,1].axvline(length_df['output_len'].mean(), color='red', linestyle='--',\n",
    "                 label=f'Mean: {length_df[\"output_len\"].mean():.0f}')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 태스크별 평균 길이\n",
    "task_length_stats = length_df.groupby('task')[['instruction_len', 'output_len']].mean().sort_values('output_len', ascending=False).head(10)\n",
    "x = range(len(task_length_stats))\n",
    "width = 0.35\n",
    "axes[1,0].bar([i - width/2 for i in x], task_length_stats['instruction_len'], \n",
    "             width, label='Instruction', alpha=0.8, color='skyblue')\n",
    "axes[1,0].bar([i + width/2 for i in x], task_length_stats['output_len'], \n",
    "             width, label='Output', alpha=0.8, color='lightgreen')\n",
    "axes[1,0].set_xlabel('Task')\n",
    "axes[1,0].set_ylabel('Average Length (characters)')\n",
    "axes[1,0].set_title('Average Text Length by Task (Top 10)', fontweight='bold')\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels([task.replace(' ', '\\n') for task in task_length_stats.index], \n",
    "                         rotation=45, ha='right', fontsize=8)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Input과 History 유무 분포\n",
    "has_input_count = length_df['has_input'].value_counts()\n",
    "has_history_count = length_df['has_history'].value_counts()\n",
    "\n",
    "categories = ['Has Input', 'Has History']\n",
    "yes_counts = [has_input_count.get(True, 0), has_history_count.get(True, 0)]\n",
    "no_counts = [has_input_count.get(False, 0), has_history_count.get(False, 0)]\n",
    "\n",
    "x = range(len(categories))\n",
    "axes[1,1].bar(x, yes_counts, label='Yes', alpha=0.8, color='lightcoral')\n",
    "axes[1,1].bar(x, no_counts, bottom=yes_counts, label='No', alpha=0.8, color='lightblue')\n",
    "axes[1,1].set_xlabel('Data Field')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_title('Distribution of Input and History Fields', fontweight='bold')\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels(categories)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 4. 고전 중국어-현대 중국어 번역 태스크 심층 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 태스크 (Classical Chinese to Modern Chinese) 분석\n",
    "main_task_data = [item for item in sample_data if item['task'] == 'Classical Chinese to Modern Chinese']\n",
    "print(f\"Classical Chinese to Modern Chinese 샘플 수: {len(main_task_data):,}개\")\n",
    "\n",
    "# 번역 패턴 분석\n",
    "def analyze_translation_patterns(data):\n",
    "    \"\"\"번역 데이터의 패턴 분석\"\"\"\n",
    "    patterns = {\n",
    "        'direct_request': 0,  # 직접적인 번역 요청\n",
    "        'context_request': 0,  # 맥락과 함께 번역 요청\n",
    "        'explanation_request': 0,  # 설명과 함께 번역 요청\n",
    "        'question_format': 0  # 질문 형식\n",
    "    }\n",
    "    \n",
    "    keywords = {\n",
    "        'direct_request': ['翻译', '译', '现代文'],\n",
    "        'context_request': ['帮我', '帮忙', '请'],\n",
    "        'explanation_request': ['解释', '说明', '意思'],\n",
    "        'question_format': ['吗', '呢', '？']\n",
    "    }\n",
    "    \n",
    "    for item in data:\n",
    "        instruction = item['data']['instruction']\n",
    "        \n",
    "        for pattern, words in keywords.items():\n",
    "            if any(word in instruction for word in words):\n",
    "                patterns[pattern] += 1\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "translation_patterns = analyze_translation_patterns(main_task_data)\n",
    "print(\"\\n=== 번역 요청 패턴 분석 ===\")\n",
    "for pattern, count in translation_patterns.items():\n",
    "    percentage = count / len(main_task_data) * 100\n",
    "    print(f\"{pattern}: {count:,}개 ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고전 중국어 텍스트 특성 분석\n",
    "def analyze_classical_chinese_features(data):\n",
    "    \"\"\"고전 중국어 텍스트의 특성 분석\"\"\"\n",
    "    features = {\n",
    "        'avg_sentence_length': [],\n",
    "        'punctuation_density': [],\n",
    "        'classical_markers': 0,\n",
    "        'historical_terms': 0,\n",
    "        'philosophical_terms': 0\n",
    "    }\n",
    "    \n",
    "    # 고전 중국어 특징 키워드\n",
    "    classical_markers = ['之', '也', '者', '而', '以', '其', '於', '乎', '哉', '矣']\n",
    "    historical_terms = ['朝', '帝', '王', '公', '侯', '卿', '大夫', '士', '民', '国']\n",
    "    philosophical_terms = ['仁', '义', '礼', '智', '信', '道', '德', '善', '恶', '天']\n",
    "    \n",
    "    for item in data[:1000]:  # 처리 속도를 위해 1000개 샘플만 분석\n",
    "        instruction = item['data']['instruction']\n",
    "        output = item['data']['output']\n",
    "        \n",
    "        # 고전 중국어 텍스트 추출 (instruction에서)\n",
    "        # 간단한 휴리스틱: 한자가 연속으로 나오는 부분\n",
    "        classical_text = re.findall(r'[\\u4e00-\\u9fff]+', instruction)\n",
    "        if classical_text:\n",
    "            text = ''.join(classical_text)\n",
    "            \n",
    "            # 문장 길이 (구두점 기준으로 분할)\n",
    "            sentences = re.split(r'[。！？；：]', text)\n",
    "            if sentences:\n",
    "                avg_len = sum(len(s) for s in sentences if s) / len([s for s in sentences if s])\n",
    "                features['avg_sentence_length'].append(avg_len)\n",
    "            \n",
    "            # 구두점 밀도\n",
    "            punctuation_count = len(re.findall(r'[。！？；：，]', text))\n",
    "            if len(text) > 0:\n",
    "                features['punctuation_density'].append(punctuation_count / len(text))\n",
    "            \n",
    "            # 특징 키워드 카운트\n",
    "            if any(marker in text for marker in classical_markers):\n",
    "                features['classical_markers'] += 1\n",
    "            if any(term in text for term in historical_terms):\n",
    "                features['historical_terms'] += 1\n",
    "            if any(term in text for term in philosophical_terms):\n",
    "                features['philosophical_terms'] += 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "classical_features = analyze_classical_chinese_features(main_task_data)\n",
    "print(\"\\n=== 고전 중국어 텍스트 특성 분석 ===\")\n",
    "if classical_features['avg_sentence_length']:\n",
    "    print(f\"평균 문장 길이: {np.mean(classical_features['avg_sentence_length']):.2f}자\")\n",
    "if classical_features['punctuation_density']:\n",
    "    print(f\"평균 구두점 밀도: {np.mean(classical_features['punctuation_density']):.4f}\")\n",
    "print(f\"고전 중국어 조사/어미 포함 비율: {classical_features['classical_markers']/1000*100:.1f}%\")\n",
    "print(f\"역사 용어 포함 비율: {classical_features['historical_terms']/1000*100:.1f}%\")\n",
    "print(f\"철학 용어 포함 비율: {classical_features['philosophical_terms']/1000*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 5. 다양한 태스크별 상세 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 태스크 유형별 샘플 분석\n",
    "def analyze_task_samples(data, task_name, max_samples=5):\n",
    "    \"\"\"특정 태스크의 샘플 분석\"\"\"\n",
    "    task_data = [item for item in data if item['task'] == task_name]\n",
    "    if not task_data:\n",
    "        return f\"'{task_name}' 태스크를 찾을 수 없습니다.\"\n",
    "    \n",
    "    print(f\"\\n=== {task_name} 태스크 분석 ===\")\n",
    "    print(f\"샘플 수: {len(task_data)}개\")\n",
    "    \n",
    "    # 텍스트 길이 통계\n",
    "    inst_lengths = [len(item['data']['instruction']) for item in task_data]\n",
    "    out_lengths = [len(item['data']['output']) for item in task_data]\n",
    "    \n",
    "    print(f\"평균 instruction 길이: {np.mean(inst_lengths):.1f}자\")\n",
    "    print(f\"평균 output 길이: {np.mean(out_lengths):.1f}자\")\n",
    "    \n",
    "    # 샘플 출력\n",
    "    print(f\"\\n--- 샘플 예시 (최대 {max_samples}개) ---\")\n",
    "    for i, item in enumerate(task_data[:max_samples]):\n",
    "        print(f\"\\n예시 {i+1}:\")\n",
    "        print(f\"Instruction: {item['data']['instruction'][:200]}{'...' if len(item['data']['instruction']) > 200 else ''}\")\n",
    "        print(f\"Output: {item['data']['output'][:200]}{'...' if len(item['data']['output']) > 200 else ''}\")\n",
    "        if item['data']['history']:\n",
    "            print(f\"History: {len(item['data']['history'])}개 대화\")\n",
    "\n",
    "# 주요 태스크들 분석\n",
    "major_tasks = ['Punctuation', 'Modern Chinese to Classical Chinese', 'Named Entity Recognition', 'Word Explanation']\n",
    "\n",
    "for task in major_tasks:\n",
    "    analyze_task_samples(sample_data, task, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 6. History 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History 데이터 분석\n",
    "def analyze_history_data(data):\n",
    "    \"\"\"대화 이력(history) 데이터 분석\"\"\"\n",
    "    history_stats = {\n",
    "        'has_history': 0,\n",
    "        'no_history': 0,\n",
    "        'history_lengths': [],\n",
    "        'total_turns': 0\n",
    "    }\n",
    "    \n",
    "    history_by_task = defaultdict(list)\n",
    "    \n",
    "    for item in data:\n",
    "        task = item['task']\n",
    "        history = item['data']['history']\n",
    "        \n",
    "        if history:\n",
    "            history_stats['has_history'] += 1\n",
    "            history_stats['history_lengths'].append(len(history))\n",
    "            history_stats['total_turns'] += len(history)\n",
    "            history_by_task[task].append(len(history))\n",
    "        else:\n",
    "            history_stats['no_history'] += 1\n",
    "            history_by_task[task].append(0)\n",
    "    \n",
    "    return history_stats, history_by_task\n",
    "\n",
    "history_stats, history_by_task = analyze_history_data(sample_data)\n",
    "\n",
    "print(\"=== 대화 이력(History) 분석 ===\")\n",
    "print(f\"History가 있는 레코드: {history_stats['has_history']:,}개 ({history_stats['has_history']/len(sample_data)*100:.2f}%)\")\n",
    "print(f\"History가 없는 레코드: {history_stats['no_history']:,}개 ({history_stats['no_history']/len(sample_data)*100:.2f}%)\")\n",
    "\n",
    "if history_stats['history_lengths']:\n",
    "    print(f\"평균 대화 턴 수: {np.mean(history_stats['history_lengths']):.2f}턴\")\n",
    "    print(f\"최대 대화 턴 수: {max(history_stats['history_lengths'])}턴\")\n",
    "    print(f\"총 대화 턴 수: {history_stats['total_turns']:,}턴\")\n",
    "\n",
    "# 태스크별 History 사용 비율\n",
    "print(\"\\n=== 태스크별 History 사용 비율 ===\")\n",
    "task_history_stats = []\n",
    "for task, lengths in history_by_task.items():\n",
    "    has_history = sum(1 for l in lengths if l > 0)\n",
    "    total = len(lengths)\n",
    "    avg_length = np.mean([l for l in lengths if l > 0]) if has_history > 0 else 0\n",
    "    \n",
    "    task_history_stats.append({\n",
    "        'task': task,\n",
    "        'total_samples': total,\n",
    "        'with_history': has_history,\n",
    "        'history_ratio': has_history / total * 100,\n",
    "        'avg_turns': avg_length\n",
    "    })\n",
    "\n",
    "task_history_df = pd.DataFrame(task_history_stats).sort_values('history_ratio', ascending=False)\n",
    "print(task_history_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History 데이터 시각화\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# History 유무 파이차트\n",
    "labels = ['Has History', 'No History']\n",
    "sizes = [history_stats['has_history'], history_stats['no_history']]\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "explode = (0.1, 0)  # History 있는 부분을 강조\n",
    "\n",
    "ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.set_title('Distribution of Records with History', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 태스크별 History 사용 비율 (상위 10개)\n",
    "top_history_tasks = task_history_df.head(10)\n",
    "bars = ax2.barh(range(len(top_history_tasks)), top_history_tasks['history_ratio'],\n",
    "               color=plt.cm.viridis(np.linspace(0, 1, len(top_history_tasks))))\n",
    "ax2.set_yticks(range(len(top_history_tasks)))\n",
    "ax2.set_yticklabels([task.replace(' ', '\\n') for task in top_history_tasks['task']], fontsize=9)\n",
    "ax2.set_xlabel('History Usage Ratio (%)')\n",
    "ax2.set_title('History Usage by Task (Top 10)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 값 표시\n",
    "for i, (ratio, count) in enumerate(zip(top_history_tasks['history_ratio'], top_history_tasks['with_history'])):\n",
    "    ax2.text(ratio, i, f'{ratio:.1f}% ({count})', va='center', ha='left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 7. 텍스트 패턴 및 키워드 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자주 사용되는 키워드 분석\n",
    "def extract_keywords_from_instructions(data, max_samples=5000):\n",
    "    \"\"\"instruction에서 키워드 추출 및 분석\"\"\"\n",
    "    import jieba  # 중국어 분할을 위한 라이브러리\n",
    "    \n",
    "    all_text = \"\"\n",
    "    sample_data = data[:max_samples] if len(data) > max_samples else data\n",
    "    \n",
    "    for item in sample_data:\n",
    "        instruction = item['data']['instruction']\n",
    "        all_text += instruction + \" \"\n",
    "    \n",
    "    # 중국어 텍스트 분할\n",
    "    words = jieba.lcut(all_text)\n",
    "    \n",
    "    # 길이가 1인 단어와 일반적인 불용어 제거\n",
    "    stop_words = {'的', '是', '在', '了', '和', '有', '个', '这', '那', '我', '你', '他', '她', '它', \n",
    "                  '我们', '你们', '他们', '一个', '一些', '什么', '怎么', '为什么', '哪里', '谁', '多少'}\n",
    "    \n",
    "    filtered_words = [word for word in words if len(word) > 1 and word not in stop_words]\n",
    "    \n",
    "    word_freq = Counter(filtered_words)\n",
    "    return word_freq\n",
    "\n",
    "try:\n",
    "    import jieba\n",
    "    \n",
    "    # 키워드 추출\n",
    "    keyword_freq = extract_keywords_from_instructions(sample_data, 3000)\n",
    "    \n",
    "    print(\"=== 상위 30개 키워드 ===\")\n",
    "    for word, freq in keyword_freq.most_common(30):\n",
    "        print(f\"{word}: {freq}회\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"jieba 라이브러리가 설치되어 있지 않습니다.\")\n",
    "    print(\"pip install jieba 로 설치할 수 있습니다.\")\n",
    "    \n",
    "    # 대안: 간단한 정규식 기반 키워드 추출\n",
    "    def simple_keyword_extraction(data, max_samples=3000):\n",
    "        common_phrases = []\n",
    "        sample_data = data[:max_samples] if len(data) > max_samples else data\n",
    "        \n",
    "        for item in sample_data:\n",
    "            instruction = item['data']['instruction']\n",
    "            common_phrases.extend(re.findall(r'[翻译|译成|现代文|古文|解释|说明|意思|帮我|请问]', instruction))\n",
    "        \n",
    "        return Counter(common_phrases)\n",
    "    \n",
    "    simple_keywords = simple_keyword_extraction(sample_data)\n",
    "    print(\"\\n=== 간단 키워드 분석 ===\")\n",
    "    for word, freq in simple_keywords.most_common(10):\n",
    "        print(f\"{word}: {freq}회\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명령어/요청 패턴 분석\n",
    "def analyze_instruction_patterns(data):\n",
    "    \"\"\"instruction의 명령어 패턴 분석\"\"\"\n",
    "    patterns = {\n",
    "        '翻译': 0,  # 번역\n",
    "        '解释': 0,  # 설명\n",
    "        '帮我': 0,  # 도와줘\n",
    "        '请': 0,   # 부탁\n",
    "        '什么': 0,  # 무엇\n",
    "        '怎么': 0,  # 어떻게\n",
    "        '为什么': 0, # 왜\n",
    "        '意思': 0,  # 의미\n",
    "        '现代文': 0, # 현대문\n",
    "        '古文': 0   # 고문\n",
    "    }\n",
    "    \n",
    "    for item in data:\n",
    "        instruction = item['data']['instruction']\n",
    "        for pattern in patterns.keys():\n",
    "            if pattern in instruction:\n",
    "                patterns[pattern] += 1\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "instruction_patterns = analyze_instruction_patterns(sample_data)\n",
    "\n",
    "print(\"\\n=== 명령어/요청 패턴 분석 ===\")\n",
    "pattern_df = pd.DataFrame([\n",
    "    {'pattern': pattern, 'count': count, 'percentage': count/len(sample_data)*100}\n",
    "    for pattern, count in instruction_patterns.items()\n",
    "]).sort_values('count', ascending=False)\n",
    "\n",
    "print(pattern_df.to_string(index=False))\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(pattern_df['pattern'], pattern_df['count'], \n",
    "               color=plt.cm.Set3(np.linspace(0, 1, len(pattern_df))))\n",
    "plt.xlabel('Pattern')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Instruction Pattern Frequency Analysis', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 값 표시\n",
    "for bar, count, pct in zip(bars, pattern_df['count'], pattern_df['percentage']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "             f'{count}\\n({pct:.1f}%)', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 8. 데이터 품질 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 품질 검사\n",
    "def analyze_data_quality(data):\n",
    "    \"\"\"데이터 품질 분석\"\"\"\n",
    "    quality_stats = {\n",
    "        'total_records': len(data),\n",
    "        'empty_instruction': 0,\n",
    "        'empty_output': 0,\n",
    "        'very_short_instruction': 0,  # < 5자\n",
    "        'very_short_output': 0,       # < 5자\n",
    "        'very_long_instruction': 0,   # > 1000자\n",
    "        'very_long_output': 0,        # > 1000자\n",
    "        'duplicate_instructions': 0,\n",
    "        'duplicate_outputs': 0,\n",
    "        'identical_input_output': 0\n",
    "    }\n",
    "    \n",
    "    instructions = []\n",
    "    outputs = []\n",
    "    \n",
    "    for item in data:\n",
    "        instruction = item['data']['instruction']\n",
    "        output = item['data']['output']\n",
    "        \n",
    "        instructions.append(instruction)\n",
    "        outputs.append(output)\n",
    "        \n",
    "        # 빈 데이터 검사\n",
    "        if not instruction.strip():\n",
    "            quality_stats['empty_instruction'] += 1\n",
    "        if not output.strip():\n",
    "            quality_stats['empty_output'] += 1\n",
    "        \n",
    "        # 길이 검사\n",
    "        if len(instruction) < 5:\n",
    "            quality_stats['very_short_instruction'] += 1\n",
    "        if len(output) < 5:\n",
    "            quality_stats['very_short_output'] += 1\n",
    "        if len(instruction) > 1000:\n",
    "            quality_stats['very_long_instruction'] += 1\n",
    "        if len(output) > 1000:\n",
    "            quality_stats['very_long_output'] += 1\n",
    "        \n",
    "        # 동일 입출력 검사\n",
    "        if instruction.strip() == output.strip():\n",
    "            quality_stats['identical_input_output'] += 1\n",
    "    \n",
    "    # 중복 검사\n",
    "    instruction_counter = Counter(instructions)\n",
    "    output_counter = Counter(outputs)\n",
    "    \n",
    "    quality_stats['duplicate_instructions'] = sum(1 for count in instruction_counter.values() if count > 1)\n",
    "    quality_stats['duplicate_outputs'] = sum(1 for count in output_counter.values() if count > 1)\n",
    "    \n",
    "    return quality_stats, instruction_counter, output_counter\n",
    "\n",
    "quality_stats, inst_counter, out_counter = analyze_data_quality(sample_data)\n",
    "\n",
    "print(\"=== 데이터 품질 분석 ===\")\n",
    "for metric, value in quality_stats.items():\n",
    "    if metric != 'total_records':\n",
    "        percentage = value / quality_stats['total_records'] * 100\n",
    "        print(f\"{metric}: {value:,}개 ({percentage:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value:,}개\")\n",
    "\n",
    "# 가장 자주 나타나는 중복 데이터\n",
    "print(\"\\n=== 가장 빈번한 중복 instruction (상위 5개) ===\")\n",
    "for instruction, count in inst_counter.most_common(5):\n",
    "    if count > 1:\n",
    "        print(f\"출현 {count}회: {instruction[:100]}{'...' if len(instruction) > 100 else ''}\")\n",
    "\n",
    "print(\"\\n=== 가장 빈번한 중복 output (상위 5개) ===\")\n",
    "for output, count in out_counter.most_common(5):\n",
    "    if count > 1:\n",
    "        print(f\"출현 {count}회: {output[:100]}{'...' if len(output) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 품질 시각화\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 품질 이슈 분포\n",
    "quality_issues = {\n",
    "    'Empty Instruction': quality_stats['empty_instruction'],\n",
    "    'Empty Output': quality_stats['empty_output'],\n",
    "    'Very Short Inst': quality_stats['very_short_instruction'],\n",
    "    'Very Short Out': quality_stats['very_short_output'],\n",
    "    'Very Long Inst': quality_stats['very_long_instruction'],\n",
    "    'Very Long Out': quality_stats['very_long_output'],\n",
    "    'Identical I/O': quality_stats['identical_input_output']\n",
    "}\n",
    "\n",
    "bars1 = ax1.bar(quality_issues.keys(), quality_issues.values(), \n",
    "               color=plt.cm.Reds(np.linspace(0.3, 0.9, len(quality_issues))))\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Data Quality Issues', fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, value in zip(bars1, quality_issues.values()):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "             str(value), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 중복 분포\n",
    "duplicate_stats = {\n",
    "    'Unique Instructions': len(inst_counter) - quality_stats['duplicate_instructions'],\n",
    "    'Duplicate Instructions': quality_stats['duplicate_instructions'],\n",
    "    'Unique Outputs': len(out_counter) - quality_stats['duplicate_outputs'],\n",
    "    'Duplicate Outputs': quality_stats['duplicate_outputs']\n",
    "}\n",
    "\n",
    "categories = ['Instructions', 'Outputs']\n",
    "unique_counts = [duplicate_stats['Unique Instructions'], duplicate_stats['Unique Outputs']]\n",
    "duplicate_counts = [duplicate_stats['Duplicate Instructions'], duplicate_stats['Duplicate Outputs']]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "bars2_1 = ax2.bar(x - width/2, unique_counts, width, label='Unique', alpha=0.8, color='lightgreen')\n",
    "bars2_2 = ax2.bar(x + width/2, duplicate_counts, width, label='Duplicate', alpha=0.8, color='lightcoral')\n",
    "\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Unique vs Duplicate Content', fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(categories)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 텍스트 길이 분포 (정상 범위)\n",
    "normal_inst_lengths = [len(item['data']['instruction']) for item in sample_data \n",
    "                      if 5 <= len(item['data']['instruction']) <= 1000]\n",
    "normal_out_lengths = [len(item['data']['output']) for item in sample_data \n",
    "                     if 5 <= len(item['data']['output']) <= 1000]\n",
    "\n",
    "ax3.hist(normal_inst_lengths, bins=50, alpha=0.7, label='Instruction', color='skyblue')\n",
    "ax3.hist(normal_out_lengths, bins=50, alpha=0.7, label='Output', color='lightgreen')\n",
    "ax3.set_xlabel('Text Length (characters)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Text Length Distribution (Normal Range: 5-1000 chars)', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 태스크별 품질 점수\n",
    "task_quality_scores = []\n",
    "for task in sample_task_df['task'].head(8):  # 상위 8개 태스크\n",
    "    task_data = [item for item in sample_data if item['task'] == task]\n",
    "    if task_data:\n",
    "        # 간단한 품질 점수: 적절한 길이 + 중복 없음 + 빈 데이터 없음\n",
    "        good_length = sum(1 for item in task_data \n",
    "                         if 5 <= len(item['data']['instruction']) <= 1000 and \n",
    "                            5 <= len(item['data']['output']) <= 1000)\n",
    "        quality_score = good_length / len(task_data) * 100\n",
    "        task_quality_scores.append((task, quality_score))\n",
    "\n",
    "task_quality_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "tasks, scores = zip(*task_quality_scores)\n",
    "\n",
    "bars4 = ax4.barh(range(len(tasks)), scores, \n",
    "                color=plt.cm.viridis(np.linspace(0, 1, len(tasks))))\n",
    "ax4.set_yticks(range(len(tasks)))\n",
    "ax4.set_yticklabels([task.replace(' ', '\\n') for task in tasks], fontsize=9)\n",
    "ax4.set_xlabel('Quality Score (%)')\n",
    "ax4.set_title('Quality Score by Task (Good Length Ratio)', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    ax4.text(score, i, f'{score:.1f}%', va='center', ha='left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 9. 종합 분석 및 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 분석 요약\n",
    "print(\"=== ACCN-INS 데이터셋 종합 분석 결과 ===\")\n",
    "print(f\"\"\"\\n1. 데이터셋 규모:\n",
    "   - 총 레코드 수: {len(full_data):,}개\n",
    "   - 분석 샘플 수: {len(sample_data):,}개\n",
    "   - 총 태스크 유형: {len(full_task_df)}개\n",
    "\n",
    "2. 태스크 분포:\n",
    "   - 주요 태스크: Classical Chinese to Modern Chinese ({full_task_df.iloc[0]['percentage']:.2f}%)\n",
    "   - 보조 태스크: {len(full_task_df)-1}개 (총 {100-full_task_df.iloc[0]['percentage']:.2f}%)\n",
    "\n",
    "3. 텍스트 특성:\n",
    "   - 평균 instruction 길이: {length_df['instruction_len'].mean():.1f}자\n",
    "   - 평균 output 길이: {length_df['output_len'].mean():.1f}자\n",
    "   - History 포함 비율: {history_stats['has_history']/len(sample_data)*100:.2f}%\n",
    "\n",
    "4. 데이터 품질:\n",
    "   - 빈 instruction: {quality_stats['empty_instruction']/quality_stats['total_records']*100:.3f}%\n",
    "   - 빈 output: {quality_stats['empty_output']/quality_stats['total_records']*100:.3f}%\n",
    "   - 매우 짧은 텍스트: {(quality_stats['very_short_instruction']+quality_stats['very_short_output'])/quality_stats['total_records']/2*100:.3f}%\n",
    "   - 동일 입출력: {quality_stats['identical_input_output']/quality_stats['total_records']*100:.3f}%\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n5. 주요 발견사항:\")\n",
    "print(\"   - 데이터셋은 고전 중국어-현대 중국어 번역에 특화됨\")\n",
    "print(\"   - 대부분의 레코드가 대화 이력을 포함하여 멀티턴 대화 형태\")\n",
    "print(\"   - 다양한 NLP 태스크가 소수 포함되어 있어 멀티태스크 학습에 활용 가능\")\n",
    "print(\"   - 전반적으로 높은 데이터 품질을 유지\")\n",
    "\n",
    "print(\"\\n6. 활용 방안:\")\n",
    "print(\"   - 고전 중국어 번역 모델 훈련\")\n",
    "print(\"   - 중국 고전 문헌 이해 및 해석 시스템 개발\")\n",
    "print(\"   - 다중 태스크 중국어 NLP 모델 훈련\")\n",
    "print(\"   - 대화형 번역 시스템 구축\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 통계 테이블\n",
    "summary_stats = {\n",
    "    '항목': ['전체 레코드 수', '주요 태스크 비율', '평균 instruction 길이', '평균 output 길이', \n",
    "            'History 포함 비율', '빈 데이터 비율', '품질 이슈 비율'],\n",
    "    '값': [\n",
    "        f\"{len(full_data):,}개\",\n",
    "        f\"{full_task_df.iloc[0]['percentage']:.2f}%\",\n",
    "        f\"{length_df['instruction_len'].mean():.1f}자\",\n",
    "        f\"{length_df['output_len'].mean():.1f}자\",\n",
    "        f\"{history_stats['has_history']/len(sample_data)*100:.2f}%\",\n",
    "        f\"{(quality_stats['empty_instruction']+quality_stats['empty_output'])/quality_stats['total_records']/2*100:.3f}%\",\n",
    "        f\"{sum(v for k, v in quality_stats.items() if k not in ['total_records'])/quality_stats['total_records']*100:.2f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "print(\"\\n=== 핵심 통계 요약 ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== EDA 완료 ===\")\n",
    "print(\"이 노트북은 ACCN-INS 데이터셋의 포괄적인 탐색적 데이터 분석을 제공합니다.\")\n",
    "print(\"추가적인 분석이나 특정 태스크에 대한 심화 분석이 필요한 경우 해당 섹션을 확장하여 사용할 수 있습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}