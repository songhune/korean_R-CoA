# KLSBench 전문가 평가 계획

## 목표

- 벤치마크 품질 검증 (라벨 정확도, 태스크 타당성)
- 학술적 신뢰도 확보
- 개선점 파악

## 평가 대상

**고려대학교 한문학과 연구원** (내정)

## 평가 방법

### 샘플 데이터 추출
- Classification: 80개 (10%)
- Retrieval: 120개 (10%)
- Punctuation: 200개 (10%)
- NLI: 180개 (10%)
- Translation: 200개 (10%)

**총 780개 항목**

### 평가 항목
1. 라벨/정답 정확성 (맞다/틀리다)
2. 난이도 평가 (1-5점)
3. 오류 지적 및 수정 제안
4. 전반적 품질 평가

### 평가 방식
- Excel 또는 간단한 웹 폼
- 소요 시간: 2-3시간

## 데이터 파일 위치

### 전체 벤치마크 데이터
```
benchmark/kls_bench/
├── kls_bench_full.json              (전체 7,871개 항목)
├── kls_bench_classification.json    (Classification: 808개)
├── kls_bench_retrieval.json         (Retrieval: 1,209개)
├── kls_bench_punctuation.json       (Punctuation: 2,000개)
├── kls_bench_nli.json               (NLI: 1,854개)
└── kls_bench_translation.json       (Translation: 2,000개)
```

### 샘플 추출 방법
```bash
# 10% 샘플 추출 (780개)
cd notebook/experiments
python3 utils/kls_bench_generator.py --sample 0.1 --output expert_evaluation_sample.json
```

또는 Python 코드:
```python
import json
import random

# 전체 데이터 로드
with open('../../benchmark/kls_bench/kls_bench_full.json') as f:
    data = json.load(f)

# 태스크별 10% 샘플링
sampled = {}
for task, items in data.items():
    sample_size = int(len(items) * 0.1)
    sampled[task] = random.sample(items, sample_size)

# 저장
with open('expert_evaluation_sample.json', 'w', encoding='utf-8') as f:
    json.dump(sampled, f, ensure_ascii=False, indent=2)
```

## 준비 자료

### 필수
1. **연구 소개서** (2-3페이지)
   - 연구 목적
   - 벤치마크 구성
   - 평가 필요성

2. **샘플 데이터** (Excel)
   - 항목 ID, 입력, 현재 라벨, 평가란
   - 소스: `expert_evaluation_sample.json` (위에서 생성)

3. **평가 가이드** (1-2페이지)
   - 평가 방법
   - 예시
   - 제출 방법

### 선택
- 동의서 (간단히)
- 질문지

## 분석

### 정량
- 라벨 정확도: 일치율 계산
- 난이도 평균

### 정성
- 오류 패턴 분석
- 개선 제안 정리

## 타임라인

1. **1주**: 자료 준비 (샘플 추출, 문서 작성)
2. **2주**: 평가 진행
3. **3주**: 결과 분석 및 수정

## 결과 활용

- 벤치마크 수정 (오류 수정)
- 논문 Acknowledgments
- 검증 데이터로 활용

---

**참고**: 이미 연구원이 내정되어 있으므로 복잡한 모집 과정은 불필요
