"""
Experiment 6: K-ClassicBench Result Aggregation and Visualization
ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ê²°ê³¼ë¥¼ í†µí•©í•˜ê³  ì‹œê°í™”í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ê¸°ëŠ¥:
1. ëª¨ë“  í‰ê°€ ê²°ê³¼ JSON/CSV íŒŒì¼ì„ ë¡œë“œ
2. ìµœì‹  ê²°ê³¼ë§Œ ì„ íƒ (ê°™ì€ ëª¨ë¸ì˜ ì—¬ëŸ¬ ì‹¤í–‰ ì¤‘)
3. ê²°ê³¼ë¥¼ í†µí•© í…Œì´ë¸”ë¡œ ì •ë¦¬
4. ì‹œê°í™” ìƒì„±:
   - íƒœìŠ¤í¬ë³„ ì„±ëŠ¥ ë¹„êµ (íˆíŠ¸ë§µ)
   - ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ (ë°” ì°¨íŠ¸)
   - ë ˆì´ë” ì°¨íŠ¸
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List
from collections import defaultdict
import argparse
import sys

CURRENT_DIR = Path(__file__).resolve().parent
UTILS_DIR = CURRENT_DIR.parent / "utils"
if str(UTILS_DIR) not in sys.path:
    sys.path.append(str(UTILS_DIR))

try:
    from font_fix import setup_korean_fonts_robust
except ImportError:
    setup_korean_fonts_robust = None

A4_WIDTH_INCH = 8.27
A4_HEIGHT_INCH = 11.69
PROJECT_ROOT = Path(__file__).resolve().parents[3]
DEFAULT_RESULTS_DIR = PROJECT_ROOT / "results" / "raw_evaluation"
DEFAULT_OUTPUT_DIR = PROJECT_ROOT / "results" / "aggregated"


def configure_matplotlib():
    """Ensure consistent typography and sizing for publication-ready figures."""
    if setup_korean_fonts_robust:
        setup_korean_fonts_robust()
    else:
        plt.rcParams['font.family'] = 'AppleGothic'
        plt.rcParams['axes.unicode_minus'] = False

    plt.rcParams.update({
        'figure.figsize': (A4_WIDTH_INCH, A4_HEIGHT_INCH * 0.6),
        'savefig.dpi': 300,
        'font.size': 11,
        'axes.titlesize': 16,
        'axes.labelsize': 13,
        'xtick.labelsize': 11,
        'ytick.labelsize': 11,
        'legend.fontsize': 11,
    })


class ResultAggregator:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í†µí•© í´ë˜ìŠ¤"""

    def __init__(self, results_dir: str):
        self.results_dir = Path(results_dir)
        self.results = []
        self.df = None

    def load_all_results(self):
        """ëª¨ë“  ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        print("ğŸ“‚ ê²°ê³¼ íŒŒì¼ ë¡œë”© ì¤‘...")

        # JSON íŒŒì¼ ì°¾ê¸°
        json_files = list(self.results_dir.glob("results_*.json"))
        print(f"  âœ“ ì´ {len(json_files)}ê°œ ê²°ê³¼ íŒŒì¼ ë°œê²¬")

        # ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í™” (ìµœì‹  ê²°ê³¼ë§Œ ì„ íƒ)
        model_results = defaultdict(list)

        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                    model_name = result['model_name']
                    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
                    timestamp = json_file.stem.split('_')[-2:]
                    timestamp_str = '_'.join(timestamp)
                    result['timestamp'] = timestamp_str
                    result['file_path'] = str(json_file)
                    model_results[model_name].append(result)
            except Exception as e:
                print(f"    íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {json_file.name} - {e}")

        # ê° ëª¨ë¸ì˜ ìµœì‹  ê²°ê³¼ë§Œ ì„ íƒ
        print(f"\n ëª¨ë¸ë³„ ìµœì‹  ê²°ê³¼ ì„ íƒ:")
        for model_name, results in model_results.items():
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ ì •ë ¬
            results.sort(key=lambda x: x['timestamp'], reverse=True)
            latest = results[0]
            self.results.append(latest)
            print(f"  âœ“ {model_name}: {latest['timestamp']}")

        print(f"\n ì´ {len(self.results)}ê°œ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")

    def create_summary_table(self) -> pd.DataFrame:
        """ê²°ê³¼ë¥¼ ìš”ì•½ í…Œì´ë¸”ë¡œ ë³€í™˜"""
        print("\n ìš”ì•½ í…Œì´ë¸” ìƒì„± ì¤‘...")

        rows = []
        for result in self.results:
            model_name = result['model_name']
            model_type = result.get('model_type', 'unknown')

            # ê° íƒœìŠ¤í¬ì˜ ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
            for task_name, task_result in result['tasks'].items():
                metrics = task_result['metrics']

                row = {
                    'model': model_name,
                    'model_type': model_type,
                    'task': task_name,
                }

                # íƒœìŠ¤í¬ë³„ ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ê°€
                if task_name == 'classification':
                    row['accuracy'] = metrics.get('accuracy', 0)
                    row['f1'] = metrics.get('f1', 0)
                    row['primary_metric'] = metrics.get('accuracy', 0)

                elif task_name == 'retrieval':
                    row['accuracy'] = metrics.get('accuracy', 0)
                    row['primary_metric'] = metrics.get('accuracy', 0)

                elif task_name == 'punctuation':
                    row['char_f1'] = metrics.get('char_f1', 0)
                    row['rougeL_f1'] = metrics.get('rougeL_f1', 0)
                    row['primary_metric'] = metrics.get('char_f1', 0)

                elif task_name == 'nli':
                    row['accuracy'] = metrics.get('accuracy', 0)
                    row['f1'] = metrics.get('f1', 0)
                    row['primary_metric'] = metrics.get('accuracy', 0)

                elif task_name == 'translation':
                    row['bleu'] = metrics.get('bleu', 0)
                    row['rougeL_f1'] = metrics.get('rougeL_f1', 0)
                    row['primary_metric'] = metrics.get('bleu', 0)

                rows.append(row)

        self.df = pd.DataFrame(rows)
        print(f"  âœ“ {len(self.df)}ê°œ í–‰ ìƒì„± ì™„ë£Œ")

        return self.df

    def save_aggregated_results(self, output_dir: str):
        """í†µí•© ê²°ê³¼ ì €ì¥"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # 1. ì „ì²´ ìš”ì•½ í…Œì´ë¸”
        summary_path = output_dir / "aggregated_summary.csv"
        self.df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        print(f"\n ìš”ì•½ í…Œì´ë¸” ì €ì¥: {summary_path}")

        # 2. í”¼ë²— í…Œì´ë¸” (ëª¨ë¸ Ã— íƒœìŠ¤í¬)
        pivot = self.df.pivot_table(
            index='model',
            columns='task',
            values='primary_metric',
            aggfunc='first'
        )
        pivot_path = output_dir / "aggregated_pivot.csv"
        pivot.to_csv(pivot_path, encoding='utf-8-sig')
        print(f" í”¼ë²— í…Œì´ë¸” ì €ì¥: {pivot_path}")

        # 3. ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
        model_avg = self.df.groupby('model')['primary_metric'].mean().sort_values(ascending=False)
        model_avg_path = output_dir / "model_average_performance.csv"
        model_avg.to_csv(model_avg_path, header=['average_score'], encoding='utf-8-sig')
        print(f" ëª¨ë¸ í‰ê·  ì„±ëŠ¥: {model_avg_path}")

        return pivot

    def visualize_results(self, output_dir: str):
        """ê²°ê³¼ ì‹œê°í™”"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        print("\n ì‹œê°í™” ìƒì„± ì¤‘...")

        configure_matplotlib()

        # 1. íˆíŠ¸ë§µ: ëª¨ë¸ Ã— íƒœìŠ¤í¬ ì„±ëŠ¥
        self._create_heatmap(output_dir)

        # 2. ë°” ì°¨íŠ¸: ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
        self._create_bar_chart(output_dir)

        # 3. íƒœìŠ¤í¬ë³„ ì„±ëŠ¥ ë¹„êµ (ê·¸ë£¹ ë°” ì°¨íŠ¸)
        self._create_grouped_bar_chart(output_dir)

        # 4. ë ˆì´ë” ì°¨íŠ¸
        self._create_radar_chart(output_dir)

        print(f" ì‹œê°í™” ì™„ë£Œ: {output_dir}")

    def _create_heatmap(self, output_dir: Path):
        """íˆíŠ¸ë§µ ìƒì„±"""
        pivot = self.df.pivot_table(
            index='model',
            columns='task',
            values='primary_metric',
            aggfunc='first'
        )

        fig, ax = plt.subplots(figsize=(A4_WIDTH_INCH, A4_HEIGHT_INCH * 0.7))
        sns.heatmap(
            pivot,
            annot=True,
            fmt='.3f',
            cmap='YlOrRd',
            cbar_kws={'label': 'Score'},
            annot_kws={'fontsize': 12},
            ax=ax
        )
        ax.set_title('K-ClassicBench: Model Performance Heatmap', fontsize=18, fontweight='bold')
        ax.set_xlabel('Task')
        ax.set_ylabel('Model')
        ax.tick_params(axis='x', rotation=45, labelsize=11)
        ax.tick_params(axis='y', labelsize=11)
        fig.tight_layout()

        heatmap_path = output_dir / 'heatmap_performance.pdf'
        fig.savefig(heatmap_path, format='pdf', bbox_inches='tight')
        plt.close(fig)
        print(f"  âœ“ íˆíŠ¸ë§µ: {heatmap_path}")

    def _create_bar_chart(self, output_dir: Path):
        """ë°” ì°¨íŠ¸: ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥"""
        model_avg = self.df.groupby('model')['primary_metric'].mean().sort_values(ascending=True)

        fig, ax = plt.subplots(figsize=(A4_WIDTH_INCH, A4_HEIGHT_INCH * 0.65))
        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(model_avg)))
        model_avg.plot(kind='barh', color=colors, ax=ax)
        ax.set_title('K-ClassicBench: Average Model Performance', fontsize=18, fontweight='bold')
        ax.set_xlabel('Average Score')
        ax.set_ylabel('Model')
        ax.grid(axis='x', alpha=0.3)
        fig.tight_layout()

        bar_path = output_dir / 'bar_average_performance.pdf'
        fig.savefig(bar_path, format='pdf', bbox_inches='tight')
        plt.close(fig)
        print(f"  âœ“ ë°” ì°¨íŠ¸: {bar_path}")

    def _create_grouped_bar_chart(self, output_dir: Path):
        """ê·¸ë£¹ ë°” ì°¨íŠ¸: íƒœìŠ¤í¬ë³„ ëª¨ë¸ ì„±ëŠ¥"""
        pivot = self.df.pivot_table(
            index='model',
            columns='task',
            values='primary_metric',
            aggfunc='first'
        )

        fig, ax = plt.subplots(figsize=(A4_WIDTH_INCH * 1.05, A4_HEIGHT_INCH * 0.75))
        pivot.plot(kind='bar', ax=ax, width=0.8)
        ax.set_title('K-ClassicBench: Task-wise Model Performance', fontsize=18, fontweight='bold')
        ax.set_xlabel('Model')
        ax.set_ylabel('Score')
        ax.legend(title='Task', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)
        ax.tick_params(axis='x', rotation=45, labelsize=11)
        ax.tick_params(axis='y', labelsize=11)
        ax.grid(axis='y', alpha=0.3)
        fig.tight_layout()

        grouped_bar_path = output_dir / 'grouped_bar_taskwise.pdf'
        fig.savefig(grouped_bar_path, format='pdf', bbox_inches='tight')
        plt.close(fig)
        print(f"  âœ“ ê·¸ë£¹ ë°” ì°¨íŠ¸: {grouped_bar_path}")

    def _create_radar_chart(self, output_dir: Path):
        """ë ˆì´ë” ì°¨íŠ¸: ëª¨ë¸ë³„ íƒœìŠ¤í¬ ì„±ëŠ¥"""
        pivot = self.df.pivot_table(
            index='model',
            columns='task',
            values='primary_metric',
            aggfunc='first'
        ).fillna(0)

        # íƒœìŠ¤í¬ ìˆ˜
        categories = list(pivot.columns)
        N = len(categories)

        # ê°ë„ ê³„ì‚°
        angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
        angles += angles[:1]  # ì›ì„ ë‹«ê¸° ìœ„í•´

        fig, ax = plt.subplots(figsize=(A4_WIDTH_INCH * 0.9, A4_HEIGHT_INCH * 0.9), subplot_kw=dict(projection='polar'))

        colors = plt.cm.Set2(np.linspace(0, 1, len(pivot)))

        for idx, (model, row) in enumerate(pivot.iterrows()):
            values = row.tolist()
            values += values[:1]  # ì›ì„ ë‹«ê¸° ìœ„í•´

            ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[idx])
            ax.fill(angles, values, alpha=0.15, color=colors[idx])

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, size=14)
        ax.set_ylim(0, 1)
        ax.set_title('K-ClassicBench: Radar Chart - Model Performance',
                     size=18, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)
        ax.grid(True)

        fig.tight_layout()
        radar_path = output_dir / 'radar_chart.pdf'
        fig.savefig(radar_path, format='pdf', bbox_inches='tight')
        plt.close(fig)
        print(f"  âœ“ ë ˆì´ë” ì°¨íŠ¸: {radar_path}")

    def print_summary_statistics(self):
        """ìš”ì•½ í†µê³„ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print(" K-ClassicBench í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("=" * 70)

        # 1. ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
        print("\nğŸ† ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥:")
        model_avg = self.df.groupby('model')['primary_metric'].mean().sort_values(ascending=False)
        for rank, (model, score) in enumerate(model_avg.items(), 1):
            print(f"  {rank}. {model:50s} {score:.4f}")

        # 2. íƒœìŠ¤í¬ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        print("\n íƒœìŠ¤í¬ë³„ ìµœê³  ì„±ëŠ¥:")
        for task in self.df['task'].unique():
            task_df = self.df[self.df['task'] == task]
            best_row = task_df.loc[task_df['primary_metric'].idxmax()]
            print(f"  - {task:15s}: {best_row['model']:40s} ({best_row['primary_metric']:.4f})")

        # 3. ëª¨ë¸ íƒ€ì…ë³„ í‰ê·  ì„±ëŠ¥
        print("\n ëª¨ë¸ íƒ€ì…ë³„ í‰ê·  ì„±ëŠ¥:")
        type_avg = self.df.groupby('model_type')['primary_metric'].mean().sort_values(ascending=False)
        for model_type, score in type_avg.items():
            print(f"  - {model_type:15s}: {score:.4f}")

        print("\n" + "=" * 70)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='K-ClassicBench Result Aggregation')
    parser.add_argument('--results-dir', type=str,
                       default=str(DEFAULT_RESULTS_DIR),
                       help='ê²°ê³¼ íŒŒì¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output-dir', type=str,
                       default=str(DEFAULT_OUTPUT_DIR),
                       help='í†µí•© ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')

    args = parser.parse_args()

    # ê²°ê³¼ í†µí•©
    aggregator = ResultAggregator(args.results_dir)
    aggregator.load_all_results()
    aggregator.create_summary_table()

    # ê²°ê³¼ ì €ì¥
    aggregator.save_aggregated_results(args.output_dir)

    # ì‹œê°í™”
    aggregator.visualize_results(args.output_dir)

    # í†µê³„ ì¶œë ¥
    aggregator.print_summary_statistics()

    print("\n ê²°ê³¼ í†µí•© ë° ì‹œê°í™” ì™„ë£Œ!")


if __name__ == "__main__":
    main()
