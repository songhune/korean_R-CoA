# R-CoA Memory Optimization Guide

H100 GPUì—ì„œ ëŒ€í˜• ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ ìµœì í™” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸš€ ì ìš©ëœ ìµœì í™” ê¸°ë²•

### 1. **Automatic Mixed Precision (AMP)**

**BF16 (Brain Float 16) ì‚¬ìš©:**
```python
# BF16ì€ H100ì— ìµœì í™”ë˜ì–´ ìˆìŒ
with torch.cuda.amp.autocast(dtype=torch.bfloat16):
    loss = model(batch)
```

**íš¨ê³¼:**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: **~50% ê°ì†Œ**
- í•™ìŠµ ì†ë„: **~2ë°° í–¥ìƒ**
- ìˆ˜ì¹˜ ì•ˆì •ì„±: FP16ë³´ë‹¤ ìš°ìˆ˜ (ë” ë„“ì€ dynamic range)

### 2. **Gradient Checkpointing**

```python
model.encoder.gradient_checkpointing_enable()
```

**íš¨ê³¼:**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: **~40% ê°ì†Œ**
- í•™ìŠµ ì†ë„: ~20% ëŠë ¤ì§ (trade-off)
- Activationì„ ì €ì¥í•˜ì§€ ì•Šê³  í•„ìš” ì‹œ ì¬ê³„ì‚°

### 3. **Gradient Accumulation**

```bash
--gradient-accumulation-steps 2
```

**íš¨ê³¼:**
- ì‹¤ì§ˆì ì¸ batch size: `batch_size Ã— accumulation_steps`
- ë©”ëª¨ë¦¬: ì‘ì€ batchë¡œ ì‹¤í–‰í•˜ë©´ì„œ í° batch íš¨ê³¼
- ì˜ˆ: batch_size=64, accumulation=2 â†’ effective batch=128

### 4. **LoRA (Low-Rank Adaptation)**

```python
# ì „ì²´ ëª¨ë¸ì´ ì•„ë‹Œ ì†Œìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
lora_config = LoraConfig(r=32, alpha=64)
```

**íš¨ê³¼:**
- í•™ìŠµ íŒŒë¼ë¯¸í„°: **~1% of total**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: **ëŒ€í­ ê°ì†Œ**
- í•™ìŠµ ì†ë„: ë¹ ë¦„

---

## ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

| ëª¨ë¸ | ê¸°ë³¸ (FP32) | + AMP | + Checkpointing | + LoRA | ìµœì¢… |
|------|------------|-------|-----------------|--------|------|
| **Large (550M)** | ~12GB | ~6GB | ~4GB | ~3GB | **~3GB** |
| **XL (3.5B)** | ~42GB | ~21GB | ~13GB | ~10GB | **~10GB** |
| **XXL (10.7B)** | ~128GB | ~64GB | ~38GB | ~28GB | **~28GB** |

---

## ğŸ¯ ìµœì  ì„¤ì • (H100 79GB)

### XLM-RoBERTa-Large (550M)
```bash
bash scripts/run_poc_h100.sh large

# ë©”ëª¨ë¦¬: ~3GB
# Batch Size: 256
# ì˜ˆìƒ ì‹œê°„: 2-3ì‹œê°„
```

### XLM-RoBERTa-XL (3.5B) â­ ì¶”ì²œ
```bash
bash scripts/run_poc_h100.sh xl

# ë©”ëª¨ë¦¬: ~10GB
# Batch Size: 192
# ì˜ˆìƒ ì‹œê°„: 6-8ì‹œê°„
```

### XLM-RoBERTa-XXL (10.7B)
```bash
bash scripts/run_poc_h100.sh xxl

# ë©”ëª¨ë¦¬: ~28GB
# Batch Size: 128
# ì˜ˆìƒ ì‹œê°„: 12-15ì‹œê°„
```

---

## ğŸ”§ ì»¤ìŠ¤í…€ ì„¤ì •

### Batch Size ì¡°ì •

```bash
python scripts/train/anchor_train.py \
    --model-name facebook/xlm-roberta-xl \
    --batch-size 128 \              # OOM ë°œìƒ ì‹œ ì¤„ì´ê¸°
    --gradient-accumulation-steps 4 \ # Effective batch=512
    --use-amp \
    --gradient-checkpointing
```

### ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ

1. **Batch size ì¤„ì´ê¸°**
   ```bash
   --batch-size 64  # ê¸°ë³¸ 192ì—ì„œ
   ```

2. **Gradient accumulation ëŠ˜ë¦¬ê¸°**
   ```bash
   --gradient-accumulation-steps 4  # ê¸°ë³¸ 2ì—ì„œ
   ```

3. **LoRA rank ì¤„ì´ê¸°**
   ```bash
   --lora-r 16  # ê¸°ë³¸ 32ì—ì„œ
   ```

4. **Projection dim ì¤„ì´ê¸°**
   ```bash
   --projection-dim 512  # ê¸°ë³¸ 768ì—ì„œ
   ```

### ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•  ë•Œ

1. **Batch size ëŠ˜ë¦¬ê¸°**
   ```bash
   --batch-size 256
   ```

2. **LoRA rank ëŠ˜ë¦¬ê¸°**
   ```bash
   --lora-r 64
   --lora-alpha 128
   ```

3. **Projection dim ëŠ˜ë¦¬ê¸°**
   ```bash
   --projection-dim 1024
   ```

---

## ğŸ§® ë©”ëª¨ë¦¬ ê³„ì‚° ê³µì‹

### ì´ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰
```
Total = Model + Optimizer + Gradients + Activations + Batch

Model:        params Ã— 2 bytes (BF16)
Optimizer:    params Ã— 8 bytes (AdamW states)
Gradients:    params Ã— 2 bytes (BF16)
Activations:  batch_size Ã— seq_len Ã— hidden Ã— layers Ã— 2
Batch Data:   batch_size Ã— seq_len Ã— 2
```

### ìµœì í™” í›„
```
Model:        params Ã— 2 Ã— lora_ratio  (~1%)
Optimizer:    params Ã— 8 Ã— lora_ratio
Gradients:    params Ã— 2 Ã— lora_ratio
Activations:  batch_size Ã— seq_len Ã— hidden Ã— sqrt(layers)  (checkpointing)
```

---

## ğŸ“ˆ ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ Trade-off

| ì„¤ì • | ë©”ëª¨ë¦¬ | ì†ë„ | ì„±ëŠ¥ |
|------|--------|------|------|
| **Full Precision (FP32)** | 100% | 50% | 100% |
| **+ AMP (BF16)** | 50% | 100% | 99% |
| **+ Gradient Checkpointing** | 30% | 80% | 99% |
| **+ LoRA** | 15% | 90% | 98% |
| **All Combined** â­ | **15%** | **90%** | **98%** |

---

## ğŸ› Troubleshooting

### OOM ì—ëŸ¬ ë°œìƒ ì‹œ

```bash
# 1ë‹¨ê³„: Batch size ì ˆë°˜ìœ¼ë¡œ
--batch-size 96  # XLì˜ ê²½ìš°

# 2ë‹¨ê³„: Gradient accumulation 2ë°°ë¡œ
--gradient-accumulation-steps 4

# 3ë‹¨ê³„: LoRA rank ì¤„ì´ê¸°
--lora-r 16

# 4ë‹¨ê³„: Sequence length ì¤„ì´ê¸° (ë°ì´í„° ì „ì²˜ë¦¬)
max_length = 128  # ê¸°ë³¸ 256ì—ì„œ
```

### CUDA Out of Memory ë©”ì‹œì§€

```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```

**í•´ê²° ë°©ë²•:**
1. Batch sizeë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„
2. `torch.cuda.empty_cache()` ì¶”ê°€
3. Gradient accumulation ì‚¬ìš©
4. ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸ (`nvidia-smi`)

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦´ ë•Œ

```bash
# Gradient checkpointing ë¹„í™œì„±í™”
# (ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´)
python scripts/train/anchor_train.py \
    --use-amp \
    # --gradient-checkpointing  ì œê±°
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [PyTorch AMP Tutorial](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)
- [Gradient Checkpointing](https://pytorch.org/docs/stable/checkpoint.html)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [BF16 vs FP16](https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407)

---

## âœ… Quick Checklist

ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­:

- [ ] H100 GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸ (`nvidia-smi`)
- [ ] CUDA 12.0+ ì„¤ì¹˜ í™•ì¸
- [ ] PyTorch 2.0+ ì„¤ì¹˜ í™•ì¸
- [ ] `--use-amp` í”Œë˜ê·¸ í™œì„±í™”
- [ ] `--gradient-checkpointing` í”Œë˜ê·¸ í™œì„±í™”
- [ ] Batch sizeê°€ GPU ë©”ëª¨ë¦¬ì— ë§ëŠ”ì§€ í™•ì¸
- [ ] ì ì ˆí•œ gradient accumulation ì„¤ì •

**ì¶”ì²œ ì‹¤í–‰ ëª…ë ¹:**
```bash
# XL ëª¨ë¸ (3.5B) - ìµœì  ê· í˜•
bash scripts/run_poc_h100.sh xl
```

ì´ì œ H100ì˜ ì„±ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
