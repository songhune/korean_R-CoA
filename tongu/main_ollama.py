"""Ollama ì‚¬ìš©ì„ ìœ„í•œ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸"""

import asyncio
import json
from pathlib import Path

from config import TranslationConfig
from translator import LargeScaleTranslator
from cost_tracker import estimate_translation_cost


async def process_with_ollama():
    """Ollamaë¥¼ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
    
    config = TranslationConfig(
        api_provider="ollama",
        model="winkefinger/alma-13b:Q4_K_M",  # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: llama3, llama3.1, qwen2, mixtral ë“±
        batch_size=5,      # ë¡œì»¬ ëª¨ë¸ì´ë¯€ë¡œ ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
        max_concurrent=1,  # ë¡œì»¬ ì²˜ë¦¬ì´ë¯€ë¡œ ë™ì‹œì„± 1ë¡œ ì„¤ì •
        delay_between_batches=0.2,
        chunk_size=100,
        checkpoint_interval=50,
        budget_limit=0.0,  # ë¬´ë£Œ
        ollama_base_url="http://localhost:11434"  # ê¸°ë³¸ Ollama URL
    )
    
    translator = LargeScaleTranslator(config)
    
    # ACCN-INS ë°ì´í„°ì…‹ ì²˜ë¦¬
    await translator.process_large_dataset(
        input_file="/home/work/songhune/ACCN-INS.json",
        output_file="sample_ollama_translated.jsonl"
    )


async def test_ollama_connection():
    """Ollama ì—°ê²° í…ŒìŠ¤íŠ¸"""
    import aiohttp
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get("http://localhost:11434/api/tags") as response:
                if response.status == 200:
                    models = await response.json()
                    print("âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ!")
                    print("ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
                    for model in models.get('models', []):
                        print(f"   - {model['name']}")
                    return True
                else:
                    print("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
    except Exception as e:
        print(f"âŒ Ollama ì—°ê²° ì˜¤ë¥˜: {e}")
        print("\nğŸ”§ í•´ê²° ë°©ë²•:")
        print("1. Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸:")
        print("   curl -fsSL https://ollama.ai/install.sh | sh")
        print("\n2. Ollama ì„œë²„ ì‹œì‘:")
        print("   ollama serve")
        print("\n3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì˜ˆì‹œ):")
        print("   ollama pull llama3.1")
        print("   ollama pull qwen2")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) == 1:
        print("KEadapter - Ollama ë²„ì „ (ë¬´ë£Œ ë¡œì»¬ ë²ˆì—­ê¸°)")
        print()
        print("ì‚¬ìš©ë²•:")
        print("  python main_ollama.py test      # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸")
        print("  python main_ollama.py translate # ë²ˆì—­ ì‹¤í–‰")
        print()
        return
    
    command = sys.argv[1]
    
    if command == "test":
        print("=== Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ===")
        result = asyncio.run(test_ollama_connection())
        if result:
            print("\nâœ… ì¤€ë¹„ ì™„ë£Œ! 'python main_ollama.py translate'ë¡œ ë²ˆì—­ì„ ì‹œì‘í•˜ì„¸ìš”.")
    
    elif command == "translate":
        print("=== Ollama ë²ˆì—­ ì‹œì‘ ===")
        asyncio.run(process_with_ollama())
    
    else:
        print(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {command}")
        print("python main_ollama.py ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()