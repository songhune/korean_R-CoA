#!/usr/bin/env python3
"""
NLI/STS ë°ì´í„° ìƒì„± í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ Plan A (ì ì§„ì  êµ¬í˜„) ë²„ì „ìœ¼ë¡œ,
ê¸°ì¡´ CC-KR ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ NLI/STS ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python run_nli_sts_generation.py --mode all
    python run_nli_sts_generation.py --mode kr_only
    python run_nli_sts_generation.py --mode cc_kr_only
"""

import argparse
import os
import sys
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from network.kr_nli_generator import KoreanNLIGenerator
from network.kr_sts_generator import KoreanSTSGenerator  
from network.cc_kr_processor import ClassicalChineseKoreanProcessor

def load_korean_text_data(data_dir: str) -> list[str]:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    korean_texts = []
    
    # 1. ê³¼ê±°ì‹œí—˜ í•œêµ­ì–´ í•´ì„¤ ë°ì´í„°
    try:
        gwashi_path = os.path.join(data_dir, "gwashi.csv")
        if os.path.exists(gwashi_path):
            import pandas as pd
            df = pd.read_csv(gwashi_path, encoding='cp949')
            contents = df['contents'].dropna().astype(str).tolist()
            korean_texts.extend([text for text in contents if len(text.strip()) > 20])
            print(f"âœ… gwashi.csvì—ì„œ {len(contents)}ê°œ í…ìŠ¤íŠ¸ ë¡œë“œ")
    except Exception as e:
        print(f"âŒ gwashi.csv ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 2. ë¬¸ì§‘ ë°ì´í„°
    try:
        munjib_path = os.path.join(data_dir, "munjib.csv") 
        if os.path.exists(munjib_path):
            import pandas as pd
            df = pd.read_csv(munjib_path, encoding='utf-8')
            if 'answer_contents' in df.columns:
                contents = df['answer_contents'].dropna().astype(str).tolist()
                korean_texts.extend([text for text in contents if len(text.strip()) > 20])
                print(f"âœ… munjib.csvì—ì„œ {len(contents)}ê°œ í…ìŠ¤íŠ¸ ë¡œë“œ")
    except Exception as e:
        print(f"âŒ munjib.csv ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 3. ì‚¬ì„œ í•œêµ­ì–´ í•´ì„¤ (JSONLì—ì„œ ì¶”ì¶œ)
    try:
        saseo_path = os.path.join(data_dir, "sigwon", "SSDB", "saseo_qwen3_chat.jsonl")
        if os.path.exists(saseo_path):
            with open(saseo_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        messages = data.get("messages", [])
                        for msg in messages:
                            if msg.get("role") == "assistant":
                                content = msg.get("content", "")
                                # <think> íƒœê·¸ ì œê±°
                                import re
                                content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
                                content = content.strip()
                                if len(content) > 20:
                                    korean_texts.append(content)
            print(f"âœ… saseo JSONLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ê°€ ë¡œë“œ")
    except Exception as e:
        print(f"âŒ saseo JSONL ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    print(f"ğŸ“Š ì´ í•œêµ­ì–´ í…ìŠ¤íŠ¸: {len(korean_texts)}ê°œ")
    return korean_texts

def generate_korean_nli_sts(korean_texts: list[str], output_dir: str):
    """í•œêµ­ì–´ NLI/STS ë°ì´í„° ìƒì„±"""
    print("\n=== í•œêµ­ì–´ NLI/STS ë°ì´í„° ìƒì„± ì‹œì‘ ===")
    
    # NLI ìƒì„±
    print("1. í•œêµ­ì–´ NLI ë°ì´í„° ìƒì„± ì¤‘...")
    nli_generator = KoreanNLIGenerator()
    
    # í…ìŠ¤íŠ¸ ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    sample_size = min(100, len(korean_texts))
    sampled_texts = korean_texts[:sample_size]
    
    nli_triples = nli_generator.generate_nli_dataset(sampled_texts, max_per_premise=3)
    
    nli_output_path = os.path.join(output_dir, "korean_nli.jsonl")
    nli_generator.save_to_jsonl(nli_triples, nli_output_path)
    
    # STS ìƒì„±
    print("2. í•œêµ­ì–´ STS ë°ì´í„° ìƒì„± ì¤‘...")
    sts_generator = KoreanSTSGenerator()
    
    sts_pairs = sts_generator.generate_sts_pairs(
        sampled_texts,
        target_distribution={"high": 30, "medium": 50, "low": 20}
    )
    
    sts_output_path = os.path.join(output_dir, "korean_sts.jsonl")
    sts_generator.save_to_jsonl(sts_pairs, sts_output_path)
    
    print(f"âœ… í•œêµ­ì–´ NLI: {len(nli_triples)}ê°œ")
    print(f"âœ… í•œêµ­ì–´ STS: {len(sts_pairs)}ê°œ")

def generate_cc_kr_nli_sts(data_dir: str, output_dir: str):
    """CC-KR ê¸°ë°˜ NLI/STS ë°ì´í„° ìƒì„±"""
    print("\n=== CC-KR ê¸°ë°˜ NLI/STS ë°ì´í„° ìƒì„± ì‹œì‘ ===")
    
    processor = ClassicalChineseKoreanProcessor()
    
    # ê¸°ì¡´ CC-KR ë°ì´í„° ë¡œë“œ
    saseo_path = os.path.join(data_dir, "sigwon", "SSDB", "saseo_qwen3_chat.jsonl")
    sigwon_path = os.path.join(data_dir, "sigwon.csv")
    
    processor.load_existing_data(saseo_path, sigwon_path)
    
    # CC-KR ë°ì´í„°ì…‹ ìƒì„± ë° ì €ì¥
    processor.save_datasets(output_dir)

def create_requirements_file(output_dir: str):
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡ íŒŒì¼ ìƒì„±"""
    requirements = [
        "pandas>=1.3.0",
        "numpy>=1.21.0", 
        "scikit-learn>=1.0.0",
        "sentence-transformers>=2.2.0",
        "torch>=1.9.0",
        "transformers>=4.20.0",
        "jsonlines>=3.0.0"
    ]
    
    requirements_path = os.path.join(output_dir, "requirements_nli_sts.txt")
    with open(requirements_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(requirements))
    
    print(f"âœ… í•„ìš” íŒ¨í‚¤ì§€ ëª©ë¡ ìƒì„±: {requirements_path}")

def create_dataset_summary(output_dir: str):
    """ìƒì„±ëœ ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´ ìƒì„±"""
    summary = {
        "dataset_info": {
            "generation_date": "2024-12-17",
            "algorithm_version": "Plan A - ì ì§„ì  êµ¬í˜„",
            "description": "ê¸°ì¡´ CC-KR ë°ì´í„°ë¥¼ í™œìš©í•œ NLI/STS ë°ì´í„°ì…‹",
            "files": []
        },
        "datasets": {}
    }
    
    # ìƒì„±ëœ íŒŒì¼ë“¤ í™•ì¸
    for filename in os.listdir(output_dir):
        if filename.endswith('.jsonl'):
            filepath = os.path.join(output_dir, filename)
            line_count = 0
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    line_count = sum(1 for line in f)
                
                summary["dataset_info"]["files"].append({
                    "filename": filename,
                    "size": line_count,
                    "type": "jsonl"
                })
                
                # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì¶”ê°€
                with open(filepath, 'r', encoding='utf-8') as f:
                    first_line = f.readline()
                    if first_line:
                        sample = json.loads(first_line)
                        summary["datasets"][filename.replace('.jsonl', '')] = {
                            "count": line_count,
                            "sample": sample
                        }
            except Exception as e:
                print(f"âš ï¸ {filename} ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    summary_path = os.path.join(output_dir, "dataset_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë°ì´í„°ì…‹ ìš”ì•½ ìƒì„±: {summary_path}")

def main():
    parser = argparse.ArgumentParser(description="NLI/STS ë°ì´í„° ìƒì„± ë„êµ¬")
    parser.add_argument("--mode", choices=["all", "kr_only", "cc_kr_only"], 
                       default="all", help="ìƒì„± ëª¨ë“œ ì„ íƒ")
    parser.add_argument("--data_dir", default="../data", 
                       help="ì…ë ¥ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ../data)")
    parser.add_argument("--output_dir", default="./output", 
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./output)")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("ğŸš€ NLI/STS ë°ì´í„° ìƒì„± ì‹œì‘")
    print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {args.data_dir}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print(f"ğŸ”§ ëª¨ë“œ: {args.mode}")
    
    try:
        if args.mode in ["all", "kr_only"]:
            # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¡œë“œ
            korean_texts = load_korean_text_data(args.data_dir)
            
            if korean_texts:
                generate_korean_nli_sts(korean_texts, args.output_dir)
            else:
                print("âŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if args.mode in ["all", "cc_kr_only"]:
            generate_cc_kr_nli_sts(args.data_dir, args.output_dir)
        
        # ë¶€ê°€ íŒŒì¼ë“¤ ìƒì„±
        create_requirements_file(args.output_dir)
        create_dataset_summary(args.output_dir)
        
        print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼ í™•ì¸: {args.output_dir}/dataset_summary.json")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()