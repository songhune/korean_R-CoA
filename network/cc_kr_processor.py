import pandas as pd
import json
import re
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import random

@dataclass
class CCKRPair:
    """ê³ ì „í•œë¬¸-í•œêµ­ì–´ ìŒ"""
    classical_chinese: str
    korean_translation: str
    korean_explanation: str
    source: str
    metadata: Dict = None

class ClassicalChineseKoreanProcessor:
    """ê¸°ì¡´ CC-KR ë°ì´í„°ë¥¼ í™œìš©í•œ NLI/STS ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self):
        self.cc_kr_pairs = []
        
        # ê³ ì „í•œë¬¸ ì ‘ì†ì–´/ë…¼ë¦¬ í‘œì§€
        self.cc_logical_markers = {
            "æ•…": "ê·¸ëŸ¬ë¯€ë¡œ",
            "æ˜¯ä»¥": "ì´ëŸ° ê¹Œë‹­ì—", 
            "ç„¶": "ê·¸ëŸ¬ë‚˜",
            "é›–": "ë¹„ë¡",
            "è€Œ": "ê·¸ëŸ°ë°",
            "è‹¥": "ë§Œì•½",
            "å‰‡": "ê·¸ëŸ¬ë©´",
            "é": "ì•„ë‹ˆë‹¤",
            "ä¸ç„¶": "ê·¸ë ‡ì§€ ì•Šë‹¤",
            "ä¸”": "ë˜í•œ",
            "äº¦": "ë˜í•œ",
            "æˆ–": "í˜¹ì€"
        }
        
        # ê³ ì „í•œë¬¸ í•µì‹¬ ì–´íœ˜
        self.classical_vocabulary = {
            "ä»": "ì¸", "ç¾©": "ì˜", "ç¦®": "ì˜ˆ", "æ™º": "ì§€",
            "å­¸": "ë°°ì›€", "é“": "ë„", "å¾·": "ë•", "è–": "ì„±ì¸",
            "å›å­": "êµ°ì", "å°äºº": "ì†Œì¸", "å­": "íš¨ë„",
            "å¼Ÿ": "ê³µê²½", "å¿ ": "ì¶©ì„±", "ä¿¡": "ë¯¿ìŒ"
        }
    
    def load_existing_data(self, saseo_jsonl_path: str, sigwon_csv_path: str):
        """ê¸°ì¡´ CC-KR ë°ì´í„° ë¡œë“œ"""
        self.cc_kr_pairs = []
        
        # 1. saseo JSONL ë°ì´í„° ë¡œë“œ (ëŒ€í™”í˜•)
        try:
            with open(saseo_jsonl_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        cc_kr_pair = self._extract_cc_kr_from_dialog(data)
                        if cc_kr_pair:
                            self.cc_kr_pairs.append(cc_kr_pair)
            print(f"âœ… Saseo JSONL ë¡œë“œ: {len([p for p in self.cc_kr_pairs if p.source == 'saseo'])}ê°œ")
        except Exception as e:
            print(f"âŒ Saseo JSONL ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 2. sigwon CSV ë°ì´í„° ë¡œë“œ (ì›ë¬¸-ë²ˆì—­)
        try:
            sigwon_df = pd.read_csv(sigwon_csv_path, encoding='utf-8')
            sigwon_pairs = self._extract_cc_kr_from_sigwon(sigwon_df)
            self.cc_kr_pairs.extend(sigwon_pairs)
            print(f"âœ… Sigwon CSV ë¡œë“œ: {len(sigwon_pairs)}ê°œ")
        except Exception as e:
            print(f"âŒ Sigwon CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ“Š ì´ CC-KR ìŒ: {len(self.cc_kr_pairs)}ê°œ")
    
    def _extract_cc_kr_from_dialog(self, dialog_data: Dict) -> Optional[CCKRPair]:
        """ëŒ€í™” ë°ì´í„°ì—ì„œ CC-KR ì¶”ì¶œ"""
        messages = dialog_data.get("messages", [])
        
        for i, message in enumerate(messages):
            if message.get("role") == "user" and "ë‹¤ìŒ í•œë¬¸ ë¬¸ì¥ì„ í•´ì„í•˜ê³ " in message.get("content", ""):
                # ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ í•œë¬¸ ì¶”ì¶œ
                content = message.get("content", "")
                cc_text = self._extract_chinese_text(content)
                
                # ë‹¤ìŒ assistant ë©”ì‹œì§€ì—ì„œ í•œêµ­ì–´ í•´ì„ ì¶”ì¶œ
                if i + 1 < len(messages) and messages[i + 1].get("role") == "assistant":
                    assistant_content = messages[i + 1].get("content", "")
                    kr_translation, kr_explanation = self._parse_assistant_response(assistant_content)
                    
                    if cc_text and kr_translation:
                        return CCKRPair(
                            classical_chinese=cc_text,
                            korean_translation=kr_translation,
                            korean_explanation=kr_explanation,
                            source="saseo"
                        )
        return None
    
    def _extract_cc_kr_from_sigwon(self, df: pd.DataFrame) -> List[CCKRPair]:
        """sigwon ë°ì´í„°ì—ì„œ CC-KR ì¶”ì¶œ"""
        pairs = []
        
        for _, row in df.iterrows():
            cc_text = str(row.get('original', '')).strip()
            kr_text = str(row.get('translation', '') or row.get('meaning', '')).strip()
            
            if cc_text and kr_text and len(cc_text) > 5 and len(kr_text) > 5:
                # í•œë¬¸ì¸ì§€ í™•ì¸ (í•œì ë¹„ìœ¨)
                chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', cc_text))
                if chinese_chars / len(cc_text) > 0.7:  # 70% ì´ìƒì´ í•œì
                    pairs.append(CCKRPair(
                        classical_chinese=cc_text,
                        korean_translation=kr_text,
                        korean_explanation="",
                        source="sigwon",
                        metadata={"year": row.get('year'), "author": row.get('writer')}
                    ))
        
        return pairs
    
    def _extract_chinese_text(self, content: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ í•œë¬¸ ì¶”ì¶œ"""
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            # í•œì ë¹„ìœ¨ì´ ë†’ì€ ë¼ì¸ ì°¾ê¸°
            if line and len(line) > 3:
                chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', line))
                if chinese_chars / len(line) > 0.7:
                    return line
        return ""
    
    def _parse_assistant_response(self, content: str) -> Tuple[str, str]:
        """assistant ì‘ë‹µì—ì„œ ë²ˆì—­ê³¼ í•´ì„¤ ë¶„ë¦¬"""
        # <think> íƒœê·¸ ì œê±°
        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
        content = content.strip()
        
        lines = content.split('\n')
        translation = ""
        explanation = ""
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('"') and not line.startswith("'"):
                if not translation:
                    translation = line
                else:
                    explanation += line + " "
        
        return translation.strip(), explanation.strip()
    
    def generate_cc_kr_nli(self, max_pairs: int = 100) -> List[Dict]:
        """CC-KR ê¸°ë°˜ NLI ìƒì„±"""
        nli_data = []
        
        for pair in self.cc_kr_pairs[:max_pairs]:
            cc_text = pair.classical_chinese
            kr_text = pair.korean_translation
            
            if not cc_text or not kr_text:
                continue
            
            # 1. Entailment: CC â†’ KR ë²ˆì—­ ê´€ê³„
            nli_data.append({
                "premise": f"ê³ ì „í•œë¬¸: {cc_text}",
                "hypothesis": f"í•œêµ­ì–´ ë²ˆì—­: {kr_text}",
                "label": "entailment",
                "metadata": {
                    "type": "translation_pair",
                    "source": pair.source,
                    "direction": "cc_to_kr"
                }
            })
            
            # 2. Contradiction: ì˜ëª»ëœ ë²ˆì—­
            wrong_translation = self._generate_wrong_translation(kr_text)
            if wrong_translation:
                nli_data.append({
                    "premise": f"ê³ ì „í•œë¬¸: {cc_text}",
                    "hypothesis": f"í•œêµ­ì–´ ë²ˆì—­: {wrong_translation}",
                    "label": "contradiction",
                    "metadata": {
                        "type": "wrong_translation",
                        "source": pair.source,
                        "direction": "cc_to_kr"
                    }
                })
            
            # 3. Neutral: ê´€ë ¨ ìˆì§€ë§Œ ì§ì ‘ì  ë²ˆì—­ì´ ì•„ë‹Œ ë‚´ìš©
            related_content = self._generate_related_content(cc_text, kr_text)
            if related_content:
                nli_data.append({
                    "premise": f"ê³ ì „í•œë¬¸: {cc_text}",
                    "hypothesis": f"ê´€ë ¨ ë‚´ìš©: {related_content}",
                    "label": "neutral",
                    "metadata": {
                        "type": "related_content", 
                        "source": pair.source,
                        "direction": "cc_to_kr"
                    }
                })
        
        return nli_data
    
    def generate_cc_kr_sts(self, max_pairs: int = 100) -> List[Dict]:
        """CC-KR ê¸°ë°˜ STS ìƒì„±"""
        sts_data = []
        
        # 1. ë™ì¼ ì›ë¬¸ì˜ ë‹¤ë¥¸ ë²ˆì—­ë“¤ (ê³ ìœ ì‚¬ë„)
        cc_groups = {}
        for pair in self.cc_kr_pairs:
            cc = pair.classical_chinese
            if cc not in cc_groups:
                cc_groups[cc] = []
            cc_groups[cc].append(pair)
        
        for cc_text, pairs in cc_groups.items():
            if len(pairs) >= 2:
                for i in range(len(pairs)):
                    for j in range(i + 1, len(pairs)):
                        sts_data.append({
                            "sentence1": pairs[i].korean_translation,
                            "sentence2": pairs[j].korean_translation,
                            "score": random.uniform(4.0, 5.0),  # ê°™ì€ ì›ë¬¸ì´ë¯€ë¡œ ê³ ìœ ì‚¬ë„
                            "metadata": {
                                "type": "same_source_different_translation",
                                "classical_source": cc_text
                            }
                        })
                        
                        if len(sts_data) >= max_pairs // 3:
                            break
                    if len(sts_data) >= max_pairs // 3:
                        break
        
        # 2. ë¹„ìŠ·í•œ ì£¼ì œ/ë‚´ìš©ì˜ CC-KR ìŒë“¤ (ì¤‘ê°„ìœ ì‚¬ë„)
        similar_pairs = self._find_similar_content_pairs(max_pairs // 3)
        sts_data.extend(similar_pairs)
        
        # 3. ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš© (ì €ìœ ì‚¬ë„)
        different_pairs = self._find_different_content_pairs(max_pairs // 3)
        sts_data.extend(different_pairs)
        
        return sts_data
    
    def _generate_wrong_translation(self, correct_translation: str) -> str:
        """ì˜ëª»ëœ ë²ˆì—­ ìƒì„±"""
        wrong_patterns = [
            (r'ì´ë‹¤', 'ì´ ì•„ë‹ˆë‹¤'),
            (r'í•œë‹¤', 'í•˜ì§€ ì•ŠëŠ”ë‹¤'),
            (r'ìˆë‹¤', 'ì—†ë‹¤'),
            (r'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤'),
            (r'í¬ë‹¤', 'ì‘ë‹¤'),
            (r'ì¤‘ìš”í•˜ë‹¤', 'ì¤‘ìš”í•˜ì§€ ì•Šë‹¤')
        ]
        
        result = correct_translation
        pattern, replacement = random.choice(wrong_patterns)
        if pattern in result:
            result = result.replace(pattern, replacement)
            return result
        
        # ë‹¨ì–´ ìˆœì„œ ë°”ê¾¸ê¸°
        words = result.split()
        if len(words) >= 3:
            words[0], words[-1] = words[-1], words[0]
            return ' '.join(words)
        
        return ""
    
    def _generate_related_content(self, cc_text: str, kr_text: str) -> str:
        """ê´€ë ¨ ìˆëŠ” ë‚´ìš© ìƒì„±"""
        related_templates = [
            f"{kr_text}ì™€ ê´€ë ¨ëœ ë‹¤ë¥¸ êµí›ˆë„ ìˆë‹¤.",
            f"ì´ì™€ ìœ ì‚¬í•œ ë§¥ë½ì—ì„œ ë‹¤ë¥¸ í•´ì„ë„ ê°€ëŠ¥í•˜ë‹¤.",
            f"{kr_text}ì˜ ë°°ê²½ì´ ë˜ëŠ” ì—­ì‚¬ì  ìƒí™©ì´ ìˆë‹¤.",
            f"ì´ëŸ¬í•œ ê°€ë¥´ì¹¨ì€ í˜„ëŒ€ì—ë„ ì ìš©ë  ìˆ˜ ìˆë‹¤.",
            f"{kr_text}ì™€ ì—°ê´€ëœ ë‹¤ë¥¸ ê³ ì „ ë¬¸í—Œë„ ìˆë‹¤."
        ]
        
        return random.choice(related_templates)
    
    def _find_similar_content_pairs(self, max_pairs: int) -> List[Dict]:
        """ìœ ì‚¬í•œ ë‚´ìš©ì˜ ìŒ ì°¾ê¸°"""
        pairs = []
        
        # ê³µí†µ í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ì„± ê²€ì‚¬
        for i, pair1 in enumerate(self.cc_kr_pairs):
            if len(pairs) >= max_pairs:
                break
                
            for pair2 in self.cc_kr_pairs[i+1:]:
                if len(pairs) >= max_pairs:
                    break
                    
                similarity = self._calculate_content_similarity(
                    pair1.korean_translation, 
                    pair2.korean_translation
                )
                
                if 0.2 < similarity < 0.8:  # ì ë‹¹í•œ ìœ ì‚¬ì„±
                    pairs.append({
                        "sentence1": pair1.korean_translation,
                        "sentence2": pair2.korean_translation,
                        "score": 2.0 + similarity * 2.0,  # 2.0~4.0 ì ìˆ˜
                        "metadata": {
                            "type": "similar_content",
                            "similarity": similarity
                        }
                    })
        
        return pairs
    
    def _find_different_content_pairs(self, max_pairs: int) -> List[Dict]:
        """ë‹¤ë¥¸ ë‚´ìš©ì˜ ìŒ ì°¾ê¸°"""
        pairs = []
        random.shuffle(self.cc_kr_pairs)
        
        for i in range(0, len(self.cc_kr_pairs) - 1, 2):
            if len(pairs) >= max_pairs:
                break
                
            pair1 = self.cc_kr_pairs[i]
            pair2 = self.cc_kr_pairs[i + 1]
            
            similarity = self._calculate_content_similarity(
                pair1.korean_translation,
                pair2.korean_translation
            )
            
            if similarity < 0.3:  # ë‚®ì€ ìœ ì‚¬ì„±
                pairs.append({
                    "sentence1": pair1.korean_translation,
                    "sentence2": pair2.korean_translation,
                    "score": similarity * 2.0,  # 0.0~0.6 ì ìˆ˜
                    "metadata": {
                        "type": "different_content",
                        "similarity": similarity
                    }
                })
        
        return pairs
    
    def _calculate_content_similarity(self, text1: str, text2: str) -> float:
        """ë‚´ìš© ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        # ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords1 = set(re.findall(r'[ê°€-í£]{2,}', text1))
        keywords2 = set(re.findall(r'[ê°€-í£]{2,}', text2))
        
        if not keywords1 or not keywords2:
            return 0.0
        
        intersection = len(keywords1 & keywords2)
        union = len(keywords1 | keywords2)
        
        return intersection / union if union > 0 else 0.0
    
    def generate_reverse_translation_nli(self, max_pairs: int = 50) -> List[Dict]:
        """ì—­ë²ˆì—­ ê¸°ë°˜ NLI (KR â†’ CC ë°©í–¥)"""
        nli_data = []
        
        for pair in self.cc_kr_pairs[:max_pairs]:
            kr_text = pair.korean_translation
            cc_text = pair.classical_chinese
            
            # 1. KR â†’ simulated CC translation
            simulated_cc = self._simulate_kr_to_cc_translation(kr_text)
            if simulated_cc:
                # Entailment: ì˜¬ë°”ë¥¸ ì—­ë²ˆì—­
                nli_data.append({
                    "premise": f"í•œêµ­ì–´: {kr_text}",
                    "hypothesis": f"ê³ ì „í•œë¬¸ ë²ˆì—­: {simulated_cc}",
                    "label": "entailment",
                    "metadata": {
                        "type": "reverse_translation",
                        "direction": "kr_to_cc",
                        "original_cc": cc_text
                    }
                })
        
        return nli_data
    
    def _simulate_kr_to_cc_translation(self, kr_text: str) -> str:
        """í•œêµ­ì–´ë¥¼ ê³ ì „í•œë¬¸ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜"""
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë³€í™˜ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ëª¨ë¸ í•„ìš”)
        kr_to_cc_mapping = {
            "ê·¸ëŸ¬ë¯€ë¡œ": "æ•…",
            "ë”°ë¼ì„œ": "æ˜¯ä»¥", 
            "ê·¸ëŸ¬ë‚˜": "ç„¶",
            "ë§Œì•½": "è‹¥",
            "ë˜í•œ": "ä¸”",
            "ì¸": "ä»",
            "ì˜": "ç¾©", 
            "ì˜ˆ": "ç¦®",
            "ë°°ì›€": "å­¸",
            "ë„": "é“"
        }
        
        result = kr_text
        for kr_word, cc_word in kr_to_cc_mapping.items():
            if kr_word in result:
                result = result.replace(kr_word, cc_word)
        
        # ë¬¸ì¥ ëì„ ê³ ì „í•œë¬¸ ìŠ¤íƒ€ì¼ë¡œ
        if result.endswith('ë‹¤'):
            result = result[:-1] + 'ä¹Ÿ'
        elif result.endswith('ì´ë‹¤'):
            result = result[:-2] + 'ä¹Ÿ'
        
        return result if result != kr_text else ""
    
    def save_datasets(self, output_dir: str = "./"):
        """ìƒì„±ëœ ë°ì´í„°ì…‹ë“¤ ì €ì¥"""
        # NLI ë°ì´í„°ì…‹
        nli_data = self.generate_cc_kr_nli(200)
        nli_data.extend(self.generate_reverse_translation_nli(50))
        
        with open(f"{output_dir}/cc_kr_nli.jsonl", 'w', encoding='utf-8') as f:
            for item in nli_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        # STS ë°ì´í„°ì…‹
        sts_data = self.generate_cc_kr_sts(150)
        
        with open(f"{output_dir}/cc_kr_sts.jsonl", 'w', encoding='utf-8') as f:
            for item in sts_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"âœ… CC-KR NLI ë°ì´í„°ì…‹ ì €ì¥: {len(nli_data)}ê°œ")
        print(f"âœ… CC-KR STS ë°ì´í„°ì…‹ ì €ì¥: {len(sts_data)}ê°œ")